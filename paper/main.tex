% IEEE Conference Paper Template
\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unwanted, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{subcaption}

% Define custom commands for ease of use
\newcommand{\PU}{PU}
\newcommand{\PUEA}{PUEA}
\newcommand{\SU}{SU}
\newcommand{\KNN}{$K$-NN}

\begin{document}

\title{Optimized PUEA Detection in Cognitive Radio Networks: A Comparative Analysis of Clustering and Statistical Approaches}

\author{\IEEEauthorblockN{Author Name}
\IEEEauthorblockA{\textit{Department of Electrical Engineering} \\
\textit{University Name}\\
City, Country \\
email@domain.edu}}

\maketitle

\begin{abstract}
Primary User Emulation Attacks (PUEA) pose a significant security threat to cognitive radio networks by impersonating legitimate primary users, leading to spectrum access denial for secondary users. This paper presents a comparative analysis of traditional clustering algorithms and enhanced detection methods for PUEA detection in cognitive radio networks. We investigate how accurately different detection approaches can distinguish between legitimate primary users and malicious attackers across various spatial separation scenarios. Our novel approach combines clustering with K-nearest neighbors (KNN) and Means algorithms to improve detection accuracy. Experimental results demonstrate that our enhanced approach achieves significant improvements in detection rate and reduced false alarm rates compared to traditional clustering methods. The performance enhancement is particularly evident in challenging scenarios with close proximity between primary users and attackers. Statistical significance testing confirms that our approach consistently outperforms traditional methods, with improvements of up to X\% in detection accuracy and Y\% in false alarm reduction across different PUEA presence percentages.
\end{abstract}

\begin{IEEEkeywords}
cognitive radio networks, primary user emulation attack, PUEA detection, clustering algorithms, K-means, DBSCAN, KNN, security
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}

Cognitive radio networks (CRNs) have emerged as a promising solution to address spectrum scarcity by enabling opportunistic access to underutilized licensed bands. In this paradigm, secondary users (SUs) can temporarily access spectrum resources when primary users (PUs) are inactive \cite{akyildiz2006next}. However, this dynamic spectrum access introduces new security vulnerabilities, among which Primary User Emulation Attacks (PUEA) present a particularly concerning threat \cite{chen2008robust}. In a PUEA scenario, malicious entities emulate the signal characteristics of legitimate PUs, causing SUs to vacate the spectrum unnecessarily, thus undermining the fundamental benefits of cognitive radio technology.

\subsection{Problem Statement}
A critical challenge in securing CRNs lies in differentiating between legitimate PUs and PUEA attackers. Both entities transmit similar signal characteristics, making traditional detection mechanisms insufficient. The problem becomes increasingly challenging when the physical separation between PUs and attackers decreases, as signal features become more similar. Despite numerous approaches proposed in literature, there remains a significant gap in developing robust PUEA detection methods that perform well across varying spatial scenarios and attacker presence levels.

\subsection{Research Objectives}
This paper aims to address the following research objectives:
\begin{itemize}
    \item To evaluate the effectiveness of traditional clustering algorithms for PUEA detection across different spatial separation scenarios
    \item To develop enhanced detection methods by combining clustering with KNN and Means algorithms
    \item To quantify the performance improvement achieved by our enhanced approach compared to traditional clustering methods
    \item To analyze the impact of varying PUEA presence percentages on detection performance
\end{itemize}

\subsection{Approach Overview}
Our approach leverages statistical features extracted from signal power measurements collected by SUs. We first extract a comprehensive feature set including mean, variance, median, and quartile values from power measurements. Using these features, we implement and compare traditional clustering algorithms (DBSCAN, K-means, Agglomerative, and Spectral clustering) with our enhanced approach that applies KNN and Means algorithms within established clusters. We evaluate performance across three distinct spatial scenarios representing different separation distances between PUs and attackers, and with varying PUEA presence percentages (10\% to 50\%).

The rest of this paper is organized as follows: Section \ref{sec:related_work} reviews existing PUEA detection techniques and identifies research gaps. Section \ref{sec:system_model} describes our system model and methodology. Sections \ref{sec:traditional_clustering} and \ref{sec:enhanced_detection} present the traditional clustering and enhanced detection approaches, respectively. Section \ref{sec:performance_analysis} provides a comprehensive performance analysis. Section \ref{sec:results_discussion} discusses results and implications, and Section \ref{sec:conclusion} concludes the paper with future research directions.

\section{Related Work}
\label{sec:related_work}

PUEA detection has been an active area of research in cognitive radio security. This section provides a brief overview of existing approaches and identifies the research gaps our work addresses.

\subsection{Existing PUEA Detection Techniques}

PUEA detection techniques in literature can be broadly categorized into several approaches:

\subsubsection{Location-based Methods}
Location verification has been widely used to distinguish between legitimate PUs and attackers \cite{chen2008defense, liu2010aldo}. These approaches typically rely on estimating the transmitter's position using techniques such as time difference of arrival (TDoA), received signal strength (RSS), and angle of arrival (AoA). While effective in certain scenarios, these methods require specialized hardware or infrastructure support, limiting their practical applicability.

\subsubsection{Signal Characteristic-based Methods}
Several researchers have exploited unique signal characteristics to identify PUEA. For instance, \cite{jin2010cyclostationarity} utilized cyclostationarity features of signals, while \cite{huang2013cross} employed cross-layer detection techniques combining PHY and MAC layer information. These approaches typically perform well in controlled environments but may struggle when attackers precisely mimic legitimate signal characteristics.

\subsubsection{Machine Learning-based Methods}
Recent years have witnessed growing interest in applying machine learning techniques for PUEA detection \cite{wang2018deep, kim2018machine}. These approaches train classifiers using features extracted from signal measurements to distinguish between legitimate and malicious transmissions. While promising, many existing machine learning approaches have not been systematically evaluated across varying spatial scenarios and attacker presence levels.

\subsection{Clustering Applications in Wireless Security}

Clustering algorithms have been applied to various wireless security problems, including intrusion detection \cite{shah2018clustering} and anomaly detection \cite{rajendran2019secure}. In the context of PUEA detection, clustering has been used to group signal features and identify outliers representing potential attacks \cite{luo2019clustering}. However, most existing work employs a single clustering algorithm without comparative analysis across different approaches.

\subsection{Research Gaps}

Despite the progress in PUEA detection, several important gaps remain:

\begin{itemize}
    \item Limited comparative analysis of different clustering algorithms for PUEA detection across varying spatial scenarios
    \item Insufficient exploration of hybrid approaches combining clustering with other machine learning techniques
    \item Lack of systematic evaluation with varying attacker presence percentages
    \item Inadequate statistical validation of performance improvements between detection approaches
\end{itemize}

Our work addresses these gaps by providing a comprehensive comparative analysis of traditional clustering and enhanced detection approaches across different scenarios and attacker presence levels, with rigorous statistical validation of performance improvements.

\section{System Model and Methodology}
\label{sec:system_model}

This section describes our system model, including network topology, signal propagation characteristics, and the methodology for data generation and feature extraction.

\subsection{Network Topology}

We consider a cognitive radio network comprising one primary user (PU), one PUEA attacker, and 30 secondary users (SUs) distributed randomly within a 100Ã—100 unit area. We investigate three distinct scenarios based on the spatial separation between the PU and PUEA:

\begin{itemize}
    \item \textbf{Scenario A (Far Distance)}: PU located at coordinates [20,20] and PUEA at [80,80], resulting in a separation distance of 84.85 units
    \item \textbf{Scenario B (Medium Distance)}: PU at [25,25] and PUEA at [55,55], with a separation distance of 42.43 units
    \item \textbf{Scenario C (Close Distance)}: PU at [40,40] and PUEA at [55,55], with a separation distance of 21.21 units
\end{itemize}

Fig. \ref{fig:network_topology} illustrates the network topology for each scenario.

\begin{figure}[!t]
\centering
\begin{tikzpicture}[scale=0.4]
    \begin{axis}[
        title={Scenario A: Far Distance},
        xlabel={X coordinate},
        ylabel={Y coordinate},
        xmin=0, xmax=100,
        ymin=0, ymax=100,
        grid=both,
        legend pos=north west
    ]
    \addplot[color=blue,mark=*,mark size=3pt] coordinates {(20,20)};
    \addplot[color=red,mark=*,mark size=3pt] coordinates {(80,80)};
    \addplot[color=green,only marks,mark=o,mark size=1pt] coordinates {
        (30,40) (45,23) (67,34) (12,56) (78,23) (34,67) (56,78) (89,45) (33,22) (67,88)
        (45,56) (23,45) (78,34) (34,56) (56,34) (89,23) (23,67) (67,12) (45,78) (78,56)
        (23,89) (56,67) (34,78) (78,45) (45,34) (89,67) (67,56) (34,12) (56,23) (12,45)
    };
    \legend{PU,PUEA,SUs}
    \end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}[scale=0.4]
    \begin{axis}[
        title={Scenario B: Medium Distance},
        xlabel={X coordinate},
        ylabel={Y coordinate},
        xmin=0, xmax=100,
        ymin=0, ymax=100,
        grid=both,
        legend pos=north west
    ]
    \addplot[color=blue,mark=*,mark size=3pt] coordinates {(25,25)};
    \addplot[color=red,mark=*,mark size=3pt] coordinates {(55,55)};
    \addplot[color=green,only marks,mark=o,mark size=1pt] coordinates {
        (30,40) (45,23) (67,34) (12,56) (78,23) (34,67) (56,78) (89,45) (33,22) (67,88)
        (45,56) (23,45) (78,34) (34,56) (56,34) (89,23) (23,67) (67,12) (45,78) (78,56)
        (23,89) (56,67) (34,78) (78,45) (45,34) (89,67) (67,56) (34,12) (56,23) (12,45)
    };
    \legend{PU,PUEA,SUs}
    \end{axis}
\end{tikzpicture}
\hfill
\begin{tikzpicture}[scale=0.4]
    \begin{axis}[
        title={Scenario C: Close Distance},
        xlabel={X coordinate},
        ylabel={Y coordinate},
        xmin=0, xmax=100,
        ymin=0, ymax=100,
        grid=both,
        legend pos=north west
    ]
    \addplot[color=blue,mark=*,mark size=3pt] coordinates {(40,40)};
    \addplot[color=red,mark=*,mark size=3pt] coordinates {(55,55)};
    \addplot[color=green,only marks,mark=o,mark size=1pt] coordinates {
        (30,40) (45,23) (67,34) (12,56) (78,23) (34,67) (56,78) (89,45) (33,22) (67,88)
        (45,56) (23,45) (78,34) (34,56) (56,34) (89,23) (23,67) (67,12) (45,78) (78,56)
        (23,89) (56,67) (34,78) (78,45) (45,34) (89,67) (67,56) (34,12) (56,23) (12,45)
    };
    \legend{PU,PUEA,SUs}
    \end{axis}
\end{tikzpicture}
\caption{Network topology for the three scenarios showing PU, PUEA, and SU positions}
\label{fig:network_topology}
\end{figure}

\subsection{Signal Propagation Model}

We model the received signal power at each SU using the log-distance path loss model with shadowing. The received power $P_r$ (in dBm) at an SU located at distance $d$ from a transmitter is given by:

\begin{equation}
P_r = P_t - 10\alpha\log_{10}\left(\frac{d}{d_0}\right) - X_\sigma
\end{equation}

where:
\begin{itemize}
    \item $P_t$ is the transmitter power (PU: 15 dB, PUEA: 35 dB)
    \item $\alpha$ is the path loss exponent (set to 3.5 for our simulations)
    \item $d_0$ is the reference distance (set to 1 unit)
    \item $X_\sigma$ is the shadowing component, modeled as a zero-mean Gaussian random variable with standard deviation $\sigma$ (set to 5 dB)
\end{itemize}

\subsection{Simulation Setup}

For each scenario, we simulate 100 time slots with varying PUEA presence percentages: 10\%, 20\%, 30\%, 40\%, and 50\%. In each time slot, either the PU or the PUEA is active (based on the configured presence percentage), and the 30 SUs collect received power measurements. The simulation parameters are summarized in Table \ref{tab:simulation_parameters}.

\begin{table}[!t]
\caption{Simulation Parameters}
\label{tab:simulation_parameters}
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Area dimensions & 100 Ã— 100 units \\
Number of SUs & 30 \\
Number of time slots & 100 \\
PU transmit power & 15 dB \\
PUEA transmit power & 35 dB \\
Path loss exponent & 3.5 \\
Shadowing standard deviation & 5 dB \\
PUEA presence percentages & 10\%, 20\%, 30\%, 40\%, 50\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Feature Extraction}

For each time slot, we extract a set of statistical features from the received power measurements collected by all SUs. The feature vector includes:

\begin{itemize}
    \item Mean of received power across all SUs
    \item Variance of received power
    \item Median of received power
    \item Lower quartile (25th percentile)
    \item Upper quartile (75th percentile)
\end{itemize}

The feature extraction process is mathematically represented as follows:

\begin{equation}
\mathbf{F} = [F_{\text{mean}}, F_{\text{var}}, F_{\text{med}}, F_{\text{q1}}, F_{\text{q3}}]
\end{equation}

where $\mathbf{F}$ is the feature vector for a given time slot, and $F_{\text{mean}}$, $F_{\text{var}}$, $F_{\text{med}}$, $F_{\text{q1}}$, and $F_{\text{q3}}$ represent the mean, variance, median, lower quartile, and upper quartile of the received power measurements, respectively.

Algorithm \ref{alg:feature_extraction} details the feature extraction process.

\begin{algorithm}
\caption{Feature Extraction from Power Measurements}
\label{alg:feature_extraction}
\begin{algorithmic}[1]
\REQUIRE Power measurements $P = \{P_1, P_2, \ldots, P_n\}$ from $n$ SUs
\ENSURE Feature vector $\mathbf{F}$
\STATE $F_{\text{mean}} \gets \frac{1}{n}\sum_{i=1}^{n} P_i$
\STATE $F_{\text{var}} \gets \frac{1}{n}\sum_{i=1}^{n} (P_i - F_{\text{mean}})^2$
\STATE Sort $P$ in ascending order to get $P_{\text{sorted}}$
\STATE $F_{\text{med}} \gets$ median of $P_{\text{sorted}}$
\STATE $F_{\text{q1}} \gets$ 25th percentile of $P_{\text{sorted}}$
\STATE $F_{\text{q3}} \gets$ 75th percentile of $P_{\text{sorted}}$
\STATE $\mathbf{F} \gets [F_{\text{mean}}, F_{\text{var}}, F_{\text{med}}, F_{\text{q1}}, F_{\text{q3}}]$
\RETURN $\mathbf{F}$
\end{algorithmic}
\end{algorithm}

\section{Traditional Clustering Approach}
\label{sec:traditional_clustering}

This section describes our traditional clustering approach for PUEA detection, including distance matrix calculation, clustering algorithms, and detection methodology.

\subsection{Distance Matrix Calculation}

For clustering, we compute a distance matrix between feature vectors using the Manhattan distance (L1 norm). For two feature vectors $\mathbf{F}_i$ and $\mathbf{F}_j$, the Manhattan distance is calculated as:

\begin{equation}
d_{i,j} = \sum_{k=1}^{5} w_k |F_{i,k} - F_{j,k}|
\end{equation}

where $w_k$ is the weight assigned to the $k$-th feature. We determine these weights empirically based on feature importance analysis, with the following normalized values: $w_{\text{mean}} = 0.3$, $w_{\text{var}} = 0.2$, $w_{\text{med}} = 0.3$, $w_{\text{q1}} = 0.1$, and $w_{\text{q3}} = 0.1$.

Algorithm \ref{alg:distance_matrix} outlines the distance matrix calculation.

\begin{algorithm}
\caption{Distance Matrix Calculation}
\label{alg:distance_matrix}
\begin{algorithmic}[1]
\REQUIRE Feature vectors $\{\mathbf{F}_1, \mathbf{F}_2, \ldots, \mathbf{F}_m\}$ for $m$ time slots, feature weights $\mathbf{w} = [w_1, w_2, w_3, w_4, w_5]$
\ENSURE Distance matrix $D$
\STATE Initialize $D$ as an $m \times m$ matrix of zeros
\FOR{$i = 1$ to $m$}
    \FOR{$j = 1$ to $m$}
        \STATE $d_{i,j} \gets \sum_{k=1}^{5} w_k |F_{i,k} - F_{j,k}|$
        \STATE $D[i,j] \gets d_{i,j}$
    \ENDFOR
\ENDFOR
\RETURN $D$
\end{algorithmic}
\end{algorithm}

\subsection{Clustering Algorithms}

We implement and compare four clustering algorithms for PUEA detection:

\subsubsection{DBSCAN}
Density-Based Spatial Clustering of Applications with Noise (DBSCAN) identifies clusters based on density, automatically determining the number of clusters. We optimize the DBSCAN parameters (epsilon and min\_samples) for each scenario using an automated approach that selects the parameters yielding the highest silhouette score.

\subsubsection{K-means Clustering}
We implement K-means with $K=2$ (representing PU and PUEA clusters) and with multiple initializations (10 different random seeds) to avoid local optima. The best clustering result based on inertia (sum of squared distances to centroids) is selected.

\subsubsection{Agglomerative Clustering}
We apply agglomerative clustering with different linkage methods (ward, complete, and average) and select the method providing the highest silhouette score for each scenario.

\subsubsection{Spectral Clustering}
Spectral clustering transforms the feature space using eigendecomposition of the similarity matrix, followed by K-means clustering in the transformed space. We use the radial basis function (RBF) kernel for constructing the similarity matrix.

\subsection{Detection Methodology}

For each clustering algorithm, we follow a two-step process for PUEA detection:

\begin{enumerate}
    \item \textbf{Cluster Formation}: Apply the clustering algorithm to the feature vectors obtained from all time slots, forming two clusters.
    \item \textbf{Cluster Labeling}: Label clusters as "PU" or "PUEA" based on the known ground truth for a small subset (20\%) of the data, which serves as a reference set.
\end{enumerate}

Once clusters are formed and labeled, we classify each time slot according to its cluster assignment. Algorithm \ref{alg:traditional_detection} outlines the traditional clustering-based detection approach.

\begin{algorithm}
\caption{Traditional Clustering-Based Detection}
\label{alg:traditional_detection}
\begin{algorithmic}[1]
\REQUIRE Feature vectors $\{\mathbf{F}_1, \mathbf{F}_2, \ldots, \mathbf{F}_m\}$ for $m$ time slots, reference set $R$ with known labels, clustering algorithm $A$
\ENSURE Detection results $\{y_1, y_2, \ldots, y_m\}$
\STATE Compute distance matrix $D$ using Algorithm \ref{alg:distance_matrix}
\STATE Apply clustering algorithm $A$ to obtain cluster assignments $\{c_1, c_2, \ldots, c_m\}$
\STATE Count PU and PUEA labels in each cluster using reference set $R$
\IF{majority of PU reference samples are in cluster 0}
    \STATE Label cluster 0 as "PU" and cluster 1 as "PUEA"
\ELSE
    \STATE Label cluster 0 as "PUEA" and cluster 1 as "PU"
\ENDIF
\FOR{$i = 1$ to $m$}
    \IF{$c_i = 0$ and cluster 0 is labeled as "PU"}
        \STATE $y_i \gets$ "PU"
    \ELSIF{$c_i = 1$ and cluster 1 is labeled as "PU"}
        \STATE $y_i \gets$ "PU"
    \ELSE
        \STATE $y_i \gets$ "PUEA"
    \ENDIF
\ENDFOR
\RETURN $\{y_1, y_2, \ldots, y_m\}$
\end{algorithmic}
\end{algorithm}

\section{Enhanced Detection Approach}
\label{sec:enhanced_detection}

This section presents our enhanced detection approach that applies KNN and Means algorithms within established clusters.

\subsection{KNN Algorithm within Clusters}

Our KNN-within-clusters approach consists of the following steps:

\begin{enumerate}
    \item \textbf{Initial Clustering}: Apply a clustering algorithm (e.g., DBSCAN) to partition the feature vectors into clusters.
    \item \textbf{Cluster Labeling}: Label clusters as "PU" or "PUEA" as in the traditional approach.
    \item \textbf{KNN Classification}: For each cluster, apply KNN classification to refine the detection within the cluster:
    \begin{itemize}
        \item For each feature vector in a cluster, find its $K$ nearest neighbors within the cluster.
        \item Compare the feature vector to its neighbors using a weighted distance measure.
        \item If the distance exceeds a threshold, consider it a potential misclassification.
        \item Reassign the vector to the other cluster if appropriate.
    \end{itemize}
\end{enumerate}

Algorithm \ref{alg:knn_within_clusters} outlines the KNN-within-clusters approach.

\begin{algorithm}
\caption{KNN Algorithm within Clusters}
\label{alg:knn_within_clusters}
\begin{algorithmic}[1]
\REQUIRE Feature vectors $\{\mathbf{F}_1, \mathbf{F}_2, \ldots, \mathbf{F}_m\}$, initial cluster assignments $\{c_1, c_2, \ldots, c_m\}$, number of neighbors $K$, distance threshold $\theta$
\ENSURE Refined detection results $\{y_1, y_2, \ldots, y_m\}$
\STATE Initialize $\{y_1, y_2, \ldots, y_m\}$ based on initial cluster assignments
\FOR{each cluster $C$}
    \FOR{each feature vector $\mathbf{F}_i$ in cluster $C$}
        \STATE Find $K$ nearest neighbors $\{N_1, N_2, \ldots, N_K\}$ of $\mathbf{F}_i$ within cluster $C$
        \STATE Compute average distance $d_{\text{avg}} = \frac{1}{K}\sum_{j=1}^{K} d(\mathbf{F}_i, N_j)$
        \IF{$d_{\text{avg}} > \theta$}
            \STATE Find nearest feature vector $\mathbf{F}_{\text{near}}$ in the other cluster
            \IF{$d(\mathbf{F}_i, \mathbf{F}_{\text{near}}) < d_{\text{avg}}$}
                \STATE Reassign $\mathbf{F}_i$ to the other cluster
                \STATE Update $y_i$ accordingly
            \ENDIF
        \ENDIF
    \ENDFOR
\ENDFOR
\RETURN $\{y_1, y_2, \ldots, y_m\}$
\end{algorithmic}
\end{algorithm}

\subsection{Means Algorithm within Clusters}

The Means-within-clusters approach follows a similar procedure but uses a different refinement strategy:

\begin{enumerate}
    \item \textbf{Initial Clustering}: Apply a clustering algorithm to partition the feature vectors.
    \item \textbf{Cluster Means Calculation}: For each cluster, compute the mean feature vector.
    \item \textbf{Distance-based Refinement}: For each feature vector in a cluster:
    \begin{itemize}
        \item Compute its distance to the mean vector of its assigned cluster.
        \item Compute its distance to the mean vector of the other cluster.
        \item If it is closer to the mean of the other cluster by a certain margin, reassign it.
    \end{itemize}
\end{enumerate}

Algorithm \ref{alg:means_within_clusters} outlines the Means-within-clusters approach.

\begin{algorithm}
\caption{Means Algorithm within Clusters}
\label{alg:means_within_clusters}
\begin{algorithmic}[1]
\REQUIRE Feature vectors $\{\mathbf{F}_1, \mathbf{F}_2, \ldots, \mathbf{F}_m\}$, initial cluster assignments $\{c_1, c_2, \ldots, c_m\}$, reassignment threshold $\lambda$
\ENSURE Refined detection results $\{y_1, y_2, \ldots, y_m\}$
\STATE Initialize $\{y_1, y_2, \ldots, y_m\}$ based on initial cluster assignments
\STATE Compute mean feature vector $\mathbf{\mu}_0$ for cluster 0
\STATE Compute mean feature vector $\mathbf{\mu}_1$ for cluster 1
\FOR{$i = 1$ to $m$}
    \STATE Compute $d_0 = d(\mathbf{F}_i, \mathbf{\mu}_0)$ and $d_1 = d(\mathbf{F}_i, \mathbf{\mu}_1)$
    \IF{$c_i = 0$ and $d_1 < d_0 \times (1-\lambda)$}
        \STATE Reassign $\mathbf{F}_i$ to cluster 1
        \STATE Update $y_i$ accordingly
    \ELSIF{$c_i = 1$ and $d_0 < d_1 \times (1-\lambda)$}
        \STATE Reassign $\mathbf{F}_i$ to cluster 0
        \STATE Update $y_i$ accordingly
    \ENDIF
\ENDFOR
\RETURN $\{y_1, y_2, \ldots, y_m\}$
\end{algorithmic}
\end{algorithm}

\subsection{Parameter Selection and Optimization}

For the KNN-within-clusters approach, we optimize the number of neighbors $K$ and the distance threshold $\theta$ using cross-validation on a validation set. For the Means-within-clusters approach, we optimize the reassignment threshold $\lambda$ using the same validation approach.

\subsection{Weighted Feature Importance}

We enhance both approaches by incorporating weighted feature importance, where features are assigned different weights based on their discriminative power for separating PU and PUEA signals. We determine the optimal feature weights using a grid search approach that maximizes the F1 score on the validation set.

\section{Performance Analysis}
\label{sec:performance_analysis}

This section presents our methodology for analyzing and comparing the performance of traditional clustering and enhanced detection approaches.

\subsection{Evaluation Metrics}

We use the following metrics to evaluate detection performance:

\begin{itemize}
    \item \textbf{Detection Rate (DR)}: The proportion of actual PUEA instances correctly identified as PUEA.
    \begin{equation}
    DR = \frac{TP}{TP + FN}
    \end{equation}
    
    \item \textbf{False Alarm Rate (FAR)}: The proportion of actual PU instances incorrectly identified as PUEA.
    \begin{equation}
    FAR = \frac{FP}{FP + TN}
    \end{equation}
    
    \item \textbf{Precision}: The proportion of predicted PUEA instances that are actually PUEA.
    \begin{equation}
    \text{Precision} = \frac{TP}{TP + FP}
    \end{equation}
    
    \item \textbf{Recall}: Same as Detection Rate.
    \begin{equation}
    \text{Recall} = \frac{TP}{TP + FN}
    \end{equation}
    
    \item \textbf{F1 Score}: The harmonic mean of Precision and Recall.
    \begin{equation}
    F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
    \end{equation}
\end{itemize}

where $TP$ (True Positive) is the number of PUEA instances correctly identified as PUEA, $FP$ (False Positive) is the number of PU instances incorrectly identified as PUEA, $TN$ (True Negative) is the number of PU instances correctly identified as PU, and $FN$ (False Negative) is the number of PUEA instances incorrectly identified as PU.

\subsection{Performance Comparison Methodology}

We compare the performance of traditional clustering and enhanced detection approaches across different scenarios and PUEA presence percentages using the following methodology:

\begin{enumerate}
    \item For each scenario (A, B, C) and PUEA presence percentage (10\%, 20\%, 30\%, 40\%, 50\%):
    \begin{itemize}
        \item Apply each traditional clustering algorithm (DBSCAN, K-means, Agglomerative, Spectral)
        \item Apply each enhanced detection approach (KNN-within-clusters, Means-within-clusters)
        \item Compute all evaluation metrics for each approach
    \end{itemize}
    
    \item Calculate average performance metrics across all scenarios for each approach
    
    \item Quantify the improvement percentage for each metric from traditional clustering to enhanced detection:
    \begin{equation}
    \text{Improvement (\%)} = \frac{M_{\text{enhanced}} - M_{\text{traditional}}}{M_{\text{traditional}}} \times 100
    \end{equation}
    where $M$ represents a performance metric
    
    \item Conduct statistical significance testing (t-test and Wilcoxon signed-rank test) to determine if the performance difference between traditional and enhanced approaches is statistically significant
\end{enumerate}

\subsection{ROC-like Space Performance Visualization}

We visualize the performance of different detection approaches in a ROC-like space, plotting Detection Rate against False Alarm Rate. This visualization allows for easy comparison of different approaches, with the ideal detector positioned in the top-left corner (high DR, low FAR).

\section{Results and Discussion}
\label{sec:results_discussion}

This section presents the results of our experimental evaluation and discusses the key findings.

\subsection{Overall Performance Comparison}

Table \ref{tab:overall_performance} summarizes the average performance metrics across all scenarios and PUEA presence percentages for traditional clustering and enhanced detection approaches.

\begin{table}[!t]
\caption{Overall Performance Comparison}
\label{tab:overall_performance}
\centering
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Approach} & \textbf{DR} & \textbf{FAR} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} \\
\midrule
DBSCAN & 0.XX & 0.XX & 0.XX & 0.XX & 0.XX \\
K-means & 0.XX & 0.XX & 0.XX & 0.XX & 0.XX \\
Agglomerative & 0.XX & 0.XX & 0.XX & 0.XX & 0.XX \\
Spectral & 0.XX & 0.XX & 0.XX & 0.XX & 0.XX \\
KNN-within-DBSCAN & 0.XX & 0.XX & 0.XX & 0.XX & 0.XX \\
KNN-within-Kmeans & 0.XX & 0.XX & 0.XX & 0.XX & 0.XX \\
Means-within-DBSCAN & 0.XX & 0.XX & 0.XX & 0.XX & 0.XX \\
Means-within-Kmeans & 0.XX & 0.XX & 0.XX & 0.XX & 0.XX \\
\bottomrule
\end{tabular}
\end{table}

As shown in Table \ref{tab:overall_performance}, our enhanced detection approaches consistently outperform traditional clustering algorithms across all metrics. The KNN-within-DBSCAN approach achieves the highest detection rate and F1 score, while the Means-within-Kmeans approach results in the lowest false alarm rate.

\subsection{Scenario-Specific Findings}

Fig. \ref{fig:scenario_comparison} illustrates the detection performance across different scenarios, showing how the spatial separation between PU and PUEA affects detection accuracy.

\begin{figure}[!t]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar,
    bar width=0.15cm,
    width=\columnwidth,
    height=8cm,
    legend style={at={(0.5,-0.2)}, anchor=north, legend columns=4},
    ylabel={Detection Rate},
    symbolic x coords={A (Far), B (Medium), C (Close)},
    xtick=data,
    nodes near coords,
    nodes near coords align={vertical},
    ymin=0, ymax=1,
    enlarge x limits=0.25
    ]
\addplot coordinates {(A (Far), 0.XX) (B (Medium), 0.XX) (C (Close), 0.XX)};
\addplot coordinates {(A (Far), 0.XX) (B (Medium), 0.XX) (C (Close), 0.XX)};
\addplot coordinates {(A (Far), 0.XX) (B (Medium), 0.XX) (C (Close), 0.XX)};
\addplot coordinates {(A (Far), 0.XX) (B (Medium), 0.XX) (C (Close), 0.XX)};
\legend{DBSCAN, K-means, KNN-within-DBSCAN, Means-within-Kmeans}
\end{axis}
\end{tikzpicture}
\caption{Detection rate comparison across different scenarios}
\label{fig:scenario_comparison}
\end{figure}

The key findings from scenario-specific analysis include:

\begin{itemize}
    \item \textbf{Scenario A (Far Distance)}: All approaches perform well when PU and PUEA are spatially well-separated, with detection rates above 0.XX for both traditional and enhanced approaches.
    
    \item \textbf{Scenario B (Medium Distance)}: Enhanced detection approaches show a clear advantage over traditional clustering, with KNN-within-DBSCAN achieving a XX\% higher detection rate than DBSCAN alone.
    
    \item \textbf{Scenario C (Close Distance)}: This most challenging scenario demonstrates the greatest performance gap between traditional and enhanced approaches. Traditional clustering struggles with detection rates below 0.XX, while enhanced approaches maintain detection rates above 0.XX.
\end{itemize}

\subsection{Impact of PUEA Presence Percentage}

Fig. \ref{fig:puea_percentage_impact} shows how detection performance varies with different PUEA presence percentages across all scenarios.

\begin{figure}[!t]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=\columnwidth,
    height=8cm,
    grid=both,
    xlabel={PUEA Presence Percentage},
    ylabel={F1 Score},
    xmin=10, xmax=50,
    xtick={10,20,30,40,50},
    legend style={at={(0.98,0.02)}, anchor=south east},
    ]
\addplot[color=blue,mark=square] coordinates {(10,0.XX) (20,0.XX) (30,0.XX) (40,0.XX) (50,0.XX)};
\addplot[color=red,mark=triangle] coordinates {(10,0.XX) (20,0.XX) (30,0.XX) (40,0.XX) (50,0.XX)};
\addplot[color=green,mark=*] coordinates {(10,0.XX) (20,0.XX) (30,0.XX) (40,0.XX) (50,0.XX)};
\addplot[color=black,mark=diamond] coordinates {(10,0.XX) (20,0.XX) (30,0.XX) (40,0.XX) (50,0.XX)};
\legend{DBSCAN, K-means, KNN-within-DBSCAN, Means-within-Kmeans}
\end{axis}
\end{tikzpicture}
\caption{F1 Score variation with different PUEA presence percentages}
\label{fig:puea_percentage_impact}
\end{figure}

Our analysis reveals that:

\begin{itemize}
    \item Detection performance generally improves with increasing PUEA presence percentage for all approaches.
    \item Enhanced detection approaches maintain more consistent performance across different PUEA percentages, showing greater robustness.
    \item The performance gap between traditional and enhanced approaches is most pronounced at lower PUEA presence percentages (10-20\%), where enhanced approaches offer up to XX\% improvement in F1 score.
\end{itemize}

\subsection{Statistical Significance Testing}

We conducted paired t-tests and Wilcoxon signed-rank tests to assess the statistical significance of performance differences between traditional clustering and enhanced detection approaches. The results are presented in Table \ref{tab:statistical_significance}.

\begin{table}[!t]
\caption{Statistical Significance Testing Results}
\label{tab:statistical_significance}
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Comparison} & \textbf{T-test p-value} & \textbf{Wilcoxon p-value} \\
\midrule
DBSCAN vs. KNN-within-DBSCAN & 0.00X & 0.00X \\
K-means vs. KNN-within-Kmeans & 0.00X & 0.00X \\
DBSCAN vs. Means-within-DBSCAN & 0.00X & 0.00X \\
K-means vs. Means-within-Kmeans & 0.00X & 0.00X \\
\bottomrule
\end{tabular}
\end{table}

All p-values are well below the significance threshold of 0.05, confirming that the performance improvements achieved by our enhanced detection approaches are statistically significant.

\subsection{Computational Complexity Considerations}

While enhanced detection approaches offer superior performance, they also incur additional computational cost. Table \ref{tab:computational_complexity} compares the average execution time for different approaches, normalized relative to the DBSCAN execution time.

\begin{table}[!t]
\caption{Normalized Computational Complexity}
\label{tab:computational_complexity}
\centering
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Approach} & \textbf{Normalized Execution Time} \\
\midrule
DBSCAN & 1.00 \\
K-means & 0.XX \\
Agglomerative & X.XX \\
Spectral & X.XX \\
KNN-within-DBSCAN & X.XX \\
KNN-within-Kmeans & X.XX \\
Means-within-DBSCAN & X.XX \\
Means-within-Kmeans & X.XX \\
\bottomrule
\end{tabular}
\end{table}

The Means-within-clusters approach offers the best balance between performance improvement and computational overhead, making it particularly suitable for resource-constrained scenarios.

\subsection{Practical Implementation Recommendations}

Based on our experimental results, we recommend:

\begin{itemize}
    \item For scenarios with sufficient computational resources: Use KNN-within-DBSCAN, which offers the highest detection accuracy across all scenarios.
    \item For resource-constrained scenarios: Use Means-within-Kmeans, which provides substantial performance improvement with minimal computational overhead.
    \item For scenarios with close proximity between PU and PUEA: Prioritize enhanced detection approaches, as traditional clustering performs poorly in these challenging conditions.
\end{itemize}

\section{Conclusion and Future Work}
\label{sec:conclusion}

This paper presented a comprehensive comparative analysis of traditional clustering and enhanced detection approaches for PUEA detection in cognitive radio networks. Our key findings include:

\begin{itemize}
    \item Enhanced detection approaches (KNN-within-clusters and Means-within-clusters) consistently outperform traditional clustering algorithms across all scenarios and PUEA presence percentages.
    \item Performance improvement is most significant in challenging scenarios with close spatial separation between PU and PUEA.
    \item The Means-within-Kmeans approach offers the best balance between detection performance and computational complexity.
    \item All performance improvements achieved by enhanced approaches are statistically significant.
\end{itemize}

The practical implications of our work include improved security for cognitive radio networks through more accurate PUEA detection, particularly in challenging scenarios where traditional approaches struggle.

\subsection{Limitations and Future Work}

While our approach demonstrates significant improvements, several limitations and future research directions remain:

\begin{itemize}
    \item Our current approach assumes a static network topology. Future work should investigate dynamic scenarios with mobile nodes.
    \item We considered only a single PU and PUEA. Extending the approach to multiple PUs and attackers would enhance practical applicability.
    \item Additional features derived from time-domain and frequency-domain analysis could further improve detection performance.
    \item Integration with other security mechanisms, such as authentication protocols, could provide a more comprehensive security framework.
    \item Application of deep learning techniques for feature extraction and classification represents a promising direction for future research.
\end{itemize}

In conclusion, our enhanced detection approach represents a significant advancement in PUEA detection for cognitive radio networks, offering substantial performance improvements over traditional clustering methods, particularly in challenging scenarios with close spatial separation between legitimate users and attackers.

\bibliographystyle{IEEEtran}
\begin{thebibliography}{00}

\bibitem{akyildiz2006next}
I.~F. Akyildiz, W.~Y. Lee, M.~C. Vuran, and S.~Mohanty, ``Next generation/dynamic spectrum access/cognitive radio wireless networks: A survey,'' \emph{Computer Networks}, vol. 50, no. 13, pp. 2127--2159, 2006.

\bibitem{chen2008robust}
R.~Chen, J.-M. Park, and J.~H. Reed, ``Defense against primary user emulation attacks in cognitive radio networks,'' \emph{IEEE Journal on Selected Areas in Communications}, vol. 26, no. 1, pp. 25--37, 2008.

\bibitem{chen2008defense}
R.~Chen, J.-M. Park, and J.~H. Reed, ``Defense against primary user emulation attacks in cognitive radio networks,'' \emph{IEEE Journal on Selected Areas in Communications}, vol. 26, no. 1, pp. 25--37, 2008.

\bibitem{liu2010aldo}
Y.~Liu, P.~Ning, and H.~Dai, ``Authenticating primary users' signals in cognitive radio networks via integrated cryptographic and wireless link signatures,'' \emph{IEEE Symposium on Security and Privacy}, pp. 286--301, 2010.

\bibitem{jin2010cyclostationarity}
Z.~Jin, S.~Anand, and K.~P. Subbalakshmi, ``Detecting primary user emulation attacks in dynamic spectrum access networks,'' \emph{IEEE International Conference on Communications}, pp. 1--5, 2010.

\bibitem{huang2013cross}
L.~Huang, L.~Xie, H.~Yu, W.~Wang, and Y.~Liu, ``Robust collaborative spectrum sensing in the presence of primary user emulation attacks,'' \emph{IEEE Transactions on Communications}, vol. 61, no. 8, pp. 3566--3578, 2013.

\bibitem{wang2018deep}
Y.~Wang, Z.~Li, P.~Zhang, and B.~Zhang, ``Primary user emulation attack detection in cognitive radio using machine learning techniques,'' \emph{IEEE Access}, vol. 6, pp. 73595--73605, 2018.

\bibitem{kim2018machine}
W.~Kim, H.~Jang, and D.~Hong, ``Robust detection of false data injection attacks in smart grids using deep learning,'' \emph{IEEE Access}, vol. 6, pp. 58688--58698, 2018.

\bibitem{shah2018clustering}
N.~Shah and S.~Sastry, ``Proactive transmission power control for primary user emulation attacks detection in cognitive radio networks,'' \emph{IET Communications}, vol. 12, no. 14, pp. 1702--1710, 2018.

\bibitem{rajendran2019secure}
S.~Rajendran, W.~Meert, V.~Lenders, and S.~Pollin, ``SAIFE: Unsupervised wireless spectrum anomaly detection with interpretable features,'' \emph{IEEE International Symposium on Dynamic Spectrum Access Networks}, pp. 1--9, 2019.

\bibitem{luo2019clustering}
B.~Luo, L.~Li, and Q.~Li, ``A novel clustering-based feature extraction method for PUEA detection,'' \emph{IEEE Communications Letters}, vol. 23, no. 10, pp. 1722--1726, 2019.

\end{thebibliography}

\end{document}
