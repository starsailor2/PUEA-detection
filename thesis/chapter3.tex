\chapter{Methodology}

\section{Overview of the Proposed Approach}
This chapter presents the methodology for detecting Primary User Emulation Attacks (PUEA) in Cognitive Radio Networks using clustering-based approaches. The proposed methodology consists of four main stages: (1) dataset generation, (2) feature extraction and preprocessing, (3) clustering-based detection, and (4) performance evaluation. Figure \ref{fig:methodology} illustrates the overall architecture of the proposed approach.

% You would need to add a figure here showing the methodology framework

\section{Dataset Generation}
Since real-world datasets for PUEA detection are limited, we generate a synthetic dataset that reflects the signal propagation characteristics of both legitimate primary users (PUs) and PUEAs. The dataset generation process considers various signal propagation factors that affect the received signal power at secondary users (SUs).

\subsection{Signal Propagation Model}
The received power at a secondary user from a transmitter (either a legitimate PU or a PUEA) can be modeled using the log-normal shadowing path loss model \cite{rappaport1996wireless}:

\begin{equation}
P_r = P_t - 10 \alpha \log_{10}\left(\frac{d}{d_0}\right) + X_\sigma
\end{equation}

where:
\begin{itemize}
    \item $P_r$ is the received power at the secondary user (in dBm)
    \item $P_t$ is the transmit power (in dBm)
    \item $\alpha$ is the path loss exponent
    \item $d$ is the distance between the transmitter and receiver (in meters)
    \item $d_0$ is the reference distance (typically 1 meter)
    \item $X_\sigma$ is the shadowing effect, modeled as a zero-mean Gaussian random variable with standard deviation $\sigma$ (in dB)
\end{itemize}

\subsection{Dataset Parameters}
Our dataset consists of five key components for each data point:
\begin{enumerate}
    \item Transmitted power ($P_t$): The power at which the signal is transmitted by either a PU or PUEA.
    \item Distance ($d$): The distance between the transmitter (PU or PUEA) and the receiving secondary user.
    \item Path loss exponent ($\alpha$): Characterizes the environment in which the signal propagates.
    \item Shadowing effect ($X_\sigma$): Represents the random fluctuations in received signal power due to obstacles.
    \item Received power ($P_r$): The power of the signal as measured by the secondary user.
\end{enumerate}

\subsection{Generating PU and PUEA Signals}
To create a realistic dataset, we consider the following approach:

\begin{enumerate}
    \item \textbf{Legitimate PU signals}: 
    \begin{itemize}
        \item Transmit power: Fixed at a standard level (e.g., 30 dBm)
        \item Location: Placed at known, fixed locations
        \item Path loss exponent: Based on the environment (e.g., 2 for free space, 4-6 for urban environments)
    \end{itemize}
    
    \item \textbf{PUEA signals}: 
    \begin{itemize}
        \item Transmit power: Variable, potentially higher than PUs to maximize attack impact
        \item Location: Random locations, different from PU locations
        \item Path loss exponent: Similar to PUs but with potential variations
    \end{itemize}
\end{enumerate}

The following Python code generates the synthetic dataset:

\begin{lstlisting}[language=Python, caption=Dataset Generation Code]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# Set random seed for reproducibility
np.random.seed(42)

# Function to calculate received power using the log-normal shadowing model
def calculate_received_power(transmit_power, distance, path_loss_exponent, shadowing_std):
    # Reference distance (in meters)
    d0 = 1.0
    
    # Calculate path loss
    path_loss = 10 * path_loss_exponent * np.log10(distance / d0)
    
    # Generate random shadowing effect
    shadowing = np.random.normal(0, shadowing_std)
    
    # Calculate received power
    received_power = transmit_power - path_loss + shadowing
    
    return received_power

# Parameters for generating the dataset
num_pu_samples = 300  # Number of legitimate PU signal samples
num_puea_samples = 150  # Number of PUEA signal samples

# Parameters for legitimate PU signals
pu_transmit_power_mean = 30  # dBm
pu_transmit_power_std = 1  # Small variation in transmit power
pu_distance_mean = 500  # meters
pu_distance_std = 100
pu_path_loss_exponent_mean = 3.0
pu_path_loss_exponent_std = 0.2
pu_shadowing_std = 4  # dB

# Parameters for PUEA signals
puea_transmit_power_mean = 35  # dBm (higher than PU)
puea_transmit_power_std = 2
puea_distance_mean = 450  # meters (potentially closer to SUs)
puea_distance_std = 120
puea_path_loss_exponent_mean = 2.8  # Potentially different environment
puea_path_loss_exponent_std = 0.3
puea_shadowing_std = 5  # dB (potentially more variable)

# Generate legitimate PU signal data
pu_transmit_power = np.random.normal(pu_transmit_power_mean, pu_transmit_power_std, num_pu_samples)
pu_distance = np.random.normal(pu_distance_mean, pu_distance_std, num_pu_samples)
pu_path_loss_exponent = np.random.normal(pu_path_loss_exponent_mean, pu_path_loss_exponent_std, num_pu_samples)
pu_shadowing = np.random.normal(0, pu_shadowing_std, num_pu_samples)

# Calculate received power for legitimate PU signals
pu_received_power = np.array([calculate_received_power(pt, d, alpha, pu_shadowing_std) 
                             for pt, d, alpha in zip(pu_transmit_power, pu_distance, pu_path_loss_exponent)])

# Generate PUEA signal data
puea_transmit_power = np.random.normal(puea_transmit_power_mean, puea_transmit_power_std, num_puea_samples)
puea_distance = np.random.normal(puea_distance_mean, puea_distance_std, num_puea_samples)
puea_path_loss_exponent = np.random.normal(puea_path_loss_exponent_mean, puea_path_loss_exponent_std, num_puea_samples)
puea_shadowing = np.random.normal(0, puea_shadowing_std, num_puea_samples)

# Calculate received power for PUEA signals
puea_received_power = np.array([calculate_received_power(pt, d, alpha, puea_shadowing_std) 
                               for pt, d, alpha in zip(puea_transmit_power, puea_distance, puea_path_loss_exponent)])

# Create dataframes for PU and PUEA signals
pu_df = pd.DataFrame({
    'transmit_power': pu_transmit_power,
    'distance': pu_distance,
    'path_loss_exponent': pu_path_loss_exponent,
    'shadowing_effect': pu_shadowing,
    'received_power': pu_received_power,
    'label': 0  # 0 for legitimate PU
})

puea_df = pd.DataFrame({
    'transmit_power': puea_transmit_power,
    'distance': puea_distance,
    'path_loss_exponent': puea_path_loss_exponent,
    'shadowing_effect': puea_shadowing,
    'received_power': puea_received_power,
    'label': 1  # 1 for PUEA
})

# Combine the dataframes
dataset = pd.concat([pu_df, puea_df], ignore_index=True)

# Shuffle the dataset
dataset = dataset.sample(frac=1).reset_index(drop=True)

# Save the dataset to a CSV file
dataset.to_csv('puea_detection_dataset.csv', index=False)

print(f"Dataset generated with {len(dataset)} samples")
print(f"PU signals: {num_pu_samples}, PUEA signals: {num_puea_samples}")
print(f"Features: {', '.join(dataset.columns[:-1])}")
\end{lstlisting}

\section{Feature Preprocessing}
Before applying clustering algorithms, we preprocess the dataset to ensure optimal performance. The preprocessing steps include:

\subsection{Feature Scaling}
Since the features in our dataset have different scales and units, we apply standardization to transform them to have zero mean and unit variance. This ensures that features with larger scales do not dominate the clustering process.

\begin{lstlisting}[language=Python, caption=Feature Scaling Code]
# Load the dataset
dataset = pd.read_csv('puea_detection_dataset.csv')

# Extract features and labels
X = dataset.drop('label', axis=1)
y_true = dataset['label']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convert back to DataFrame for easier handling
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)
\end{lstlisting}

\subsection{Feature Selection}
Although our dataset has only five features, it is important to identify the most discriminative ones for clustering. We can use principal component analysis (PCA) for dimensionality reduction or feature importance analysis to select the most relevant features.

\begin{lstlisting}[language=Python, caption=Feature Selection Code]
from sklearn.decomposition import PCA

# Apply PCA for dimensionality reduction
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Percentage of variance explained by each component
explained_variance = pca.explained_variance_ratio_
print(f"Explained variance ratio: {explained_variance}")
print(f"Total explained variance: {sum(explained_variance):.2f}")

# Plot PCA results to visualize data distribution
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_true, alpha=0.6, cmap='viridis')
plt.colorbar(scatter, label='Class (0: PU, 1: PUEA)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of PUEA Detection Dataset')
plt.grid(alpha=0.3)
plt.savefig('pca_visualization.png', dpi=300, bbox_inches='tight')
plt.show()
\end{lstlisting}

\section{Clustering-Based Detection}
We apply three clustering algorithms—DBSCAN, Agglomerative Hierarchical Clustering, and K-means—to identify and separate legitimate PU signals from PUEA signals.

\subsection{K-means Clustering}
K-means is a partition-based clustering algorithm that divides the dataset into K distinct, non-overlapping clusters. For our PUEA detection problem, we set K=2 to separate the dataset into two clusters: one for legitimate PUs and one for PUEAs.

\begin{lstlisting}[language=Python, caption=K-means Clustering Code]
from sklearn.cluster import KMeans
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

# Apply K-means clustering
kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
kmeans_labels = kmeans.fit_predict(X_scaled)

# Map cluster labels to original classes (may need adjustment based on results)
# Since we know there should be two clusters (PU and PUEA)
# We need to check if the assigned labels match the true labels
if (kmeans_labels == y_true).mean() < 0.5:
    kmeans_labels = 1 - kmeans_labels

# Evaluate clustering performance
kmeans_accuracy = accuracy_score(y_true, kmeans_labels)
kmeans_conf_matrix = confusion_matrix(y_true, kmeans_labels)

print(f"K-means Accuracy: {kmeans_accuracy:.4f}")
print("K-means Confusion Matrix:")
print(kmeans_conf_matrix)
print("K-means Classification Report:")
print(classification_report(y_true, kmeans_labels))

# Visualize the clustering results
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, alpha=0.6, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], 
            c='red', marker='X', s=200, label='Cluster Centers')
plt.colorbar(scatter, label='Cluster Label')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('K-means Clustering Results')
plt.legend()
plt.grid(alpha=0.3)
plt.savefig('kmeans_clustering.png', dpi=300, bbox_inches='tight')
plt.show()
\end{lstlisting}

\subsection{DBSCAN Clustering}
DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together points that are closely packed while marking points in low-density regions as outliers. DBSCAN is particularly suitable for PUEA detection as it can identify anomalous signals without requiring the number of clusters to be specified in advance.

\begin{lstlisting}[language=Python, caption=DBSCAN Clustering Code]
from sklearn.cluster import DBSCAN

# Apply DBSCAN clustering
# The eps and min_samples parameters need to be tuned for optimal performance
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_scaled)

# DBSCAN labels outliers as -1, we need to map these properly
# Assuming outliers are PUEAs (label 1) and the main cluster is PUs (label 0)
dbscan_mapped_labels = np.where(dbscan_labels == -1, 1, 0)

# Check if we need to invert the labels
if (dbscan_mapped_labels == y_true).mean() < 0.5:
    dbscan_mapped_labels = 1 - dbscan_mapped_labels

# Evaluate clustering performance
dbscan_accuracy = accuracy_score(y_true, dbscan_mapped_labels)
dbscan_conf_matrix = confusion_matrix(y_true, dbscan_mapped_labels)

print(f"DBSCAN Accuracy: {dbscan_accuracy:.4f}")
print("DBSCAN Confusion Matrix:")
print(dbscan_conf_matrix)
print("DBSCAN Classification Report:")
print(classification_report(y_true, dbscan_mapped_labels))

# Visualize the clustering results
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=dbscan_mapped_labels, alpha=0.6, cmap='viridis')
plt.colorbar(scatter, label='Cluster Label')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('DBSCAN Clustering Results')
plt.grid(alpha=0.3)
plt.savefig('dbscan_clustering.png', dpi=300, bbox_inches='tight')
plt.show()
\end{lstlisting}

\subsection{Agglomerative Hierarchical Clustering}
Agglomerative Hierarchical Clustering starts with each data point as a singleton cluster and then progressively merges pairs of clusters until all points are in a single cluster. By cutting the dendrogram at an appropriate level, we can obtain the desired number of clusters (in our case, two).

\begin{lstlisting}[language=Python, caption=Agglomerative Clustering Code]
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# Apply Agglomerative Clustering
agg_clustering = AgglomerativeClustering(n_clusters=2, linkage='ward')
agg_labels = agg_clustering.fit_predict(X_scaled)

# Check if we need to invert the labels
if (agg_labels == y_true).mean() < 0.5:
    agg_labels = 1 - agg_labels

# Evaluate clustering performance
agg_accuracy = accuracy_score(y_true, agg_labels)
agg_conf_matrix = confusion_matrix(y_true, agg_labels)

print(f"Agglomerative Clustering Accuracy: {agg_accuracy:.4f}")
print("Agglomerative Clustering Confusion Matrix:")
print(agg_conf_matrix)
print("Agglomerative Clustering Classification Report:")
print(classification_report(y_true, agg_labels))

# Create linkage matrix for dendrogram visualization
# We'll use only a subset of data for clearer visualization
subset_indices = np.random.choice(len(X_scaled), size=100, replace=False)
X_subset = X_scaled[subset_indices]
y_subset = y_true.iloc[subset_indices]

# Calculate linkage matrix
linkage_matrix = linkage(X_subset, method='ward')

# Plot dendrogram
plt.figure(figsize=(12, 8))
dendrogram(linkage_matrix)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.savefig('hierarchical_dendrogram.png', dpi=300, bbox_inches='tight')
plt.show()

# Visualize the clustering results
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=agg_labels, alpha=0.6, cmap='viridis')
plt.colorbar(scatter, label='Cluster Label')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Agglomerative Clustering Results')
plt.grid(alpha=0.3)
plt.savefig('agglomerative_clustering.png', dpi=300, bbox_inches='tight')
plt.show()
\end{lstlisting}

\section{Iterative Clustering Method}
To improve the detection accuracy, we propose an iterative clustering method that combines the strengths of different clustering algorithms. The basic idea is to apply one clustering algorithm first, use its results to identify potential PUEAs, and then apply another algorithm to refine the results.

\begin{lstlisting}[language=Python, caption=Iterative Clustering Method]
def iterative_clustering(X, initial_clustering_method='kmeans', refinement_method='dbscan'):
    """
    Perform iterative clustering for improved PUEA detection
    
    Parameters:
        X (array): Feature matrix
        initial_clustering_method (str): Initial clustering method ('kmeans', 'dbscan', or 'agglomerative')
        refinement_method (str): Method for refinement ('kmeans', 'dbscan', or 'agglomerative')
    
    Returns:
        labels (array): Final cluster assignments
    """
    # Step 1: Initial clustering
    if initial_clustering_method == 'kmeans':
        initial_model = KMeans(n_clusters=2, random_state=42, n_init=10)
        initial_labels = initial_model.fit_predict(X)
    elif initial_clustering_method == 'dbscan':
        initial_model = DBSCAN(eps=0.5, min_samples=5)
        initial_labels = initial_model.fit_predict(X)
        # Map DBSCAN outliers (-1) to label 1
        initial_labels = np.where(initial_labels == -1, 1, 0)
    elif initial_clustering_method == 'agglomerative':
        initial_model = AgglomerativeClustering(n_clusters=2, linkage='ward')
        initial_labels = initial_model.fit_predict(X)
    
    # Step 2: Identify potential outliers
    # Assuming smaller cluster is more likely to be PUEA
    if np.sum(initial_labels == 0) < np.sum(initial_labels == 1):
        potential_puea_indices = np.where(initial_labels == 0)[0]
        potential_pu_indices = np.where(initial_labels == 1)[0]
    else:
        potential_puea_indices = np.where(initial_labels == 1)[0]
        potential_pu_indices = np.where(initial_labels == 0)[0]
    
    # Step 3: Refinement using another clustering method
    final_labels = np.zeros(len(X))
    
    # Mark PU cluster from initial clustering
    final_labels[potential_pu_indices] = 0
    
    # Apply refinement clustering only on potential PUEA samples
    X_potential_puea = X[potential_puea_indices]
    
    if len(X_potential_puea) > 0:
        if refinement_method == 'kmeans':
            refinement_model = KMeans(n_clusters=2, random_state=42, n_init=10)
        elif refinement_method == 'dbscan':
            refinement_model = DBSCAN(eps=0.4, min_samples=3)  # Tighter parameters for refinement
        elif refinement_method == 'agglomerative':
            refinement_model = AgglomerativeClustering(n_clusters=2, linkage='ward')
        
        refinement_labels = refinement_model.fit_predict(X_potential_puea)
        
        # Map refinement labels and assign to final labels
        if refinement_method == 'dbscan':
            # DBSCAN outliers (-1) are definitely PUEAs
            final_labels[potential_puea_indices[refinement_labels == -1]] = 1
            
            # For non-outliers, pick the smaller cluster as PUEA (more conservative approach)
            non_outlier_indices = potential_puea_indices[refinement_labels != -1]
            if len(non_outlier_indices) > 0:
                non_outlier_labels = refinement_labels[refinement_labels != -1]
                if np.sum(non_outlier_labels == 0) < np.sum(non_outlier_labels == 1):
                    final_labels[potential_puea_indices[refinement_labels == 0]] = 1
                else:
                    final_labels[potential_puea_indices[refinement_labels == 1]] = 1
        else:
            # For K-means and Agglomerative, pick the smaller cluster as PUEA
            if np.sum(refinement_labels == 0) < np.sum(refinement_labels == 1):
                final_labels[potential_puea_indices[refinement_labels == 0]] = 1
            else:
                final_labels[potential_puea_indices[refinement_labels == 1]] = 1
    
    return final_labels

# Apply iterative clustering
iterative_labels = iterative_clustering(X_scaled, 'kmeans', 'dbscan')

# Evaluate performance
iterative_accuracy = accuracy_score(y_true, iterative_labels)
iterative_conf_matrix = confusion_matrix(y_true, iterative_labels)

print(f"Iterative Clustering Accuracy: {iterative_accuracy:.4f}")
print("Iterative Clustering Confusion Matrix:")
print(iterative_conf_matrix)
print("Iterative Clustering Classification Report:")
print(classification_report(y_true, iterative_labels))

# Visualize the clustering results
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=iterative_labels, alpha=0.6, cmap='viridis')
plt.colorbar(scatter, label='Cluster Label')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Iterative Clustering Results')
plt.grid(alpha=0.3)
plt.savefig('iterative_clustering.png', dpi=300, bbox_inches='tight')
plt.show()
\end{lstlisting}

\section{Performance Evaluation Metrics}
To evaluate the performance of the different clustering methods, we use the following metrics:

\begin{enumerate}
    \item \textbf{Accuracy}: The proportion of correctly classified instances.
    \item \textbf{Precision}: The proportion of correctly identified PUEAs among all instances classified as PUEAs.
    \item \textbf{Recall}: The proportion of correctly identified PUEAs among all actual PUEA instances.
    \item \textbf{F1-score}: The harmonic mean of precision and recall.
    \item \textbf{Confusion Matrix}: Shows the number of true positives, false positives, true negatives, and false negatives.
\end{enumerate}

\begin{lstlisting}[language=Python, caption=Performance Comparison Code]
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Collect performance metrics for all methods
methods = ['K-means', 'DBSCAN', 'Agglomerative', 'Iterative']
labels = [kmeans_labels, dbscan_mapped_labels, agg_labels, iterative_labels]

accuracies = [accuracy_score(y_true, label) for label in labels]
precisions = [precision_score(y_true, label) for label in labels]
recalls = [recall_score(y_true, label) for label in labels]
f1_scores = [f1_score(y_true, label) for label in labels]

# Plot performance comparison
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score']
values = np.array([accuracies, precisions, recalls, f1_scores])

x = np.arange(len(metrics))
width = 0.2
multiplier = 0

fig, ax = plt.subplots(figsize=(12, 8))

for i, method in enumerate(methods):
    offset = width * multiplier
    rects = ax.bar(x + offset, values[:, i], width, label=method)
    multiplier += 1

ax.set_ylabel('Score')
ax.set_title('Performance Comparison of Clustering Methods')
ax.set_xticks(x + width * (len(methods) - 1) / 2)
ax.set_xticklabels(metrics)
ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=len(methods))
ax.set_ylim(0, 1.1)

# Add value labels on bars
for i in range(len(metrics)):
    for j in range(len(methods)):
        ax.text(i + width * j - width/2, values[i, j] + 0.01, 
                f'{values[i, j]:.2f}', ha='center', va='bottom', fontsize=8)

plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.savefig('performance_comparison.png', dpi=300, bbox_inches='tight')
plt.show()
\end{lstlisting}

\section{Summary}
In this chapter, we presented a comprehensive methodology for detecting Primary User Emulation Attacks in Cognitive Radio Networks. Our approach begins with generating a realistic dataset that captures various signal propagation factors affecting both legitimate PU and PUEA signals. We then preprocess the data using standardization and optionally apply dimensionality reduction techniques.

For detection, we explored three clustering algorithms—K-means, DBSCAN, and Agglomerative Hierarchical Clustering—each with its strengths and limitations. To leverage the advantages of different algorithms, we proposed an iterative clustering method that combines multiple techniques for improved detection accuracy.

The performance evaluation metrics enable us to compare the effectiveness of different clustering approaches and determine which method is most suitable for PUEA detection in various scenarios. The methodology described in this chapter provides a foundation for implementing and evaluating the proposed PUEA detection system, which will be presented in the next chapter.

% Add references section
\begin{thebibliography}{99}
\bibitem{rappaport1996wireless} Rappaport, T. S. (1996). Wireless communications: principles and practice. Prentice Hall.
\end{thebibliography}
