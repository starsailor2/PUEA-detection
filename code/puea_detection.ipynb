{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((point1 - point2)**2))\n",
    "\n",
    "def calculate_received_power(tx_power, distance, path_loss_exp, shadowing):\n",
    "    path_loss = 10 * path_loss_exp * np.log10(distance)\n",
    "    received_power = tx_power - path_loss + shadowing\n",
    "    return received_power\n",
    "\n",
    "def extract_enhanced_features(received_powers):\n",
    "    \"\"\"\n",
    "    Extract basic statistical features from received power measurements\n",
    "    to distinguish between PU and PUEA signals.\n",
    "\n",
    "    Parameters:\n",
    "        received_powers: Array of received power measurements\n",
    "\n",
    "    Returns:\n",
    "        List of statistical features\n",
    "    \"\"\"\n",
    "    # Basic statistics (using only these five as requested)\n",
    "    mean = np.mean(received_powers)\n",
    "    variance = np.var(received_powers)\n",
    "    median = np.median(received_powers)\n",
    "    lower_quartile = np.percentile(received_powers, 25)\n",
    "    upper_quartile = np.percentile(received_powers, 75)\n",
    "\n",
    "    return [mean, variance, median, lower_quartile, upper_quartile]\n",
    "\n",
    "\n",
    "def run_simulation(primary_power, puea_power, num_iterations, primary_position, puea_position, scenario_name):\n",
    "    # Network parameters\n",
    "    num_secondary_users = 20\n",
    "    area_size = 100\n",
    "\n",
    "    # Initialize separate result matrices for PU and PUEA for each time slot\n",
    "    pu_result_matrix = []\n",
    "    puea_result_matrix = []\n",
    "\n",
    "    # Initialize arrays to store statistical features for each iteration\n",
    "    pu_features = []\n",
    "    puea_features = []\n",
    "\n",
    "    # Calculate distance between PU and PUEA for reference\n",
    "    pu_puea_distance = calculate_distance(primary_position, puea_position)\n",
    "\n",
    "    # Plot network topology for this scenario\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(f'Scenario {scenario_name}: Network Topology\\nPU-PUEA Distance: {pu_puea_distance:.2f} units')\n",
    "    plt.xlabel('X coordinate')\n",
    "    plt.ylabel('Y coordinate')\n",
    "\n",
    "    # Generate positions of secondary users (constant for all iterations in this scenario)\n",
    "    # Use ASCII value of first character as a seed offset\n",
    "    seed_value = 42 + (ord(scenario_name[0]) - ord('A'))\n",
    "    np.random.seed(seed_value)\n",
    "    secondary_positions = np.random.uniform(0, area_size, size=(num_secondary_users, 2))\n",
    "\n",
    "    # Plot secondary users\n",
    "    plt.scatter(secondary_positions[:, 0], secondary_positions[:, 1],\n",
    "               label='Secondary Users', c='blue', marker='o')\n",
    "\n",
    "    # Plot primary user and PUEA\n",
    "    plt.scatter(primary_position[0], primary_position[1],\n",
    "               label='Primary User', c='red', marker='^', s=200)\n",
    "    plt.scatter(puea_position[0], puea_position[1],\n",
    "               label='PUEA', c='green', marker='s', s=200)\n",
    "\n",
    "    # Annotate secondary users\n",
    "    for i in range(num_secondary_users):\n",
    "        plt.annotate(f'SU{i+1}', (secondary_positions[i, 0], secondary_positions[i, 1]))\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.axis([0, area_size, 0, area_size])\n",
    "    #plt.savefig(f'scenario_{scenario_name}_topology.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Time slot simulation\n",
    "    print(f\"\\n===== Scenario {scenario_name} - Simulation over {num_iterations} Time Slots =====\")\n",
    "    print(f\"PU Position: ({primary_position[0]}, {primary_position[1]})\")\n",
    "    print(f\"PUEA Position: ({puea_position[0]}, {puea_position[1]})\")\n",
    "    print(f\"PU-PUEA Distance: {pu_puea_distance:.2f} units\")\n",
    "\n",
    "    # Create arrays to store received powers for each time slot for visualization\n",
    "    time_slots = np.arange(1, num_iterations + 1)\n",
    "    avg_pu_power_per_slot = np.zeros(num_iterations)\n",
    "    avg_puea_power_per_slot = np.zeros(num_iterations)\n",
    "\n",
    "    for time_slot in range(num_iterations):\n",
    "        # Add some randomness per time slot (represents changing channel conditions)\n",
    "        np.random.seed(100 + time_slot)\n",
    "\n",
    "        # Initialize arrays\n",
    "        distances_to_primary = np.zeros(num_secondary_users)\n",
    "        distances_to_puea = np.zeros(num_secondary_users)\n",
    "        path_loss_exponents = np.random.uniform(2, 6, num_secondary_users)  # Range 2-6\n",
    "        shadowing_values = np.random.uniform(4, 12, num_secondary_users)  # Range 4-12\n",
    "        received_powers_primary = np.zeros(num_secondary_users)\n",
    "        received_powers_puea = np.zeros(num_secondary_users)\n",
    "\n",
    "        # Calculate features\n",
    "        for i in range(num_secondary_users):\n",
    "            distances_to_primary[i] = calculate_distance(secondary_positions[i], primary_position)\n",
    "            distances_to_puea[i] = calculate_distance(secondary_positions[i], puea_position)\n",
    "\n",
    "            received_powers_primary[i] = calculate_received_power(\n",
    "                primary_power,\n",
    "                distances_to_primary[i],\n",
    "                path_loss_exponents[i],\n",
    "                shadowing_values[i]\n",
    "            )\n",
    "\n",
    "            received_powers_puea[i] = calculate_received_power(\n",
    "                puea_power,\n",
    "                distances_to_puea[i],\n",
    "                path_loss_exponents[i],\n",
    "                shadowing_values[i]\n",
    "            )\n",
    "            print(f\"PU and PUEA Pr :{received_powers_primary[i]},    {received_powers_puea[i]}\")\n",
    "\n",
    "        # Store average powers for this time slot\n",
    "        avg_pu_power_per_slot[time_slot] = np.mean(received_powers_primary)\n",
    "        avg_puea_power_per_slot[time_slot] = np.mean(received_powers_puea)\n",
    "\n",
    "        # Print results for each time slot\n",
    "        print(f\"\\nTime Slot {time_slot + 1}:\")\n",
    "        print(f\"Average PU Received Power: {avg_pu_power_per_slot[time_slot]:.2f} dB\")\n",
    "        print(f\"Average PUEA Received Power: {avg_puea_power_per_slot[time_slot]:.2f} dB\")\n",
    "\n",
    "        # Print detailed array of received powers from each source\n",
    "        print(\"Array of received powers from Primary User:\")\n",
    "        print(received_powers_primary)\n",
    "        print(\"Array of received powers from PUEA:\")\n",
    "        print(received_powers_puea)\n",
    "\n",
    "        # Append to respective result matrices\n",
    "        pu_result_matrix.append(received_powers_primary)\n",
    "        puea_result_matrix.append(received_powers_puea)\n",
    "\n",
    "        # Use the enhanced feature extraction function (now only returns the basic 5 features)\n",
    "        # Calculate ONE feature vector per time slot by aggregating across all secondary users\n",
    "        pu_enhanced_features = extract_enhanced_features(received_powers_primary)\n",
    "        puea_enhanced_features = extract_enhanced_features(received_powers_puea)\n",
    "\n",
    "        # Add the class labels (0 for PU, 1 for PUEA)\n",
    "        pu_features.append(pu_enhanced_features + [0])  # Label 0 for PU\n",
    "        puea_features.append(puea_enhanced_features + [1])  # Label 1 for PUEA\n",
    "\n",
    "        # For printing the basic statistics (to maintain output format)\n",
    "        pu_mean = pu_enhanced_features[0]\n",
    "        pu_variance = pu_enhanced_features[1]\n",
    "        pu_median = pu_enhanced_features[2]\n",
    "        pu_lower_quartile = pu_enhanced_features[3]\n",
    "        pu_upper_quartile = pu_enhanced_features[4]\n",
    "\n",
    "        puea_mean = puea_enhanced_features[0]\n",
    "        puea_variance = puea_enhanced_features[1]\n",
    "        puea_median = puea_enhanced_features[2]\n",
    "        puea_lower_quartile = puea_enhanced_features[3]\n",
    "        puea_upper_quartile = puea_enhanced_features[4]\n",
    "\n",
    "        # Print statistical features\n",
    "        print(\"\\nStatistical Features for PU:\")\n",
    "        print(f\"  Mean: {pu_mean:.2f}, Variance: {pu_variance:.2f}, Median: {pu_median:.2f}\")\n",
    "        print(f\"  Lower Quartile: {pu_lower_quartile:.2f}, Upper Quartile: {pu_upper_quartile:.2f}\")\n",
    "\n",
    "        print(\"\\nStatistical Features for PUEA:\")\n",
    "        print(f\"  Mean: {puea_mean:.2f}, Variance: {puea_variance:.2f}, Median: {puea_median:.2f}\")\n",
    "        print(f\"  Lower Quartile: {puea_lower_quartile:.2f}, Upper Quartile: {puea_upper_quartile:.2f}\")\n",
    "\n",
    "    # Convert feature lists to numpy arrays for easier handling\n",
    "    pu_features = np.array(pu_features)\n",
    "    puea_features = np.array(puea_features)\n",
    "\n",
    "    return pu_result_matrix, puea_result_matrix, avg_pu_power_per_slot, avg_puea_power_per_slot, pu_features, puea_features\n",
    "\n",
    "# Main simulation parameters\n",
    "primary_power = 20  # Constant value for primary user\n",
    "puea_power = 30     # Fixed PUEA power for all scenarios\n",
    "\n",
    "num_time_slots = 1000 # Number of time slots (iterations) per scenario\n",
    "\n",
    "# Define the three scenarios with different PU and PUEA positions\n",
    "scenarios = {\n",
    "    'A': {\n",
    "        'primary_position': np.array([20, 20]),\n",
    "        'puea_position': np.array([80, 80]),\n",
    "        'description': 'Far distance'\n",
    "    },\n",
    "    'B': {\n",
    "        'primary_position': np.array([25, 25]),\n",
    "        'puea_position': np.array([55, 55]),\n",
    "        'description': 'Medium distance'\n",
    "    },\n",
    "    'C': {\n",
    "        'primary_position': np.array([40, 40]),\n",
    "        'puea_position': np.array([55, 55]),\n",
    "        'description': 'Close distance'\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Store results for different scenarios\n",
    "scenario_results = {}\n",
    "matrix_for_combining_scenarios = []\n",
    "\n",
    "# Define percentages of PUEA energy labels to mix with PU labels\n",
    "puea_percentages = [10, 20, 30, 40, 50]\n",
    "\n",
    "# Store combined matrices for each case\n",
    "scenario_case_matrices = {}\n",
    "\n",
    "# Iterate through each scenario\n",
    "for scenario_name, scenario_config in scenarios.items():\n",
    "    print(f\"\\n\\n========== SCENARIO {scenario_name}: {scenario_config['description']} ==========\")\n",
    "\n",
    "    primary_position = scenario_config['primary_position']\n",
    "    puea_position = scenario_config['puea_position']\n",
    "\n",
    "    pu_result, puea_result, avg_pu_per_slot, avg_puea_per_slot, pu_features, puea_features = run_simulation(\n",
    "        primary_power,\n",
    "        puea_power,\n",
    "        num_time_slots,\n",
    "        primary_position,\n",
    "        puea_position,\n",
    "        scenario_name\n",
    "    )\n",
    "\n",
    "    # Store results for original scenario\n",
    "    scenario_results[scenario_name] = {\n",
    "        'pu_result': pu_result,\n",
    "        'puea_result': puea_result,\n",
    "        'avg_pu_per_slot': avg_pu_per_slot,\n",
    "        'avg_puea_per_slot': avg_puea_per_slot,\n",
    "        'primary_position': primary_position,\n",
    "        'puea_position': puea_position,\n",
    "        'description': scenario_config['description'],\n",
    "        'pu_features': pu_features,\n",
    "        'puea_features': puea_features\n",
    "    }\n",
    "\n",
    "    print(f\"\\nCompleted simulation for Scenario {scenario_name}: {scenario_config['description']}\")\n",
    "    print(f\"PU Features Matrix Shape: {pu_features.shape}\")\n",
    "    print(f\"PUEA Features Matrix Shape: {puea_features.shape}\")\n",
    "\n",
    "    # Display the feature matrices\n",
    "    print(\"\\nPU Features Matrix (Rows: Time Slots, Columns: [Mean, Variance, Median, Lower Quartile, Upper Quartile]):\")\n",
    "    print(pu_features)\n",
    "\n",
    "    print(\"\\nPUEA Features Matrix (Rows: Time Slots, Columns: [Mean, Variance, Median, Lower Quartile, Upper Quartile]):\")\n",
    "    print(puea_features)\n",
    "\n",
    "    # Combine the processed matrices into a single matrix for the base scenario\n",
    "    combined_matrix = np.vstack((pu_features, puea_features))\n",
    "    print(\"Combined Matrix for Base Scenario:\")\n",
    "    print(combined_matrix)\n",
    "    matrix_for_combining_scenarios.append(combined_matrix)\n",
    "\n",
    "    # Save the original combined matrix to a CSV file\n",
    "    original_combined_file = f\"{scenario_name}_original_combined_matrix.csv\"\n",
    "    columns = ['Mean', 'Variance', 'Median', 'Lower Quartile', 'Upper Quartile', 'Label']\n",
    "\n",
    "    pd.DataFrame(combined_matrix, columns=columns).to_csv(original_combined_file, index=False)\n",
    "    print(f\"Saved original combined matrix for Scenario {scenario_name} to {original_combined_file}\")\n",
    "\n",
    "    # Now create 5 different cases for each scenario based on different percentages\n",
    "    print(f\"\\nCreating different percentage cases for scenario {scenario_name}:\")\n",
    "\n",
    "    # Always use only the first num_time_slots PU and PUEA features\n",
    "    pu_features_trimmed = pu_features[:num_time_slots]\n",
    "    puea_features_trimmed = puea_features[:num_time_slots]\n",
    "\n",
    "    for percentage in puea_percentages:\n",
    "        num_puea_samples = max(1, int(round(num_time_slots * (percentage / 100))))\n",
    "        num_pu_samples = num_time_slots - num_puea_samples\n",
    "\n",
    "        # Shuffle before selecting to avoid always picking the same samples\n",
    "        np.random.seed(int(percentage * 100 + ord(scenario_name[0])))\n",
    "        pu_shuffled = pu_features_trimmed.copy()\n",
    "        puea_shuffled = puea_features_trimmed.copy()\n",
    "        np.random.shuffle(pu_shuffled)\n",
    "        np.random.shuffle(puea_shuffled)\n",
    "\n",
    "        selected_pu_features = pu_shuffled[:num_pu_samples]\n",
    "        selected_puea_features = puea_shuffled[:num_puea_samples]\n",
    "\n",
    "        # Combine and shuffle again\n",
    "        case_combined_matrix = np.vstack((selected_pu_features, selected_puea_features))\n",
    "        np.random.shuffle(case_combined_matrix)\n",
    "\n",
    "        case_name = f\"{scenario_name}_case_{percentage}percent\"\n",
    "        scenario_case_matrices[case_name] = case_combined_matrix\n",
    "\n",
    "        output_file = f\"{case_name}_matrix.csv\"\n",
    "        pd.DataFrame(case_combined_matrix, columns=columns).to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"  Created case {case_name} with {num_puea_samples} PUEA samples and {num_pu_samples} PU samples (total {num_time_slots})\")\n",
    "        print(f\"  Saved combined matrix to {output_file}\")\n",
    "\n",
    "# Combine all original scenario matrices into one final matrix with gaps\n",
    "final_matrix = []\n",
    "for i, matrix in enumerate(matrix_for_combining_scenarios):\n",
    "    # Add an identifier column to distinguish between matrices\n",
    "    identifier_column = np.full((matrix.shape[0], 1), i)  # Identifier for each matrix\n",
    "    matrix_with_id = np.hstack((matrix, identifier_column))\n",
    "    final_matrix.append(matrix_with_id)\n",
    "\n",
    "# Stack all matrices vertically\n",
    "final_matrix = np.vstack(final_matrix)\n",
    "\n",
    "# Save the final matrix to a CSV file\n",
    "output_file = \"final_matrix.csv\"\n",
    "columns = ['Mean', 'Variance', 'Median', 'Lower Quartile', 'Upper Quartile', 'Label', 'Matrix_ID']\n",
    "pd.DataFrame(final_matrix, columns=columns).to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nAll original scenario matrices combined and saved to {output_file}\")\n",
    "\n",
    "# Compare scenarios with scatter plot\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# Plot average PU and PUEA received powers for each scenario\n",
    "for scenario_name, results in scenario_results.items():\n",
    "    # Calculate distance between PU and PUEA\n",
    "    distance = calculate_distance(results['primary_position'], results['puea_position'])\n",
    "\n",
    "    # Calculate overall average powers across all time slots\n",
    "    avg_pu = np.mean(results['avg_pu_per_slot'])\n",
    "    avg_puea = np.mean(results['avg_puea_per_slot'])\n",
    "\n",
    "    # Add points to the scatter plot\n",
    "    plt.scatter(distance, avg_pu, label=f'PU - Scenario {scenario_name}',\n",
    "                marker='o', s=100, edgecolors='black')\n",
    "    plt.scatter(distance, avg_puea, label=f'PUEA - Scenario {scenario_name}',\n",
    "                marker='s', s=100, edgecolors='black')\n",
    "\n",
    "plt.title('Comparison of Scenarios: Effect of PU-PUEA Distance on Received Power')\n",
    "plt.xlabel('Distance between PU and PUEA')\n",
    "plt.ylabel('Average Received Power (dB)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig('scenario_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# Output summary of statistical features\n",
    "print(\"\\n===== STATISTICAL FEATURES SUMMARY =====\")\n",
    "for scenario_name, results in scenario_results.items():\n",
    "    distance = calculate_distance(results['primary_position'], results['puea_position'])\n",
    "    print(f\"\\nScenario {scenario_name}: {results['description']} (PU-PUEA Distance: {distance:.2f} units)\")\n",
    "\n",
    "# Output summary of cases created\n",
    "print(\"\\n===== PERCENTAGE CASES SUMMARY =====\")\n",
    "for case_name in sorted(scenario_case_matrices.keys()):\n",
    "    matrix = scenario_case_matrices[case_name]\n",
    "    num_pu = np.sum(matrix[:, 5] == 0)\n",
    "    num_puea = np.sum(matrix[:, 5] == 1)\n",
    "    total = len(matrix)\n",
    "    print(f\"Case {case_name}: {num_pu} PU samples ({num_pu/total*100:.1f}%), {num_puea} PUEA samples ({num_puea/total*100:.1f}%), Total: {total}\")\n",
    "\n",
    "# Output overall summary\n",
    "print(\"\\n===== SIMULATION OVERALL SUMMARY =====\")\n",
    "print(f\"Primary User Power: {primary_power} dB (constant)\")\n",
    "print(f\"PUEA Power: {puea_power} dB (constant)\")\n",
    "print(f\"Number of Time Slots per Scenario: {num_time_slots}\")\n",
    "print(\"\\nScenario Descriptions:\")\n",
    "\n",
    "for scenario_name, results in scenario_results.items():\n",
    "    distance = calculate_distance(results['primary_position'], results['puea_position'])\n",
    "    print(f\"\\nScenario {scenario_name}: {results['description']}\")\n",
    "    print(f\"  PU Position: ({results['primary_position'][0]}, {results['primary_position'][1]})\")\n",
    "    print(f\"  PUEA Position: ({results['puea_position'][0]}, {results['puea_position'][1]})\")\n",
    "    print(f\"  PU-PUEA Distance: {distance:.2f} units\")\n",
    "\n",
    "    # Calculate overall average powers across all time slots\n",
    "    avg_pu = np.mean(results['avg_pu_per_slot'])\n",
    "    avg_puea = np.mean(results['avg_puea_per_slot'])\n",
    "    print(f\"  Average PU Received Power: {avg_pu:.2f} dB\")\n",
    "    print(f\"  Average PUEA Received Power: {avg_puea:.2f} dB\")\n",
    "\n",
    "print(f\"\\nTotal Cases Created: {len(scenario_case_matrices)}\")\n",
    "print(f\"Percentages Used: {puea_percentages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Calculate Manhattan distance matrix for each case and scenario\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the cases and percentages (matching those from Cell 1)\n",
    "cases = ['A', 'B', 'C']\n",
    "percentages = [10, 20, 30, 40, 50]\n",
    "\n",
    "# Function to calculate Manhattan distance matrix\n",
    "def calculate_manhattan_distance(df):\n",
    "    # Extract numerical features (excluding the Label column)\n",
    "    features = df.iloc[:, :-1].values  # All columns except the last (Label)\n",
    "    \n",
    "    # Calculate pairwise Manhattan distances\n",
    "    distances = pdist(features, metric='cityblock')\n",
    "    \n",
    "    # Convert to a square distance matrix\n",
    "    distance_matrix = squareform(distances)\n",
    "    \n",
    "    return distance_matrix, features\n",
    "\n",
    "# Create directory for storing distance matrices if it doesn't exist\n",
    "distance_dir = \"distance_matrices\"\n",
    "if not os.path.exists(distance_dir):\n",
    "    os.makedirs(distance_dir)\n",
    "\n",
    "# Dictionary to store distance matrices\n",
    "distance_matrices = {}\n",
    "\n",
    "# Process each case and scenario\n",
    "for case in cases:\n",
    "    for percentage in percentages:\n",
    "        file_path = f\"{case}_case_{percentage}percent_matrix.csv\"\n",
    "        \n",
    "        try:\n",
    "            # Read the scenario data\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Processing {file_path} with shape {df.shape}\")\n",
    "            \n",
    "            # Calculate Manhattan distance matrix\n",
    "            distance_matrix, features = calculate_manhattan_distance(df)\n",
    "            \n",
    "            # Round to 2 decimal places\n",
    "            distance_matrix = np.round(distance_matrix, 2)\n",
    "            \n",
    "            # Store in dictionary with a key that identifies the case and scenario\n",
    "            key = f\"{case}_{percentage}\"\n",
    "            distance_matrices[key] = distance_matrix\n",
    "            \n",
    "            print(f\"Calculated Manhattan distance matrix for {key} with shape {distance_matrix.shape}\")\n",
    "            \n",
    "            # Save the distance matrix as CSV instead of NPY\n",
    "            output_path = os.path.join(distance_dir, f\"{case}_{percentage}_manhattan_dist.csv\")\n",
    "            # Create a DataFrame to save as CSV (index=False for cleaner output)\n",
    "            pd.DataFrame(distance_matrix).to_csv(output_path, index=False)\n",
    "            print(f\"Saved distance matrix to {output_path}\")\n",
    "            \n",
    "            # Also save a normalized version for visualization\n",
    "            scaler = StandardScaler()\n",
    "            normalized_features = scaler.fit_transform(features)\n",
    "            normalized_distances = pdist(normalized_features, metric='cityblock')\n",
    "            normalized_matrix = squareform(normalized_distances)\n",
    "            \n",
    "            # Round normalized matrix to 2 decimal places\n",
    "            normalized_matrix = np.round(normalized_matrix, 2)\n",
    "            \n",
    "            # Save normalized matrix as CSV\n",
    "            norm_output_path = os.path.join(distance_dir, f\"{case}_{percentage}_normalized_dist.csv\")\n",
    "            pd.DataFrame(normalized_matrix).to_csv(norm_output_path, index=False)\n",
    "            print(f\"Saved normalized distance matrix to {norm_output_path}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "\n",
    "# Display summary of processed matrices\n",
    "print(f\"\\nProcessed {len(distance_matrices)} distance matrices\")\n",
    "\n",
    "# Display a sample of the first distance matrix (for verification)\n",
    "if distance_matrices:\n",
    "    sample_key = list(distance_matrices.keys())[0]\n",
    "    print(f\"\\nSample of distance matrix for {sample_key}:\")\n",
    "    sample_matrix = distance_matrices[sample_key]\n",
    "    print(sample_matrix[:5, :5])  # Display a 5x5 subset\n",
    "    \n",
    "    # Create heatmap visualization of the sample distance matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.title(f\"Manhattan Distance Matrix Heatmap - {sample_key}\")\n",
    "    sns.heatmap(sample_matrix[:20, :20], cmap=\"viridis\", annot=False)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the heatmap\n",
    "    heatmap_path = os.path.join(distance_dir, f\"{sample_key}_heatmap.png\")\n",
    "    plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved heatmap visualization to {heatmap_path}\")\n",
    "else:\n",
    "    print(\"No distance matrices were created. Please check file paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 3: Improved clustering algorithms with better distance matrix utilization\n",
    "# # 1st try (2d plotting) with PCA and MDS\n",
    "\n",
    "# from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import adjusted_rand_score\n",
    "# from sklearn.manifold import MDS\n",
    "\n",
    "# # Dictionary to store clustering results\n",
    "# clustering_results = {}\n",
    "\n",
    "# # Function to perform DBSCAN clustering with the specified epsilon and minPoints calculation\n",
    "# def perform_dbscan(distance_matrix, k=0.55, min_points=None):\n",
    "#     \"\"\"\n",
    "#     Perform DBSCAN clustering with parameters based on the specified approach:\n",
    "#     - minPoints (min_samples) is set to n/2 where n is the number of time slots (rows in distance matrix)\n",
    "#     - epsilon is calculated as max(D) * k where D is the distance matrix\n",
    "    \n",
    "#     Parameters:\n",
    "#         distance_matrix: Precomputed Manhattan distance matrix\n",
    "#         k: Multiplier for epsilon calculation (default 0.5, can be tuned using ROC curves)\n",
    "#         min_points: Override for min_points calculation (if None, will use n/2)\n",
    "        \n",
    "#     Returns:\n",
    "#         Array of cluster labels for each data point\n",
    "#     \"\"\"\n",
    "#     # Get number of time slots (n)\n",
    "#     n = distance_matrix.shape[0]\n",
    "    \n",
    "#     # Set minPoints to n/2 as specified\n",
    "#     if min_points is None:\n",
    "#         min_points = max(2, int(n / 2))\n",
    "#         print(f\"Setting minPoints to n/2: {min_points}\")\n",
    "    \n",
    "#     # Calculate epsilon as max(D) * k\n",
    "#     # Get maximum distance value in the matrix\n",
    "#     max_distance = np.max(distance_matrix)\n",
    "#     epsilon = max_distance * k\n",
    "#     print(f\"Calculated epsilon = max(D) * k = {max_distance:.2f} * {k} = {epsilon:.2f}\")\n",
    "    \n",
    "#     # Apply DBSCAN with calculated parameters\n",
    "#     dbscan = DBSCAN(eps=epsilon, min_samples=min_points, metric='precomputed')\n",
    "#     labels = dbscan.fit_predict(distance_matrix)\n",
    "    \n",
    "#     # Check number of clusters (excluding noise)\n",
    "#     n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "#     print(f\"DBSCAN produced {n_clusters} clusters with epsilon={epsilon:.2f}, minPoints={min_points}\")\n",
    "    \n",
    "#     # Calculate percentage of points classified as noise\n",
    "#     noise_percentage = 100 * np.sum(labels == -1) / len(labels)\n",
    "#     print(f\"Noise percentage: {noise_percentage:.2f}%\")\n",
    "    \n",
    "#     # If we didn't get the expected 2 clusters, try different k values\n",
    "#     if n_clusters != 2:\n",
    "#         print(f\"DBSCAN produced {n_clusters} clusters instead of 2, trying different k values...\")\n",
    "        \n",
    "#         # Try a range of k values\n",
    "#         k_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        \n",
    "#         for test_k in k_values:\n",
    "#             if test_k == k:  # Skip the value we already tried\n",
    "#                 continue\n",
    "                \n",
    "#             test_epsilon = max_distance * test_k\n",
    "#             test_dbscan = DBSCAN(eps=test_epsilon, min_samples=min_points, metric='precomputed')\n",
    "#             test_labels = test_dbscan.fit_predict(distance_matrix)\n",
    "#             test_n_clusters = len(set(test_labels)) - (1 if -1 in test_labels else 0)\n",
    "#             test_noise = 100 * np.sum(test_labels == -1) / len(test_labels)\n",
    "            \n",
    "#             print(f\"  k={test_k}: epsilon={test_epsilon:.2f}, clusters={test_n_clusters}, noise={test_noise:.2f}%\")\n",
    "            \n",
    "#             if test_n_clusters == 2:\n",
    "#                 print(f\"Found optimal k = {test_k} producing 2 clusters\")\n",
    "#                 return test_labels\n",
    "    \n",
    "#     # After all attempts, return the original labels\n",
    "#     return labels\n",
    "\n",
    "# # Function to perform Agglomerative clustering with improved parameter selection\n",
    "# def perform_agglomerative(distance_matrix, n_clusters=2, linkage='average'):\n",
    "#     # Try different linkage methods to find the best one\n",
    "#     linkage_methods = ['average', 'complete', 'ward', 'single']\n",
    "#     best_labels = None\n",
    "#     best_silhouette = -1\n",
    "    \n",
    "#     for method in linkage_methods:\n",
    "#         try:\n",
    "#             # Ward linkage needs Euclidean distances, skip if using different metric\n",
    "#             if method == 'ward' and distance_matrix.dtype != np.float64:\n",
    "#                 continue\n",
    "                \n",
    "#             # Use the current linkage method\n",
    "#             agg = AgglomerativeClustering(\n",
    "#                 n_clusters=n_clusters, \n",
    "#                 linkage=method,\n",
    "#                 metric='precomputed' if method != 'ward' else 'euclidean'\n",
    "#             )\n",
    "            \n",
    "#             # For ward linkage, transform distances to euclidean representation if needed\n",
    "#             if method == 'ward':\n",
    "#                 # Use MDS to convert the distance matrix to euclidean space\n",
    "#                 mds = MDS(n_components=min(5, distance_matrix.shape[0]-1), \n",
    "#                          dissimilarity='precomputed', random_state=42)\n",
    "#                 euclidean_points = mds.fit_transform(distance_matrix)\n",
    "#                 current_labels = agg.fit_predict(euclidean_points)\n",
    "#             else:\n",
    "#                 current_labels = agg.fit_predict(distance_matrix)\n",
    "            \n",
    "#             # Skip methods that produce only one cluster\n",
    "#             if len(np.unique(current_labels)) < 2:\n",
    "#                 continue\n",
    "                \n",
    "#             # Calculate silhouette score directly from distance matrix\n",
    "#             from sklearn.metrics import silhouette_score\n",
    "#             try:\n",
    "#                 # For precomputed distances, we need to extract points\n",
    "#                 mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "#                 points = mds.fit_transform(distance_matrix)\n",
    "#                 current_silhouette = silhouette_score(points, current_labels)\n",
    "                \n",
    "#                 if current_silhouette > best_silhouette:\n",
    "#                     best_silhouette = current_silhouette\n",
    "#                     best_labels = current_labels\n",
    "#                     print(f\"Linkage method '{method}' has silhouette score: {current_silhouette:.4f}\")\n",
    "#             except:\n",
    "#                 # If silhouette calculation fails, still keep the result\n",
    "#                 if best_labels is None:\n",
    "#                     best_labels = current_labels\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error with linkage method '{method}': {str(e)}\")\n",
    "    \n",
    "#     # If no good method found, use average linkage as fallback\n",
    "#     if best_labels is None:\n",
    "#         try:\n",
    "#             agg = AgglomerativeClustering(n_clusters=n_clusters, linkage='average', metric='precomputed')\n",
    "#             best_labels = agg.fit_predict(distance_matrix)\n",
    "#         except:\n",
    "#             # Last resort: apply K-means on MDS coordinates\n",
    "#             mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "#             pos = mds.fit_transform(distance_matrix)\n",
    "#             kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "#             best_labels = kmeans.fit_predict(pos)\n",
    "    \n",
    "#     return best_labels\n",
    "\n",
    "# # Function to perform K-means clustering with distance-aware initialization\n",
    "# def perform_kmeans(distance_matrix, df, n_clusters=2):\n",
    "#     # Use MDS to embed the distance matrix in a lower-dimensional Euclidean space\n",
    "#     mds = MDS(n_components=min(5, distance_matrix.shape[0]-1), \n",
    "#              dissimilarity='precomputed', random_state=42)\n",
    "#     points = mds.fit_transform(distance_matrix)\n",
    "    \n",
    "#     # Use the embedded points for K-means clustering\n",
    "#     kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "#     labels = kmeans.fit_predict(points)\n",
    "    \n",
    "#     # Return the labels\n",
    "#     return labels\n",
    "\n",
    "# # Function to compute purity score (agreement with true labels when available)\n",
    "# def compute_purity_score(labels, true_labels):\n",
    "#     # Make sure cluster labels are comparable\n",
    "#     from scipy.optimize import linear_sum_assignment\n",
    "#     import numpy as np\n",
    "    \n",
    "#     # Create contingency matrix\n",
    "#     contingency_matrix = np.zeros((np.max(labels) + 1, np.max(true_labels) + 1))\n",
    "#     for i in range(len(labels)):\n",
    "#         contingency_matrix[labels[i], true_labels[i]] += 1\n",
    "        \n",
    "#     # Find optimal one-to-one mapping between cluster and true labels\n",
    "#     row_ind, col_ind = linear_sum_assignment(-contingency_matrix)\n",
    "    \n",
    "#     # Return purity score\n",
    "#     return sum([contingency_matrix[row_ind[i], col_ind[i]] for i in range(len(row_ind))]) / len(labels)\n",
    "\n",
    "# # Function to visualize clustering results\n",
    "# def visualize_clustering(data, labels, title, original_labels=None):\n",
    "#     # Apply PCA for visualization \n",
    "#     pca = PCA(n_components=2)\n",
    "#     reduced_data = pca.fit_transform(data)\n",
    "    \n",
    "#     # Create a figure\n",
    "#     plt.figure(figsize=(12, 10))\n",
    "    \n",
    "#     # Get unique labels\n",
    "#     unique_labels = np.unique(labels)\n",
    "    \n",
    "#     # Plot each cluster\n",
    "#     for label in unique_labels:\n",
    "#         if label == -1:\n",
    "#             # Noise points in black\n",
    "#             mask = labels == label\n",
    "#             plt.scatter(reduced_data[mask, 0], reduced_data[mask, 1], \n",
    "#                         c='black', marker='x', alpha=0.5, label='Noise')\n",
    "#         else:\n",
    "#             # Regular clusters\n",
    "#             mask = labels == label\n",
    "#             plt.scatter(reduced_data[mask, 0], reduced_data[mask, 1], \n",
    "#                         marker='o', alpha=0.7, label=f'Cluster {label}')\n",
    "    \n",
    "#     # If original labels are provided, add agreement metrics\n",
    "#     if original_labels is not None:\n",
    "#         # Calculate basic agreement (assuming binary labels)\n",
    "#         if len(np.unique(labels)) == 2 and len(np.unique(original_labels)) == 2:\n",
    "#             # If both have exactly 2 clusters, find best mapping and show agreement\n",
    "#             # Consider both possible mappings and take the better one\n",
    "#             mapping1 = {0:0, 1:1}\n",
    "#             mapping2 = {0:1, 1:0}\n",
    "            \n",
    "#             # Map the cluster labels to the original labels using both mappings\n",
    "#             mapped_labels1 = np.array([mapping1[l] if l in mapping1 else l for l in labels])\n",
    "#             mapped_labels2 = np.array([mapping2[l] if l in mapping2 else l for l in labels])\n",
    "            \n",
    "#             # Calculate agreements\n",
    "#             agreement1 = np.mean(mapped_labels1 == original_labels) * 100\n",
    "#             agreement2 = np.mean(mapped_labels2 == original_labels) * 100\n",
    "            \n",
    "#             # Use the better mapping\n",
    "#             agreement = max(agreement1, agreement2)\n",
    "            \n",
    "#             # Also calculate adjusted Rand Index for a more robust measure\n",
    "#             ari = adjusted_rand_score(original_labels, labels)\n",
    "            \n",
    "#             title = f\"{title}\\nAgreement: {agreement:.1f}%, ARI: {ari:.3f}\"\n",
    "#         else:\n",
    "#             # If not both binary, use ARI only\n",
    "#             # Filter out noise points for the calculation\n",
    "#             valid_indices = labels != -1\n",
    "#             if np.sum(valid_indices) > 0:\n",
    "#                 ari = adjusted_rand_score(original_labels[valid_indices], labels[valid_indices])\n",
    "#                 title = f\"{title}\\nAdjusted Rand Index: {ari:.3f}\"\n",
    "    \n",
    "#     plt.title(title, fontsize=14)\n",
    "#     plt.xlabel('Principal Component 1', fontsize=12)\n",
    "#     plt.ylabel('Principal Component 2', fontsize=12)\n",
    "#     plt.legend(fontsize=10)\n",
    "#     plt.grid(True, linestyle='--', alpha=0.7)\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     return plt\n",
    "\n",
    "# # Create a directory for storing cluster visualizations\n",
    "# cluster_viz_dir = \"cluster_visualizations\"\n",
    "# if not os.path.exists(cluster_viz_dir):\n",
    "#     os.makedirs(cluster_viz_dir)\n",
    "\n",
    "# # Apply clustering to each case and scenario\n",
    "# for case in cases:\n",
    "#     for percentage in percentages:\n",
    "#         key = f\"{case}_{percentage}\"\n",
    "        \n",
    "#         try:\n",
    "#             # Get the distance matrix\n",
    "#             distance_matrix = distance_matrices[key]\n",
    "            \n",
    "#             # Read the original data\n",
    "#             file_path = f\"{case}_case_{percentage}percent_matrix.csv\"\n",
    "#             df = pd.read_csv(file_path)\n",
    "            \n",
    "#             # Extract features and standardize for visualization\n",
    "#             features = df.select_dtypes(include=['float64', 'int64']).values\n",
    "#             scaler = StandardScaler()\n",
    "#             scaled_features = scaler.fit_transform(features)\n",
    "            \n",
    "#             # Get original labels if available (for calculating agreement)\n",
    "#             original_labels = None\n",
    "#             if 'Label' in df.columns:\n",
    "#                 original_labels = df['Label'].values\n",
    "            \n",
    "#             print(f\"\\nClustering {key}...\")\n",
    "            \n",
    "#             # Perform DBSCAN with optimal eps calculation\n",
    "#             print(f\"Running DBSCAN on {key}...\")\n",
    "#             dbscan_labels = perform_dbscan(distance_matrix)\n",
    "            \n",
    "#             # Perform Agglomerative Clustering with multiple linkage methods\n",
    "#             print(f\"Running Agglomerative clustering on {key}...\")\n",
    "#             agg_labels = perform_agglomerative(distance_matrix)\n",
    "            \n",
    "#             # Perform K-means on MDS embedding of distance matrix\n",
    "#             print(f\"Running K-means on {key}...\")\n",
    "#             kmeans_labels = perform_kmeans(distance_matrix, df)\n",
    "            \n",
    "#             # Store results\n",
    "#             clustering_results[f\"{key}_dbscan\"] = dbscan_labels\n",
    "#             clustering_results[f\"{key}_agglomerative\"] = agg_labels\n",
    "#             clustering_results[f\"{key}_kmeans\"] = kmeans_labels\n",
    "            \n",
    "#             # Print cluster distribution\n",
    "#             print(f\"\\nCluster distribution for {key}:\")\n",
    "#             print(f\"DBSCAN: {np.bincount(dbscan_labels + 1) if -1 in dbscan_labels else np.bincount(dbscan_labels)}\")\n",
    "#             print(f\"Agglomerative: {np.bincount(agg_labels)}\")\n",
    "#             print(f\"K-means: {np.bincount(kmeans_labels)}\")\n",
    "            \n",
    "#             # Check if clusters match expected count\n",
    "#             dbscan_n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "#             print(f\"Number of clusters - DBSCAN: {dbscan_n_clusters}, Agglomerative: {len(set(agg_labels))}, K-means: {len(set(kmeans_labels))}\")\n",
    "            \n",
    "#             # If original labels exist, calculate agreement metrics\n",
    "#             if original_labels is not None:\n",
    "#                 # Calculate Adjusted Rand Index (ARI) for each method\n",
    "#                 # For DBSCAN, exclude noise points for agreement calculation\n",
    "#                 if -1 in dbscan_labels:\n",
    "#                     valid_indices = dbscan_labels != -1\n",
    "#                     valid_dbscan = dbscan_labels[valid_indices]\n",
    "#                     valid_original = original_labels[valid_indices]\n",
    "#                     dbscan_ari = adjusted_rand_score(valid_original, valid_dbscan) if len(valid_dbscan) > 0 else 0\n",
    "#                 else:\n",
    "#                     dbscan_ari = adjusted_rand_score(original_labels, dbscan_labels)\n",
    "                \n",
    "#                 agg_ari = adjusted_rand_score(original_labels, agg_labels)\n",
    "#                 kmeans_ari = adjusted_rand_score(original_labels, kmeans_labels)\n",
    "                \n",
    "#                 print(f\"Adjusted Rand Index - DBSCAN: {dbscan_ari:.3f}, Agglomerative: {agg_ari:.3f}, K-means: {kmeans_ari:.3f}\")\n",
    "            \n",
    "#             # Visualize clustering results\n",
    "#             # DBSCAN\n",
    "#             dbscan_plot = visualize_clustering(scaled_features, dbscan_labels, \n",
    "#                                             f\"DBSCAN Clustering - Case {case}, {percentage}% Data\", \n",
    "#                                             original_labels)\n",
    "#             dbscan_plot.savefig(f\"{cluster_viz_dir}/{key}_dbscan_clustering.png\", dpi=300, bbox_inches='tight')\n",
    "#             plt.close()\n",
    "            \n",
    "#             # Agglomerative\n",
    "#             agg_plot = visualize_clustering(scaled_features, agg_labels, \n",
    "#                                         f\"Agglomerative Clustering - Case {case}, {percentage}% Data\",\n",
    "#                                         original_labels)\n",
    "#             agg_plot.savefig(f\"{cluster_viz_dir}/{key}_agglomerative_clustering.png\", dpi=300, bbox_inches='tight')\n",
    "#             plt.close()\n",
    "            \n",
    "#             # K-means\n",
    "#             kmeans_plot = visualize_clustering(scaled_features, kmeans_labels, \n",
    "#                                             f\"K-means Clustering - Case {case}, {percentage}% Data\",\n",
    "#                                             original_labels)\n",
    "#             kmeans_plot.savefig(f\"{cluster_viz_dir}/{key}_kmeans_clustering.png\", dpi=300, bbox_inches='tight')\n",
    "#             plt.close()\n",
    "            \n",
    "#             print(f\"Visualizations saved for {key} in {cluster_viz_dir} directory\")\n",
    "            \n",
    "#         except KeyError:\n",
    "#             print(f\"Distance matrix not found for {key}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error clustering {key}: {str(e)}\")\n",
    "\n",
    "# # Add this code at the end of Cell 3 (before the summary print statements)\n",
    "# print(\"\\nSaving clustering results to CSV files for performance evaluation...\")\n",
    "\n",
    "# # Create directory if it doesn't exist\n",
    "# if not os.path.exists(cluster_viz_dir):\n",
    "#     os.makedirs(cluster_viz_dir)\n",
    "\n",
    "# # Save clustering results to CSV files\n",
    "# for case in cases:\n",
    "#     for percentage in percentages:\n",
    "#         key = f\"{case}_{percentage}\"\n",
    "        \n",
    "#         # Get the original data\n",
    "#         file_path = f\"{case}_case_{percentage}percent_matrix.csv\"\n",
    "        \n",
    "#         try:\n",
    "#             # Check if the file exists\n",
    "#             if os.path.exists(file_path):\n",
    "#                 # Load the data to get the true labels if available\n",
    "#                 df = pd.read_csv(file_path)\n",
    "                \n",
    "#                 # Check if we have clustering results for this case/percentage\n",
    "#                 dbscan_key = f\"{key}_dbscan\"\n",
    "#                 kmeans_key = f\"{key}_kmeans\"\n",
    "#                 agg_key = f\"{key}_agglomerative\"\n",
    "                \n",
    "#                 # Choose one of the clustering results (prioritize k-means, then agglomerative, then DBSCAN)\n",
    "#                 if kmeans_key in clustering_results:\n",
    "#                     selected_labels = clustering_results[kmeans_key]\n",
    "#                     selected_algorithm = \"K-means\"\n",
    "#                 elif agg_key in clustering_results:\n",
    "#                     selected_labels = clustering_results[agg_key]\n",
    "#                     selected_algorithm = \"Agglomerative\"\n",
    "#                 elif dbscan_key in clustering_results:\n",
    "#                     selected_labels = clustering_results[dbscan_key]\n",
    "#                     selected_algorithm = \"DBSCAN\"\n",
    "#                 else:\n",
    "#                     print(f\"No clustering results found for {key}\")\n",
    "#                     continue\n",
    "                \n",
    "#                 # Create DataFrame with the clustering results\n",
    "#                 result_df = pd.DataFrame({\n",
    "#                     'Cluster': selected_labels\n",
    "#                 })\n",
    "                \n",
    "#                 # Add the original labels if available\n",
    "#                 if 'Label' in df.columns:\n",
    "#                     result_df['True_Label'] = df['Label'].values\n",
    "                \n",
    "#                 # Save to CSV\n",
    "#                 output_path = f\"{cluster_viz_dir}/{key}_clustering_result.csv\"\n",
    "#                 result_df.to_csv(output_path, index=False)\n",
    "#                 print(f\"Saved clustering result for {key} ({selected_algorithm}) to {output_path}\")\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error saving clustering result for {key}: {str(e)}\")\n",
    "\n",
    "# print(\"Clustering results saved successfully.\")\n",
    "\n",
    "# # Summary of clustering results\n",
    "# print(f\"\\nCompleted clustering for {len(clustering_results) // 3} case-scenario combinations\")\n",
    "# print(f\"Generated {len(clustering_results)} clustering results (3 algorithms per case-scenario)\")\n",
    "# print(f\"Visualization files saved in {cluster_viz_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Enhanced clustering for PUEA detection with multiple algorithms \n",
    "# 2nd try (3d plotting) with PCA and MDS\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "# from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import os\n",
    "\n",
    "# # Create output directories (properly creating nested directories)\n",
    "# output_dir = \"clustering_results\"\n",
    "# algorithm_dirs = [\"kmeans\", \"dbscan\", \"agglomerative\"]\n",
    "\n",
    "# # Create main directory\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# # Create algorithm-specific subdirectories\n",
    "# for alg_dir in algorithm_dirs:\n",
    "#     dir_path = os.path.join(output_dir, alg_dir)\n",
    "#     if not os.path.exists(dir_path):\n",
    "#         os.makedirs(dir_path)\n",
    "\n",
    "# # Define cases and percentages\n",
    "# cases = ['A', 'B', 'C']\n",
    "# percentages = [10, 20, 30, 40, 50]\n",
    "\n",
    "# def perform_kmeans_clustering(features, original_labels=None):\n",
    "#     \"\"\"Performs K-means clustering on the feature space\"\"\"\n",
    "#     kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)\n",
    "#     labels = kmeans.fit_predict(features)\n",
    "    \n",
    "#     results = analyze_clustering_results(labels, original_labels, \"K-means\")\n",
    "#     return results\n",
    "\n",
    "# def perform_dbscan_clustering(features, original_labels=None):\n",
    "#     \"\"\"Performs DBSCAN clustering on the feature space\"\"\"\n",
    "#     # Normalize features for better DBSCAN performance\n",
    "#     scaler = StandardScaler()\n",
    "#     normalized_features = scaler.fit_transform(features)\n",
    "    \n",
    "#     # Try different eps values until we get close to 2 clusters\n",
    "#     best_eps = 0.5\n",
    "#     best_labels = None\n",
    "#     best_n_clusters = 0\n",
    "    \n",
    "#     for eps in [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "#         dbscan = DBSCAN(eps=eps, min_samples=5)\n",
    "#         current_labels = dbscan.fit_predict(normalized_features)\n",
    "#         n_clusters = len(set(current_labels)) - (1 if -1 in current_labels else 0)\n",
    "        \n",
    "#         # We want 2 clusters, but will accept 1-3 clusters\n",
    "#         if n_clusters >= 1 and n_clusters <= 3:\n",
    "#             best_eps = eps\n",
    "#             best_labels = current_labels\n",
    "#             best_n_clusters = n_clusters\n",
    "#             if n_clusters == 2:\n",
    "#                 break\n",
    "    \n",
    "#     if best_labels is None:\n",
    "#         # Fallback if DBSCAN fails to find reasonable clusters\n",
    "#         kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)\n",
    "#         best_labels = kmeans.fit_predict(features)\n",
    "#         print(\"DBSCAN failed to find reasonable clusters. Falling back to K-means.\")\n",
    "    \n",
    "#     # If DBSCAN found noise points (-1), assign them to the nearest cluster\n",
    "#     if -1 in best_labels:\n",
    "#         noise_indices = np.where(best_labels == -1)[0]\n",
    "#         if len(noise_indices) > 0:\n",
    "#             # For each noise point, find the nearest non-noise cluster\n",
    "#             for idx in noise_indices:\n",
    "#                 # Use a simple distance to cluster centroid approach\n",
    "#                 min_dist = float('inf')\n",
    "#                 assigned_cluster = 0\n",
    "                \n",
    "#                 for cluster in set(best_labels) - {-1}:\n",
    "#                     cluster_points = normalized_features[best_labels == cluster]\n",
    "#                     if len(cluster_points) > 0:\n",
    "#                         # Calculate mean as cluster centroid\n",
    "#                         centroid = np.mean(cluster_points, axis=0)\n",
    "#                         dist = np.sum((normalized_features[idx] - centroid) ** 2)\n",
    "#                         if dist < min_dist:\n",
    "#                             min_dist = dist\n",
    "#                             assigned_cluster = cluster\n",
    "                \n",
    "#                 best_labels[idx] = assigned_cluster\n",
    "    \n",
    "#     # If DBSCAN found only one cluster or more than two clusters, remap labels to 0/1\n",
    "#     unique_labels = set(best_labels)\n",
    "#     if len(unique_labels) != 2:\n",
    "#         if len(unique_labels) > 2:\n",
    "#             # Keep the two largest clusters\n",
    "#             sizes = [(label, np.sum(best_labels == label)) for label in unique_labels]\n",
    "#             top_two = sorted(sizes, key=lambda x: x[1], reverse=True)[:2]\n",
    "            \n",
    "#             # Create a mapping from original labels to 0/1\n",
    "#             label_map = {top_two[0][0]: 0, top_two[1][0]: 1}\n",
    "            \n",
    "#             # Apply the mapping\n",
    "#             new_labels = np.array([label_map.get(label, -1) for label in best_labels])\n",
    "            \n",
    "#             # Assign all other labels to the closest of the top two clusters\n",
    "#             for label in unique_labels:\n",
    "#                 if label not in label_map:\n",
    "#                     mask = best_labels == label\n",
    "#                     if np.any(mask):\n",
    "#                         # Assign to the nearest of the two main clusters\n",
    "#                         points = normalized_features[mask]\n",
    "#                         centroid0 = np.mean(normalized_features[best_labels == top_two[0][0]], axis=0)\n",
    "#                         centroid1 = np.mean(normalized_features[best_labels == top_two[1][0]], axis=0)\n",
    "                        \n",
    "#                         dist0 = np.mean(np.sum((points - centroid0) ** 2, axis=1))\n",
    "#                         dist1 = np.mean(np.sum((points - centroid1) ** 2, axis=1))\n",
    "                        \n",
    "#                         new_labels[mask] = 0 if dist0 < dist1 else 1\n",
    "            \n",
    "#             best_labels = new_labels\n",
    "#         else:\n",
    "#             # If there's only one cluster, split using k-means\n",
    "#             print(\"DBSCAN found only one cluster. Splitting with K-means.\")\n",
    "#             kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)\n",
    "#             best_labels = kmeans.fit_predict(features)\n",
    "    \n",
    "#     results = analyze_clustering_results(best_labels, original_labels, \"DBSCAN\")\n",
    "#     return results\n",
    "\n",
    "# def perform_agglomerative_clustering(features, original_labels=None):\n",
    "#     \"\"\"Performs Agglomerative clustering on the feature space\"\"\"\n",
    "#     # Normalize features\n",
    "#     scaler = StandardScaler()\n",
    "#     normalized_features = scaler.fit_transform(features)\n",
    "    \n",
    "#     # Try different linkage criteria\n",
    "#     best_labels = None\n",
    "#     best_silhouette = -1\n",
    "    \n",
    "#     for linkage in ['ward', 'complete', 'average', 'single']:\n",
    "#         try:\n",
    "#             agg = AgglomerativeClustering(n_clusters=2, linkage=linkage)\n",
    "#             current_labels = agg.fit_predict(normalized_features)\n",
    "            \n",
    "#             # Calculate silhouette score if possible\n",
    "#             if len(set(current_labels)) > 1:\n",
    "#                 try:\n",
    "#                     silhouette = silhouette_score(normalized_features, current_labels)\n",
    "#                     if silhouette > best_silhouette:\n",
    "#                         best_silhouette = silhouette\n",
    "#                         best_labels = current_labels\n",
    "#                         print(f\"Agglomerative with {linkage} linkage: silhouette = {silhouette:.4f}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error calculating silhouette for {linkage}: {str(e)}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error with {linkage} linkage: {str(e)}\")\n",
    "    \n",
    "#     if best_labels is None:\n",
    "#         # Fallback if all linkage methods fail\n",
    "#         agg = AgglomerativeClustering(n_clusters=2)\n",
    "#         best_labels = agg.fit_predict(normalized_features)\n",
    "    \n",
    "#     results = analyze_clustering_results(best_labels, original_labels, \"Agglomerative\")\n",
    "#     return results\n",
    "\n",
    "# def analyze_clustering_results(labels, original_labels=None, algorithm_name=\"\"):\n",
    "#     \"\"\"Analyzes clustering results and calculates performance metrics\"\"\"\n",
    "#     # Count cluster sizes\n",
    "#     count_0 = np.sum(labels == 0)\n",
    "#     count_1 = np.sum(labels == 1)\n",
    "#     total = len(labels)\n",
    "    \n",
    "#     # Calculate actual PUEA percentage from ground truth\n",
    "#     actual_puea_percentage = None\n",
    "#     if original_labels is not None:\n",
    "#         actual_puea_percentage = np.mean(original_labels == 1) * 100\n",
    "        \n",
    "#         # If we have original labels, possibly flip clusters for better alignment\n",
    "#         agreement1 = np.mean(labels == original_labels) * 100\n",
    "#         swapped_labels = 1 - labels\n",
    "#         agreement2 = np.mean(swapped_labels == original_labels) * 100\n",
    "        \n",
    "#         if agreement2 > agreement1:\n",
    "#             labels = swapped_labels\n",
    "#             agreement = agreement2\n",
    "#         else:\n",
    "#             agreement = agreement1\n",
    "        \n",
    "#         ari = adjusted_rand_score(original_labels, labels)\n",
    "#         print(f\"{algorithm_name} - Agreement with original labels: {agreement:.2f}%, ARI: {ari:.4f}\")\n",
    "        \n",
    "#         # Determine which cluster is predominantly PUEA vs PU\n",
    "#         puea_in_cluster0 = np.sum((labels == 0) & (original_labels == 1))\n",
    "#         puea_in_cluster1 = np.sum((labels == 1) & (original_labels == 1))\n",
    "#         pu_in_cluster0 = np.sum((labels == 0) & (original_labels == 0))\n",
    "#         pu_in_cluster1 = np.sum((labels == 1) & (original_labels == 0))\n",
    "        \n",
    "#         # Identify PUEA cluster\n",
    "#         puea_cluster = 1 if puea_in_cluster1 > puea_in_cluster0 else 0\n",
    "#         pu_cluster = 0 if puea_cluster == 1 else 1\n",
    "        \n",
    "#         # Calculate true counts\n",
    "#         total_pu = np.sum(original_labels == 0)\n",
    "#         total_puea = np.sum(original_labels == 1)\n",
    "        \n",
    "#         # Calculate detection rates\n",
    "#         puea_detection_rate = np.sum((labels == puea_cluster) & (original_labels == 1)) / total_puea if total_puea > 0 else 0\n",
    "#         false_detection_rate = np.sum((labels == puea_cluster) & (original_labels == 0)) / total_pu if total_pu > 0 else 0\n",
    "        \n",
    "#         print(f\"{algorithm_name} - PUEA Detection Rate: {puea_detection_rate:.4f}\")\n",
    "#         print(f\"{algorithm_name} - False Detection Rate: {false_detection_rate:.4f}\")\n",
    "        \n",
    "#         # Return results dictionary\n",
    "#         results = {\n",
    "#             'labels': labels,\n",
    "#             'count_0': count_0, \n",
    "#             'count_1': count_1,\n",
    "#             'puea_cluster': puea_cluster,\n",
    "#             'pu_cluster': pu_cluster,\n",
    "#             'puea_detection_rate': puea_detection_rate,\n",
    "#             'false_detection_rate': false_detection_rate,\n",
    "#             'accuracy': agreement / 100,\n",
    "#             'ari': ari,\n",
    "#             'actual_puea_percentage': actual_puea_percentage,\n",
    "#             'pu_in_cluster0': pu_in_cluster0,\n",
    "#             'pu_in_cluster1': pu_in_cluster1,\n",
    "#             'puea_in_cluster0': puea_in_cluster0,\n",
    "#             'puea_in_cluster1': puea_in_cluster1,\n",
    "#             'total_pu': total_pu,\n",
    "#             'total_puea': total_puea\n",
    "#         }\n",
    "#     else:\n",
    "#         # Return basic results if no ground truth\n",
    "#         results = {\n",
    "#             'labels': labels,\n",
    "#             'count_0': count_0, \n",
    "#             'count_1': count_1\n",
    "#         }\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# def visualize_clustering_results(features, results, case, percentage, algorithm, output_dir):\n",
    "#     \"\"\"Creates 2D and 3D visualizations of clustering results\"\"\"\n",
    "#     # Fix algorithm folder name to lowercase for consistent paths\n",
    "#     alg_folder = algorithm.lower()\n",
    "    \n",
    "#     # Create 2D plot (mean vs variance)\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     plt.title(f\"{algorithm} Clustering - Case {case}, {percentage}% Scenario\", fontsize=14)\n",
    "#     plt.xlabel('Mean', fontsize=12)\n",
    "#     plt.ylabel('Variance', fontsize=12)\n",
    "    \n",
    "#     # Plot each cluster with different colors\n",
    "#     for cluster_label in [0, 1]:\n",
    "#         mask = results['labels'] == cluster_label\n",
    "#         plt.scatter(features[mask, 0], features[mask, 1],\n",
    "#                    label=f'Cluster {cluster_label}', alpha=0.7)\n",
    "    \n",
    "#     # Add detection metrics in title if available\n",
    "#     if 'puea_detection_rate' in results:\n",
    "#         plt.title(f\"{algorithm} Clustering - Case {case}, {percentage}% Scenario\\n\" +\n",
    "#                  f\"PUEA Detection Rate: {results['puea_detection_rate']:.2f}, \" +\n",
    "#                  f\"False Detection Rate: {results['false_detection_rate']:.2f}, \" +\n",
    "#                  f\"Accuracy: {results['accuracy']:.2f}\")\n",
    "    \n",
    "#     plt.grid(True)\n",
    "#     plt.legend()\n",
    "    \n",
    "#     # Save 2D plot\n",
    "#     plt.savefig(f\"{output_dir}/{alg_folder}/{case}_{percentage}_2d_clustering.png\", \n",
    "#                dpi=300, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "    \n",
    "#     # Create 3D plot (median, lower quartile, upper quartile)\n",
    "#     fig = plt.figure(figsize=(12, 10))\n",
    "#     ax = fig.add_subplot(111, projection='3d')\n",
    "#     ax.set_title(f\"{algorithm} 3D Clustering - Case {case}, {percentage}% Scenario\", fontsize=14)\n",
    "#     ax.set_xlabel('Median', fontsize=12)\n",
    "#     ax.set_ylabel('Lower Quartile', fontsize=12)\n",
    "#     ax.set_zlabel('Upper Quartile', fontsize=12)\n",
    "\n",
    "#     # Plot each cluster in 3D\n",
    "#     for cluster_label in [0, 1]:\n",
    "#         mask = results['labels'] == cluster_label\n",
    "#         if np.any(mask):  # Only plot if there are points in this cluster\n",
    "#             scatter = ax.scatter(\n",
    "#                 features[mask, 2],  # Median (index 2)\n",
    "#                 features[mask, 3],  # Lower Quartile (index 3)\n",
    "#                 features[mask, 4],  # Upper Quartile (index 4)\n",
    "#                 label=f'Cluster {cluster_label}',\n",
    "#                 alpha=0.7,\n",
    "#                 s=50\n",
    "#             )\n",
    "    \n",
    "#     # Add detection metrics in title if available\n",
    "#     if 'puea_detection_rate' in results:\n",
    "#         ax.set_title(f\"{algorithm} 3D Clustering - Case {case}, {percentage}% Scenario\\n\" +\n",
    "#                  f\"PUEA Detection Rate: {results['puea_detection_rate']:.2f}, \" +\n",
    "#                  f\"False Detection Rate: {results['false_detection_rate']:.2f}\")\n",
    "    \n",
    "#     # Add legend and adjust view\n",
    "#     ax.legend()\n",
    "#     ax.view_init(elev=30, azim=45)\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     # Save 3D plot\n",
    "#     plt.savefig(f\"{output_dir}/{alg_folder}/{case}_{percentage}_3d_clustering.png\", \n",
    "#                dpi=300, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "\n",
    "# # Process each case and scenario\n",
    "# all_results = {\n",
    "#     'kmeans': [],\n",
    "#     'dbscan': [],\n",
    "#     'agglomerative': []\n",
    "# }\n",
    "\n",
    "# for case in cases:\n",
    "#     for percentage in percentages:\n",
    "#         key = f\"{case}_{percentage}\"\n",
    "#         print(f\"\\n\\nProcessing {key}...\")\n",
    "        \n",
    "#         try:\n",
    "#             # Load data\n",
    "#             file_path = f\"{case}_case_{percentage}percent_matrix.csv\"\n",
    "#             df = pd.read_csv(file_path)\n",
    "            \n",
    "#             # Extract features and labels\n",
    "#             features = df.iloc[:, :-1].values\n",
    "#             original_labels = df['Label'].values if 'Label' in df.columns else None\n",
    "            \n",
    "#             print(f\"Loaded dataset with {features.shape[0]} samples, {features.shape[1]} features\")\n",
    "            \n",
    "#             # Perform K-means clustering\n",
    "#             print(\"\\nPerforming K-means clustering...\")\n",
    "#             kmeans_results = perform_kmeans_clustering(features, original_labels)\n",
    "            \n",
    "#             # Perform DBSCAN clustering\n",
    "#             print(\"\\nPerforming DBSCAN clustering...\")\n",
    "#             dbscan_results = perform_dbscan_clustering(features, original_labels)\n",
    "            \n",
    "#             # Perform Agglomerative clustering\n",
    "#             print(\"\\nPerforming Agglomerative clustering...\")\n",
    "#             agglomerative_results = perform_agglomerative_clustering(features, original_labels)\n",
    "            \n",
    "#             # Visualize results for each algorithm\n",
    "#             print(\"\\nGenerating visualizations...\")\n",
    "#             visualize_clustering_results(features, kmeans_results, case, percentage, \n",
    "#                                         \"K-means\", output_dir)\n",
    "#             visualize_clustering_results(features, dbscan_results, case, percentage, \n",
    "#                                         \"DBSCAN\", output_dir)\n",
    "#             visualize_clustering_results(features, agglomerative_results, case, percentage, \n",
    "#                                         \"Agglomerative\", output_dir)\n",
    "            \n",
    "#             # Save results to DataFrame if original labels are available\n",
    "#             if original_labels is not None:\n",
    "#                 # K-means results\n",
    "#                 all_results['kmeans'].append({\n",
    "#                     'Case': case,\n",
    "#                     'Percentage': percentage,\n",
    "#                     'Actual_PUEA_Percentage': kmeans_results['actual_puea_percentage'],\n",
    "#                     'PUEA_Detection_Rate': kmeans_results['puea_detection_rate'],\n",
    "#                     'False_Detection_Rate': kmeans_results['false_detection_rate'],\n",
    "#                     'Accuracy': kmeans_results['accuracy'],\n",
    "#                     'ARI': kmeans_results['ari'],\n",
    "#                     'PUEA_Cluster': kmeans_results['puea_cluster'],\n",
    "#                     'Cluster0_Size': kmeans_results['count_0'],\n",
    "#                     'Cluster1_Size': kmeans_results['count_1']\n",
    "#                 })\n",
    "                \n",
    "#                 # DBSCAN results\n",
    "#                 all_results['dbscan'].append({\n",
    "#                     'Case': case,\n",
    "#                     'Percentage': percentage,\n",
    "#                     'Actual_PUEA_Percentage': dbscan_results['actual_puea_percentage'],\n",
    "#                     'PUEA_Detection_Rate': dbscan_results['puea_detection_rate'],\n",
    "#                     'False_Detection_Rate': dbscan_results['false_detection_rate'],\n",
    "#                     'Accuracy': dbscan_results['accuracy'],\n",
    "#                     'ARI': dbscan_results['ari'],\n",
    "#                     'PUEA_Cluster': dbscan_results['puea_cluster'],\n",
    "#                     'Cluster0_Size': dbscan_results['count_0'],\n",
    "#                     'Cluster1_Size': dbscan_results['count_1']\n",
    "#                 })\n",
    "                \n",
    "#                 # Agglomerative results\n",
    "#                 all_results['agglomerative'].append({\n",
    "#                     'Case': case,\n",
    "#                     'Percentage': percentage,\n",
    "#                     'Actual_PUEA_Percentage': agglomerative_results['actual_puea_percentage'],\n",
    "#                     'PUEA_Detection_Rate': agglomerative_results['puea_detection_rate'],\n",
    "#                     'False_Detection_Rate': agglomerative_results['false_detection_rate'],\n",
    "#                     'Accuracy': agglomerative_results['accuracy'],\n",
    "#                     'ARI': agglomerative_results['ari'],\n",
    "#                     'PUEA_Cluster': agglomerative_results['puea_cluster'],\n",
    "#                     'Cluster0_Size': agglomerative_results['count_0'],\n",
    "#                     'Cluster1_Size': agglomerative_results['count_1']\n",
    "#                 })\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {key}: {str(e)}\")\n",
    "\n",
    "# # Save all results to CSV\n",
    "# for algorithm, results in all_results.items():\n",
    "#     if results:\n",
    "#         results_df = pd.DataFrame(results)\n",
    "#         results_df.to_csv(f\"{output_dir}/{algorithm}_clustering_summary.csv\", index=False)\n",
    "#         print(f\"\\nResults saved to {output_dir}/{algorithm}_clustering_summary.csv\")\n",
    "\n",
    "# # Create a 3D feature space visualization showing all features for each case and percentage\n",
    "# for case in cases:\n",
    "#     for percentage in percentages:\n",
    "#         try:\n",
    "#             file_path = f\"{case}_case_{percentage}percent_matrix.csv\"\n",
    "#             if not os.path.exists(file_path):\n",
    "#                 continue\n",
    "                \n",
    "#             df = pd.read_csv(file_path)\n",
    "#             features = df.iloc[:, :-1].values\n",
    "#             labels = df['Label'].values if 'Label' in df.columns else None\n",
    "            \n",
    "#             if labels is not None:\n",
    "#                 plt.figure(figsize=(14, 12))\n",
    "#                 ax = plt.axes(projection='3d')\n",
    "                \n",
    "#                 # Plot PU points (label 0)\n",
    "#                 pu_mask = labels == 0\n",
    "#                 ax.scatter3D(\n",
    "#                     features[pu_mask, 2],     # Median\n",
    "#                     features[pu_mask, 3],     # Lower Quartile\n",
    "#                     features[pu_mask, 4],     # Upper Quartile\n",
    "#                     c='blue',\n",
    "#                     marker='o',\n",
    "#                     label='Primary User (PU)',\n",
    "#                     alpha=0.7,\n",
    "#                     s=50\n",
    "#                 )\n",
    "                \n",
    "#                 # Plot PUEA points (label 1)\n",
    "#                 puea_mask = labels == 1\n",
    "#                 ax.scatter3D(\n",
    "#                     features[puea_mask, 2],   # Median\n",
    "#                     features[puea_mask, 3],   # Lower Quartile\n",
    "#                     features[puea_mask, 4],   # Upper Quartile\n",
    "#                     c='red',\n",
    "#                     marker='^',\n",
    "#                     label='PUEA',\n",
    "#                     alpha=0.7,\n",
    "#                     s=50\n",
    "#                 )\n",
    "                \n",
    "#                 # Add labels and title\n",
    "#                 ax.set_xlabel('Median', fontsize=12)\n",
    "#                 ax.set_ylabel('Lower Quartile', fontsize=12)\n",
    "#                 ax.set_zlabel('Upper Quartile', fontsize=12)\n",
    "#                 ax.set_title(f'3D Feature Space - Case {case}, {percentage}% PUEA', \n",
    "#                              fontsize=14)\n",
    "#                 ax.legend(fontsize=12)\n",
    "                \n",
    "#                 # Adjust view angle\n",
    "#                 ax.view_init(elev=30, azim=45)\n",
    "                \n",
    "#                 # Save the 3D feature visualization\n",
    "#                 plt.savefig(f\"{output_dir}/3d_feature_space_{case}_{percentage}.png\", dpi=300, bbox_inches='tight')\n",
    "#                 plt.close()\n",
    "#                 print(f\"\\n3D feature space visualization saved for Case {case}, {percentage}% PUEA\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error creating 3D feature space for {case}_{percentage}: {str(e)}\")\n",
    "\n",
    "# # Create comparative detection rate visualization\n",
    "# if all(len(all_results[alg]) > 0 for alg in ['kmeans', 'dbscan', 'agglomerative']):\n",
    "#     plt.figure(figsize=(18, 12))\n",
    "    \n",
    "#     # Create 3x2 grid of subplots (3 cases, 2 metrics each)\n",
    "#     for i, case in enumerate(cases):\n",
    "#         # PUEA Detection Rate plot\n",
    "#         plt.subplot(3, 2, 2*i+1)\n",
    "        \n",
    "#         # Plot for each algorithm\n",
    "#         for algorithm, results in all_results.items():\n",
    "#             case_data = pd.DataFrame(results)\n",
    "#             case_data = case_data[case_data['Case'] == case]\n",
    "#             if not case_data.empty:\n",
    "#                 plt.plot(case_data['Percentage'], case_data['PUEA_Detection_Rate'], \n",
    "#                         marker='o', linewidth=2, label=f\"{algorithm.capitalize()}\")\n",
    "        \n",
    "#         plt.title(f'Case {case} - PUEA Detection Rate', fontsize=14)\n",
    "#         plt.xlabel('PUEA Percentage in Dataset (%)', fontsize=12)\n",
    "#         plt.ylabel('Detection Rate', fontsize=12)\n",
    "#         plt.grid(True)\n",
    "#         plt.xticks(percentages)\n",
    "#         plt.ylim(0, 1.05)\n",
    "#         if i == 0:  # Add legend only to the first row\n",
    "#             plt.legend(loc='lower right')\n",
    "        \n",
    "#         # False Detection Rate plot\n",
    "#         plt.subplot(3, 2, 2*i+2)\n",
    "        \n",
    "#         # Plot for each algorithm\n",
    "#         for algorithm, results in all_results.items():\n",
    "#             case_data = pd.DataFrame(results)\n",
    "#             case_data = case_data[case_data['Case'] == case]\n",
    "#             if not case_data.empty:\n",
    "#                 plt.plot(case_data['Percentage'], case_data['False_Detection_Rate'], \n",
    "#                         marker='x', linestyle='--', linewidth=2, label=f\"{algorithm.capitalize()}\")\n",
    "        \n",
    "#         plt.title(f'Case {case} - False Detection Rate', fontsize=14)\n",
    "#         plt.xlabel('PUEA Percentage in Dataset (%)', fontsize=12)\n",
    "#         plt.ylabel('False Detection Rate', fontsize=12)\n",
    "#         plt.grid(True)\n",
    "#         plt.xticks(percentages)\n",
    "#         plt.ylim(0, 1.05)\n",
    "#         if i == 0:  # Add legend only to the first row\n",
    "#             plt.legend(loc='upper right')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.subplots_adjust(top=0.92)\n",
    "#     plt.suptitle('Comparative PUEA Detection Performance Across All Algorithms', fontsize=16)\n",
    "    \n",
    "#     plt.savefig(f\"{output_dir}/comparative_detection_performance.png\", dpi=300, bbox_inches='tight')\n",
    "#     plt.close()\n",
    "#     print(f\"\\nComparative detection performance visualization saved to {output_dir}/comparative_detection_performance.png\")\n",
    "\n",
    "# print(\"\\nClustering analysis completed with multiple algorithms!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Cell 3: Improved clustering algorithms with better distance matrix utilization\n",
    "    from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "    from sklearn.manifold import MDS\n",
    "    from sklearn.neighbors import NearestNeighbors, kneighbors_graph\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from collections import Counter\n",
    "\n",
    "    # Dictionary to store clustering results\n",
    "    clustering_results = {}\n",
    "\n",
    "    # Function to perform DBSCAN clustering with KNN refinement\n",
    "    def perform_dbscan(distance_matrix, k=0.4, min_points=None):\n",
    "        \"\"\"\n",
    "        Perform DBSCAN clustering with parameters based on the specified approach:\n",
    "        - minPoints (min_samples) is set to n/2 where n is the number of time slots (rows in distance matrix)\n",
    "        - epsilon is calculated as max(D) * k where D is the distance matrix\n",
    "        \n",
    "        Parameters:\n",
    "            distance_matrix: Precomputed Manhattan distance matrix\n",
    "            k: Multiplier for epsilon calculation (default 0.4, can be tuned)\n",
    "            min_points: Override for min_points calculation (if None, will use n/2)\n",
    "            \n",
    "        Returns:\n",
    "            Array of cluster labels for each data point\n",
    "        \"\"\"\n",
    "        # Get number of time slots (n)\n",
    "        n = distance_matrix.shape[0]\n",
    "        \n",
    "        # Set minPoints to n/2 as specified\n",
    "        if min_points is None:\n",
    "            min_points = max(2, int(n / 2))\n",
    "            print(f\"Setting minPoints to n/2: {min_points}\")\n",
    "        \n",
    "        # Calculate epsilon as max(D) * k\n",
    "        max_distance = np.max(distance_matrix)\n",
    "        epsilon = max_distance * k\n",
    "        print(f\"Calculated epsilon = max(D) * k = {max_distance:.2f} * {k} = {epsilon:.2f}\")\n",
    "        \n",
    "        # Apply DBSCAN with calculated parameters\n",
    "        dbscan = DBSCAN(eps=epsilon, min_samples=min_points, metric='precomputed')\n",
    "        labels = dbscan.fit_predict(distance_matrix)\n",
    "        \n",
    "        # Check number of clusters (excluding noise)\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        noise_percentage = 100 * np.sum(labels == -1) / len(labels)\n",
    "        print(f\"DBSCAN produced {n_clusters} clusters with epsilon={epsilon:.2f}, minPoints={min_points}\")\n",
    "        print(f\"Noise percentage: {noise_percentage:.2f}%\")\n",
    "        \n",
    "        # If we didn't get the expected 2 clusters or have too much noise, try different k values\n",
    "        if n_clusters != 2 or noise_percentage > 30:\n",
    "            print(f\"DBSCAN produced {n_clusters} clusters with {noise_percentage:.2f}% noise, trying different parameters...\")\n",
    "            \n",
    "            # Try a range of k values and min_points adjustments\n",
    "            k_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "            min_points_factors = [0.25, 0.33, 0.5, 0.75]  # Different fractions of n\n",
    "            \n",
    "            best_labels = labels\n",
    "            best_n_clusters = n_clusters\n",
    "            best_noise = noise_percentage\n",
    "            \n",
    "            for test_k in k_values:\n",
    "                for mp_factor in min_points_factors:\n",
    "                    test_min_points = max(2, int(n * mp_factor))\n",
    "                    if test_k == k and test_min_points == min_points:  # Skip the value we already tried\n",
    "                        continue\n",
    "                        \n",
    "                    test_epsilon = max_distance * test_k\n",
    "                    test_dbscan = DBSCAN(eps=test_epsilon, min_samples=test_min_points, metric='precomputed')\n",
    "                    test_labels = test_dbscan.fit_predict(distance_matrix)\n",
    "                    test_n_clusters = len(set(test_labels)) - (1 if -1 in test_labels else 0)\n",
    "                    test_noise = 100 * np.sum(test_labels == -1) / len(test_labels)\n",
    "                    \n",
    "                    # We want exactly 2 clusters with lower noise\n",
    "                    if test_n_clusters == 2 and test_noise < best_noise:\n",
    "                        best_labels = test_labels\n",
    "                        best_n_clusters = 2\n",
    "                        best_noise = test_noise\n",
    "                        print(f\"  Found better parameters: k={test_k}, min_points={test_min_points}, noise={test_noise:.2f}%\")\n",
    "                        \n",
    "                        # If we find a good solution with low noise, we can stop\n",
    "                        if test_noise < 15:\n",
    "                            print(f\"  Found excellent solution with low noise: {test_noise:.2f}%\")\n",
    "                            labels = test_labels\n",
    "                            n_clusters = test_n_clusters\n",
    "                            break\n",
    "                else:\n",
    "                    continue\n",
    "                break\n",
    "        \n",
    "        # Apply KNN refinement to improve cluster boundaries\n",
    "        if -1 in labels or n_clusters != 2:\n",
    "            print(\"Applying KNN refinement to improve clustering...\")\n",
    "            \n",
    "            # Use MDS to convert distance matrix to points in Euclidean space for KNN\n",
    "            mds = MDS(n_components=min(5, n-1), dissimilarity='precomputed', random_state=42)\n",
    "            points = mds.fit_transform(distance_matrix)\n",
    "            \n",
    "            # Choose k for KNN (dynamically based on dataset size)\n",
    "            knn_k = min(15, max(5, int(n/10)))\n",
    "            print(f\"Using k={knn_k} for KNN refinement\")\n",
    "            \n",
    "            # Create a KNN graph\n",
    "            nbrs = NearestNeighbors(n_neighbors=knn_k).fit(points)\n",
    "            distances, indices = nbrs.kneighbors(points)\n",
    "            \n",
    "            # Handle noise points first\n",
    "            refined_labels = np.copy(labels)\n",
    "            \n",
    "            # Assign noise points to clusters based on nearest neighbors\n",
    "            if -1 in labels:\n",
    "                noise_indices = np.where(labels == -1)[0]\n",
    "                for i in noise_indices:\n",
    "                    # Get neighbors that aren't noise\n",
    "                    valid_neighbors = [idx for idx in indices[i] if labels[idx] != -1]\n",
    "                    if valid_neighbors:\n",
    "                        # Assign to most common class among neighbors\n",
    "                        neighbor_labels = [labels[idx] for idx in valid_neighbors]\n",
    "                        most_common = Counter(neighbor_labels).most_common(1)[0][0]\n",
    "                        refined_labels[i] = most_common\n",
    "            \n",
    "            # Refine edge points (points that might be misclassified)\n",
    "            for i in range(len(points)):\n",
    "                # Get labels of neighbors\n",
    "                neighbor_labels = [refined_labels[idx] for idx in indices[i]]\n",
    "                counts = Counter(neighbor_labels)\n",
    "                # If majority of neighbors have different label, change it\n",
    "                if counts.most_common(1)[0][0] != refined_labels[i] and counts.most_common(1)[0][1] > knn_k/2:\n",
    "                    refined_labels[i] = counts.most_common(1)[0][0]\n",
    "            \n",
    "            # Ensure we have exactly 2 clusters now\n",
    "            unique_labels = list(set(refined_labels))\n",
    "            if len(unique_labels) == 2:\n",
    "                print(\"KNN refinement successful - achieved 2 clusters\")\n",
    "                \n",
    "                # Map unique labels to 0 and 1 if needed\n",
    "                if unique_labels != [0, 1]:\n",
    "                    new_labels = np.zeros_like(refined_labels)\n",
    "                    new_labels[refined_labels == unique_labels[1]] = 1\n",
    "                    return new_labels\n",
    "                return refined_labels\n",
    "                \n",
    "            elif len(unique_labels) > 2:\n",
    "                print(f\"KNN refinement produced {len(unique_labels)} clusters - merging to get 2\")\n",
    "                \n",
    "                # Merge smaller clusters into the two largest ones\n",
    "                cluster_sizes = [(label, np.sum(refined_labels == label)) for label in unique_labels]\n",
    "                top_two = sorted(cluster_sizes, key=lambda x: x[1], reverse=True)[:2]\n",
    "                main_clusters = [top_two[0][0], top_two[1][0]]\n",
    "                \n",
    "                # Create final labels\n",
    "                final_labels = np.zeros_like(refined_labels)\n",
    "                final_labels[refined_labels == main_clusters[1]] = 1\n",
    "                \n",
    "                # Assign remaining clusters to closest main cluster\n",
    "                for label in unique_labels:\n",
    "                    if label not in main_clusters:\n",
    "                        mask = refined_labels == label\n",
    "                        if np.any(mask):\n",
    "                            # Find closest main cluster\n",
    "                            pts = points[mask]\n",
    "                            centroid0 = np.mean(points[refined_labels == main_clusters[0]], axis=0)\n",
    "                            centroid1 = np.mean(points[refined_labels == main_clusters[1]], axis=0)\n",
    "                            \n",
    "                            dist0 = np.mean(np.sum((pts - centroid0)**2, axis=1))\n",
    "                            dist1 = np.mean(np.sum((pts - centroid1)**2, axis=1))\n",
    "                            \n",
    "                            assigned_cluster = 0 if dist0 < dist1 else 1\n",
    "                            final_labels[mask] = assigned_cluster\n",
    "                \n",
    "                return final_labels\n",
    "            else:\n",
    "                # If KNN created only 1 cluster, force 2 clusters with K-means\n",
    "                print(\"KNN refinement produced only 1 cluster - using K-means to create 2\")\n",
    "                kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "                final_labels = kmeans.fit_predict(points)\n",
    "                return final_labels\n",
    "        \n",
    "        # If we already have 2 clusters, just normalize labels to 0 and 1\n",
    "        if n_clusters == 2:\n",
    "            unique_non_noise = sorted([l for l in set(labels) if l != -1])\n",
    "            if len(unique_non_noise) == 2 and (unique_non_noise != [0, 1]):\n",
    "                final_labels = np.zeros_like(labels)\n",
    "                final_labels[labels == unique_non_noise[1]] = 1\n",
    "                return final_labels\n",
    "        \n",
    "        return labels\n",
    "\n",
    "    # Enhanced Agglomerative clustering with improved KNN refinement\n",
    "    def perform_agglomerative(distance_matrix, n_clusters=2, linkage='average'):\n",
    "        # Try different linkage methods with more sophisticated evaluation\n",
    "        linkage_methods = ['average', 'complete', 'ward', 'single']\n",
    "        best_labels = None\n",
    "        best_score = -float('inf')  # Combined quality metric\n",
    "        best_method = None\n",
    "        \n",
    "        for method in linkage_methods:\n",
    "            try:\n",
    "                # Ward linkage needs Euclidean distances\n",
    "                if method == 'ward':\n",
    "                    # Use MDS with higher dimensions to get better Euclidean representation\n",
    "                    mds = MDS(n_components=min(8, distance_matrix.shape[0]-1), \n",
    "                            dissimilarity='precomputed', random_state=42,\n",
    "                            n_init=3, max_iter=300)  # Multiple initializations, more iterations\n",
    "                    euclidean_points = mds.fit_transform(distance_matrix)\n",
    "                    current_labels = AgglomerativeClustering(\n",
    "                        n_clusters=n_clusters, \n",
    "                        linkage=method\n",
    "                    ).fit_predict(euclidean_points)\n",
    "                else:\n",
    "                    # Use precomputed distance matrix directly\n",
    "                    current_labels = AgglomerativeClustering(\n",
    "                        n_clusters=n_clusters, \n",
    "                        linkage=method,\n",
    "                        metric='precomputed'\n",
    "                    ).fit_predict(distance_matrix)\n",
    "                \n",
    "                # Skip methods that produce only one cluster\n",
    "                if len(np.unique(current_labels)) < 2:\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate multiple evaluation metrics for better method selection\n",
    "                try:\n",
    "                    # For precomputed distances, we need to extract points\n",
    "                    mds_eval = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "                    points = mds_eval.fit_transform(distance_matrix)\n",
    "                    \n",
    "                    # Calculate silhouette score\n",
    "                    silhouette = silhouette_score(points, current_labels)\n",
    "                    \n",
    "                    # Calculate Davies-Bouldin Index (lower is better)\n",
    "                    from sklearn.metrics import davies_bouldin_score\n",
    "                    davies_bouldin = -davies_bouldin_score(points, current_labels)  # Negate so higher is better\n",
    "                    \n",
    "                    # Calculate cluster compactness (intra-cluster distances)\n",
    "                    intra_cluster_distances = []\n",
    "                    for cluster_id in range(n_clusters):\n",
    "                        cluster_points = points[current_labels == cluster_id]\n",
    "                        if len(cluster_points) > 1:\n",
    "                            centroid = np.mean(cluster_points, axis=0)\n",
    "                            avg_distance = np.mean([np.sum((pt - centroid) ** 2) for pt in cluster_points])\n",
    "                            intra_cluster_distances.append(avg_distance)\n",
    "                    \n",
    "                    compactness = -np.mean(intra_cluster_distances) if intra_cluster_distances else 0  # Negate so higher is better\n",
    "                    \n",
    "                    # Calculate balanced cluster size score (penalize highly imbalanced clusters)\n",
    "                    cluster_sizes = [np.sum(current_labels == i) for i in range(n_clusters)]\n",
    "                    balance = -np.std(cluster_sizes) / np.mean(cluster_sizes) if np.mean(cluster_sizes) > 0 else -1.0\n",
    "                    \n",
    "                    # Combined score (weighted average of metrics)\n",
    "                    combined_score = (0.5 * silhouette + \n",
    "                                    0.2 * davies_bouldin + \n",
    "                                    0.2 * compactness + \n",
    "                                    0.1 * balance)\n",
    "                    \n",
    "                    print(f\"  Linkage '{method}': silhouette={silhouette:.3f}, davies={davies_bouldin:.3f}, balance={balance:.3f}, combined={combined_score:.3f}\")\n",
    "                    \n",
    "                    if combined_score > best_score:\n",
    "                        best_score = combined_score\n",
    "                        best_labels = current_labels\n",
    "                        best_method = method\n",
    "                        print(f\"  Found new best method: {method} with score {combined_score:.4f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error calculating metrics for {method}: {str(e)}\")\n",
    "                    # If metric calculation fails but we don't have any labels yet, keep these\n",
    "                    if best_labels is None:\n",
    "                        best_labels = current_labels\n",
    "                        best_method = method\n",
    "            except Exception as e:\n",
    "                print(f\"  Error with linkage method '{method}': {str(e)}\")\n",
    "        \n",
    "        # If no good method found, use average linkage as fallback\n",
    "        if best_labels is None:\n",
    "            print(\"No suitable linkage method found. Using average linkage as fallback.\")\n",
    "            try:\n",
    "                agg = AgglomerativeClustering(n_clusters=n_clusters, linkage='average', metric='precomputed')\n",
    "                best_labels = agg.fit_predict(distance_matrix)\n",
    "                best_method = 'average'\n",
    "            except Exception:\n",
    "                # Last resort: apply K-means on MDS coordinates\n",
    "                print(\"Fallback to K-means on MDS coordinates\")\n",
    "                mds = MDS(n_components=3, dissimilarity='precomputed', random_state=42)\n",
    "                points = mds.fit_transform(distance_matrix)\n",
    "                kmeans = KMeans(n_clusters=n_clusters, n_init=15, random_state=42)\n",
    "                best_labels = kmeans.fit_predict(points)\n",
    "        \n",
    "        print(f\"Selected linkage method: {best_method}\")\n",
    "        \n",
    "        # Apply enhanced multi-scale KNN refinement to improve cluster boundaries\n",
    "        print(\"Applying enhanced KNN refinement to Agglomerative clustering results...\")\n",
    "        \n",
    "        # Store original agglomerative labels for ensemble\n",
    "        original_labels = np.copy(best_labels)\n",
    "        \n",
    "        # Get number of data points\n",
    "        n = distance_matrix.shape[0]\n",
    "        \n",
    "        # Create points for KNN refinement with higher dimensionality\n",
    "        mds_points = MDS(n_components=min(8, n-1), dissimilarity='precomputed', random_state=42)\n",
    "        points = mds_points.fit_transform(distance_matrix)\n",
    "        \n",
    "        # Apply multi-scale refinement with different neighborhood sizes\n",
    "        knn_scales = [\n",
    "            max(3, int(n/25)),        # Very local neighborhood\n",
    "            max(7, int(n/15)),        # Local neighborhood\n",
    "            min(20, max(10, int(n/8))) # Extended neighborhood\n",
    "        ]\n",
    "        print(f\"Using multi-scale KNN refinement with scales: {knn_scales}\")\n",
    "        \n",
    "        refined_labels = np.copy(original_labels)\n",
    "        confidence_map = np.ones(n)  # Confidence in current label assignment\n",
    "        \n",
    "        # First pass: Progressive refinement at increasing scales\n",
    "        for knn_k in knn_scales:\n",
    "            # Create KNN graph\n",
    "            nbrs = NearestNeighbors(n_neighbors=knn_k).fit(points)\n",
    "            distances, indices = nbrs.kneighbors(points)\n",
    "            \n",
    "            # Normalize distances for weighting\n",
    "            distances = distances / (np.max(distances) if np.max(distances) > 0 else 1.0)\n",
    "            weights = 1.0 - distances  # Closer neighbors have higher weight\n",
    "            \n",
    "            # Refine boundary points using distance-weighted KNN voting\n",
    "            for i in range(n):\n",
    "                # Skip high-confidence points at larger scales\n",
    "                if knn_k > 10 and confidence_map[i] > 0.85:\n",
    "                    continue\n",
    "                    \n",
    "                # Get weighted votes from neighbors\n",
    "                weighted_votes = {0: 0.0, 1: 0.0}\n",
    "                \n",
    "                for j, idx in enumerate(indices[i]):\n",
    "                    weighted_votes[refined_labels[idx]] = weighted_votes.get(refined_labels[idx], 0.0) + weights[i, j]\n",
    "                    \n",
    "                # Normalize the votes\n",
    "                total_votes = sum(weighted_votes.values())\n",
    "                if total_votes > 0:\n",
    "                    for label in weighted_votes:\n",
    "                        weighted_votes[label] /= total_votes\n",
    "                    \n",
    "                    # Get majority label and confidence\n",
    "                    majority_label = max(weighted_votes.items(), key=lambda x: x[1])[0]\n",
    "                    confidence = weighted_votes[majority_label]\n",
    "                    \n",
    "                    # Only change if confidence is high enough and different from current\n",
    "                    if confidence > 0.65 and majority_label != refined_labels[i]:\n",
    "                        refined_labels[i] = majority_label\n",
    "                        confidence_map[i] = confidence\n",
    "        \n",
    "        # Second pass: Apply graph-based spectral clustering for smoother boundaries\n",
    "        # Create KNN similarity graph\n",
    "        optimal_k = min(25, max(15, int(n/6)))\n",
    "        knn_graph = kneighbors_graph(points, n_neighbors=optimal_k, mode='distance')\n",
    "        \n",
    "        # Convert distances to similarities\n",
    "        sigma = np.median(knn_graph.data) * 1.5  # Adaptive kernel width based on median distance\n",
    "        knn_graph.data = np.exp(-knn_graph.data**2 / (2. * sigma**2))\n",
    "        \n",
    "        # Initialize spectral clustering with refined labels as guidance\n",
    "        from sklearn.cluster import SpectralClustering\n",
    "        spectral = SpectralClustering(n_clusters=2, affinity='precomputed', \n",
    "                                    assign_labels='kmeans', random_state=42,\n",
    "                                    n_init=10)  # Multiple initializations\n",
    "        spectral_labels = spectral.fit_predict(knn_graph)\n",
    "        \n",
    "        # Check if spectral labels match refined labels\n",
    "        agreement = np.mean(refined_labels == spectral_labels)\n",
    "        if agreement < 0.5:\n",
    "            # If the labels are significantly different, invert the spectral labels\n",
    "            spectral_labels = 1 - spectral_labels\n",
    "            agreement = np.mean(refined_labels == spectral_labels)\n",
    "        \n",
    "        print(f\"Agreement between KNN-refined and spectral labels: {agreement:.3f}\")\n",
    "        \n",
    "        # Third pass: Create ensemble of different methods\n",
    "        # Apply consensus voting between original, refined, and spectral\n",
    "        ensemble_votes = np.zeros((n, n_clusters))\n",
    "        \n",
    "        # Add votes from each method with different weights\n",
    "        # Original agglomerative labels (weight 1.0)\n",
    "        for i, label in enumerate(original_labels):\n",
    "            ensemble_votes[i, label] += 1.0\n",
    "            \n",
    "        # KNN refined labels (weight 1.2)\n",
    "        for i, label in enumerate(refined_labels):\n",
    "            ensemble_votes[i, label] += 1.2\n",
    "        \n",
    "        # Spectral labels (weight 0.8)\n",
    "        for i, label in enumerate(spectral_labels):\n",
    "            ensemble_votes[i, label] += 0.8\n",
    "        \n",
    "        # Final labels based on weighted voting\n",
    "        final_labels = np.argmax(ensemble_votes, axis=1)\n",
    "        \n",
    "        # Check if the ensemble inverted the labels compared to original\n",
    "        if np.mean(final_labels != original_labels) > 0.5:\n",
    "            # If more than half the points changed, probably the clusters got inverted\n",
    "            # Flip them back for consistency with original labels\n",
    "            final_labels = 1 - final_labels\n",
    "            \n",
    "        # Calculate how many points changed from original agglomerative\n",
    "        changes = np.sum(final_labels != original_labels)\n",
    "        print(f\"Agglomerative clustering with enhanced refinement complete. {changes} points ({changes/n*100:.1f}%) were refined.\")\n",
    "            \n",
    "        return final_labels\n",
    "\n",
    "    # Enhanced K-means clustering with improved KNN refinement\n",
    "    def perform_kmeans(distance_matrix, df=None, n_clusters=2):\n",
    "        \"\"\"\n",
    "        Perform K-means clustering with enhanced KNN refinement for better boundary detection\n",
    "        \n",
    "        Parameters:\n",
    "            distance_matrix: Precomputed Manhattan distance matrix\n",
    "            df: Original dataframe (optional, used for feature information)\n",
    "            n_clusters: Number of clusters to identify (default 2 for PU vs PUEA)\n",
    "            \n",
    "        Returns:\n",
    "            Array of cluster labels for each data point\n",
    "        \"\"\"\n",
    "        # Get number of data points\n",
    "        n = distance_matrix.shape[0]\n",
    "        \n",
    "        # Use MDS to embed the distance matrix in a higher dimensional Euclidean space\n",
    "        print(f\"Converting distance matrix to Euclidean space using MDS...\")\n",
    "        # Increase dimension from 5 to 8 for better embedding and preservation of distance relationships\n",
    "        mds = MDS(n_components=min(8, n-1), dissimilarity='precomputed', random_state=42, \n",
    "                n_init=3, max_iter=500)  # Multiple initializations and more iterations\n",
    "        points = mds.fit_transform(distance_matrix)\n",
    "        \n",
    "        # Apply K-means with multiple initializations and more iterations for stability\n",
    "        print(f\"Performing K-means clustering with n_clusters={n_clusters}...\")\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init=20, max_iter=500, random_state=42)\n",
    "        initial_labels = kmeans.fit_predict(points)\n",
    "        \n",
    "        # Apply multi-level KNN refinement to improve cluster boundaries\n",
    "        print(\"Applying enhanced KNN refinement to K-means results...\")\n",
    "        \n",
    "        # Store original points and labels for later comparison\n",
    "        original_labels = np.copy(initial_labels)\n",
    "        \n",
    "        # Use multiple k values for KNN to capture different scales of neighborhood structure\n",
    "        knn_k_values = [\n",
    "            max(3, int(n/20)),     # Small neighborhood\n",
    "            min(15, max(5, int(n/10))),  # Medium neighborhood\n",
    "            min(30, max(10, int(n/5)))   # Large neighborhood\n",
    "        ]\n",
    "        print(f\"Using multi-scale KNN refinement with k values: {knn_k_values}\")\n",
    "        \n",
    "        # Initialize refined labels\n",
    "        refined_labels = np.copy(initial_labels)\n",
    "        \n",
    "        # First pass: Apply KNN refinement at different scales with confidence weighting\n",
    "        confidence_scores = np.ones(n)  # Initialize confidence scores\n",
    "        \n",
    "        for knn_k in knn_k_values:\n",
    "            # Create KNN graph for current scale\n",
    "            nbrs = NearestNeighbors(n_neighbors=knn_k).fit(points)\n",
    "            distances, indices = nbrs.kneighbors(points)\n",
    "            \n",
    "            # Normalize distances for weighting\n",
    "            max_dist = np.max(distances) if np.max(distances) > 0 else 1.0\n",
    "            normalized_distances = distances / max_dist\n",
    "            \n",
    "            # Calculate distance-weighted votes for each point\n",
    "            for i in range(n):\n",
    "                # Get labels of neighbors with distance-based weights\n",
    "                neighbor_weights = 1.0 - normalized_distances[i]  # Closer neighbors get higher weight\n",
    "                \n",
    "                # Get labels and their weights\n",
    "                neighbor_labels = [refined_labels[idx] for idx in indices[i]]\n",
    "                \n",
    "                # Count weighted votes for each label\n",
    "                label_votes = {}\n",
    "                for j, label in enumerate(neighbor_labels):\n",
    "                    if label not in label_votes:\n",
    "                        label_votes[label] = 0.0\n",
    "                    label_votes[label] += neighbor_weights[j]\n",
    "                \n",
    "                # Find label with highest weighted vote\n",
    "                max_vote_label = max(label_votes.items(), key=lambda x: x[1])[0]\n",
    "                \n",
    "                # Calculate confidence in this classification\n",
    "                total_votes = sum(label_votes.values())\n",
    "                winning_vote_strength = label_votes[max_vote_label] / total_votes if total_votes > 0 else 0.0\n",
    "                \n",
    "                # Only change label if the winning vote is strong enough\n",
    "                if winning_vote_strength > 0.65 and max_vote_label != refined_labels[i]:\n",
    "                    refined_labels[i] = max_vote_label\n",
    "                    confidence_scores[i] = winning_vote_strength\n",
    "        \n",
    "        # Use weighted features if dataframe is provided (combining feature space and distance space)\n",
    "        if df is not None and 'Mean' in df.columns and 'Variance' in df.columns:\n",
    "            # Extract key features for PU vs PUEA discrimination - give more weight to discriminative features\n",
    "            features = df[['Mean', 'Variance', 'Median', 'Lower Quartile', 'Upper Quartile']].values\n",
    "            \n",
    "            # Standardize features\n",
    "            scaler = StandardScaler()\n",
    "            scaled_features = scaler.fit_transform(features)\n",
    "            \n",
    "            # Apply PCA to extract most discriminative directions\n",
    "            pca = PCA(n_components=min(3, scaled_features.shape[1]))\n",
    "            pca_features = pca.fit_transform(scaled_features)\n",
    "            print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "            \n",
    "            # Create cluster centroids in feature space\n",
    "            centroid0 = np.mean(pca_features[refined_labels == 0], axis=0)\n",
    "            centroid1 = np.mean(pca_features[refined_labels == 1], axis=0)\n",
    "            \n",
    "            # Identify ambiguous points (points near cluster boundaries)\n",
    "            ambiguous_scores = np.zeros(n)\n",
    "            for i in range(n):\n",
    "                # Calculate distance to both centroids\n",
    "                dist0 = np.sum((pca_features[i] - centroid0) ** 2)\n",
    "                dist1 = np.sum((pca_features[i] - centroid1) ** 2)\n",
    "                \n",
    "                # Ambiguity score: how close the distances to both centroids are\n",
    "                total_dist = dist0 + dist1\n",
    "                if total_dist > 0:\n",
    "                    if refined_labels[i] == 0:\n",
    "                        ambiguous_scores[i] = dist0 / total_dist  # Lower is more confident\n",
    "                    else:\n",
    "                        ambiguous_scores[i] = dist1 / total_dist  # Lower is more confident\n",
    "            \n",
    "            # Focus on ambiguous points (points that might be misclassified based on feature space)\n",
    "            ambiguous_threshold = 0.4  # Points with ambiguity above this are suspicious\n",
    "            ambiguous_indices = np.where(ambiguous_scores > ambiguous_threshold)[0]\n",
    "            \n",
    "            if len(ambiguous_indices) > 0:\n",
    "                print(f\"Found {len(ambiguous_indices)} ambiguous points to refine using feature space\")\n",
    "                \n",
    "                # Use large neighborhood for these ambiguous points\n",
    "                largest_k = min(50, max(30, int(n/4)))\n",
    "                nbrs_large = NearestNeighbors(n_neighbors=largest_k).fit(points)\n",
    "                large_distances, large_indices = nbrs_large.kneighbors(points)\n",
    "                \n",
    "                for i in ambiguous_indices:\n",
    "                    # Get weighted votes from neighbors in larger neighborhood\n",
    "                    weights = 1.0 - large_distances[i] / np.max(large_distances[i])\n",
    "                    neighbor_labels = [refined_labels[idx] for idx in large_indices[i]]\n",
    "                    \n",
    "                    # Calculate weighted votes\n",
    "                    votes = {0: 0.0, 1: 0.0}\n",
    "                    for j, label in enumerate(neighbor_labels):\n",
    "                        votes[label] += weights[j]\n",
    "                    \n",
    "                    # Assign to class with higher weighted vote\n",
    "                    if votes[0] > votes[1]:\n",
    "                        refined_labels[i] = 0\n",
    "                    else:\n",
    "                        refined_labels[i] = 1\n",
    "        \n",
    "        # Final pass: Ensemble with spectral clustering for smoother boundaries\n",
    "        print(\"Applying spectral clustering for final refinement...\")\n",
    "        \n",
    "        # Create KNN similarity graph with optimal parameters\n",
    "        optimal_k = min(20, max(10, int(n/8)))\n",
    "        knn_graph = kneighbors_graph(points, n_neighbors=optimal_k, mode='distance')\n",
    "        \n",
    "        # Convert distances to similarities with Gaussian kernel\n",
    "        sigma = np.mean(knn_graph.data) * 2  # Adaptive sigma based on mean distance\n",
    "        knn_graph.data = np.exp(-knn_graph.data**2 / (2. * sigma**2))\n",
    "        \n",
    "        # Use spectral clustering for final refinement\n",
    "        from sklearn.cluster import SpectralClustering\n",
    "        spectral = SpectralClustering(n_clusters=2, affinity='precomputed', \n",
    "                                    assign_labels='kmeans', random_state=42, \n",
    "                                    n_init=10)  # Multiple initializations\n",
    "        spectral_labels = spectral.fit_predict(knn_graph)\n",
    "        \n",
    "        # Create ensemble of all methods\n",
    "        ensemble_votes = np.zeros((n, n_clusters))\n",
    "        \n",
    "        # Add votes from each method with different weights\n",
    "        # Original K-means labels (weight 1.0)\n",
    "        for i, label in enumerate(original_labels):\n",
    "            ensemble_votes[i, label] += 1.0\n",
    "            \n",
    "        # Refined KNN labels (weight 1.5)\n",
    "        for i, label in enumerate(refined_labels):\n",
    "            ensemble_votes[i, label] += 1.5\n",
    "            \n",
    "        # Spectral labels (weight 1.2)\n",
    "        for i, label in enumerate(spectral_labels):\n",
    "            ensemble_votes[i, label] += 1.2\n",
    "        \n",
    "        # Final labels based on majority voting\n",
    "        final_labels = np.argmax(ensemble_votes, axis=1)\n",
    "        \n",
    "        # Check if we need to flip the labels to maintain consistency\n",
    "        if np.mean(final_labels != original_labels) > 0.5:\n",
    "            final_labels = 1 - final_labels\n",
    "        \n",
    "        # Calculate how many points changed from original K-means\n",
    "        changes = np.sum(final_labels != original_labels)\n",
    "        print(f\"K-means with enhanced KNN refinement complete. {changes} points ({changes/n*100:.1f}%) were refined.\")\n",
    "        \n",
    "        return final_labels\n",
    "\n",
    "    # Function to compute purity score (agreement with true labels when available)\n",
    "    def compute_purity_score(labels, true_labels):\n",
    "        # Make sure cluster labels are comparable\n",
    "        from scipy.optimize import linear_sum_assignment\n",
    "        \n",
    "        # Create contingency matrix\n",
    "        contingency_matrix = np.zeros((np.max(labels) + 1, np.max(true_labels) + 1))\n",
    "        for i in range(len(labels)):\n",
    "            contingency_matrix[labels[i], true_labels[i]] += 1\n",
    "            \n",
    "        # Find optimal one-to-one mapping between cluster and true labels\n",
    "        row_ind, col_ind = linear_sum_assignment(-contingency_matrix)\n",
    "        \n",
    "        # Return purity score\n",
    "        return sum([contingency_matrix[row_ind[i], col_ind[i]] for i in range(len(row_ind))]) / len(labels)\n",
    "\n",
    "    # Function to visualize clustering results\n",
    "    def visualize_clustering(data, labels, title, original_labels=None):\n",
    "        # Apply PCA for visualization \n",
    "        pca = PCA(n_components=2)\n",
    "        reduced_data = pca.fit_transform(data)\n",
    "        \n",
    "        # Create a figure\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Get unique labels\n",
    "        unique_labels = np.unique(labels)\n",
    "        \n",
    "        # Plot each cluster\n",
    "        for label in unique_labels:\n",
    "            if label == -1:\n",
    "                # Noise points in black\n",
    "                mask = labels == label\n",
    "                plt.scatter(reduced_data[mask, 0], reduced_data[mask, 1], \n",
    "                            c='black', marker='x', alpha=0.5, label='Noise')\n",
    "            else:\n",
    "                # Regular clusters\n",
    "                mask = labels == label\n",
    "                plt.scatter(reduced_data[mask, 0], reduced_data[mask, 1], \n",
    "                            marker='o', alpha=0.7, label=f'Cluster {label}')\n",
    "        \n",
    "        # If original labels are provided, add agreement metrics\n",
    "        if original_labels is not None:\n",
    "            # Calculate basic agreement (assuming binary labels)\n",
    "            if len(np.unique(labels)) == 2 and len(np.unique(original_labels)) == 2:\n",
    "                # If both have exactly 2 clusters, find best mapping and show agreement\n",
    "                # Consider both possible mappings and take the better one\n",
    "                mapping1 = {0:0, 1:1}\n",
    "                mapping2 = {0:1, 1:0}\n",
    "                \n",
    "                # Map the cluster labels to the original labels using both mappings\n",
    "                mapped_labels1 = np.array([mapping1[l] if l in mapping1 else l for l in labels])\n",
    "                mapped_labels2 = np.array([mapping2[l] if l in mapping2 else l for l in labels])\n",
    "                \n",
    "                # Calculate agreements\n",
    "                agreement1 = np.mean(mapped_labels1 == original_labels) * 100\n",
    "                agreement2 = np.mean(mapped_labels2 == original_labels) * 100\n",
    "                \n",
    "                # Use the better mapping\n",
    "                agreement = max(agreement1, agreement2)\n",
    "                \n",
    "                # Also calculate adjusted Rand Index for a more robust measure\n",
    "                ari = adjusted_rand_score(original_labels, labels)\n",
    "                \n",
    "                title = f\"{title}\\nAgreement: {agreement:.1f}%, ARI: {ari:.3f}\"\n",
    "            else:\n",
    "                # If not both binary, use ARI only\n",
    "                # Filter out noise points for the calculation\n",
    "                valid_indices = labels != -1\n",
    "                if np.sum(valid_indices) > 0:\n",
    "                    ari = adjusted_rand_score(original_labels[valid_indices], labels[valid_indices])\n",
    "                    title = f\"{title}\\nAdjusted Rand Index: {ari:.3f}\"\n",
    "        \n",
    "        plt.title(title, fontsize=14)\n",
    "        plt.xlabel('Principal Component 1', fontsize=12)\n",
    "        plt.ylabel('Principal Component 2', fontsize=12)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt\n",
    "\n",
    "    # Create a directory for storing cluster visualizations\n",
    "    cluster_viz_dir = \"cluster_visualizations\"\n",
    "    if not os.path.exists(cluster_viz_dir):\n",
    "        os.makedirs(cluster_viz_dir)\n",
    "\n",
    "    # Define the cases and percentages\n",
    "    cases = ['A', 'B', 'C']\n",
    "    percentages = [10, 20, 30, 40, 50]\n",
    "\n",
    "    # Apply clustering to each case and scenario\n",
    "    for case in cases:\n",
    "        for percentage in percentages:\n",
    "            key = f\"{case}_{percentage}\"\n",
    "            \n",
    "            try:\n",
    "                # Get the distance matrix\n",
    "                distance_matrix = distance_matrices[key]\n",
    "                \n",
    "                # Read the original data\n",
    "                file_path = f\"{case}_case_{percentage}percent_matrix.csv\"\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Extract features and standardize for visualization\n",
    "                features = df.select_dtypes(include=['float64', 'int64']).values\n",
    "                scaler = StandardScaler()\n",
    "                scaled_features = scaler.fit_transform(features)\n",
    "                \n",
    "                # Get original labels if available (for calculating agreement)\n",
    "                original_labels = None\n",
    "                if 'Label' in df.columns:\n",
    "                    original_labels = df['Label'].values\n",
    "                \n",
    "                print(f\"\\nClustering {key}...\")\n",
    "                \n",
    "                # Perform DBSCAN with optimal eps calculation and KNN refinement\n",
    "                print(f\"Running DBSCAN with KNN refinement on {key}...\")\n",
    "                dbscan_labels = perform_dbscan(distance_matrix)\n",
    "                \n",
    "                # Perform Agglomerative Clustering with multiple linkage methods and KNN refinement\n",
    "                print(f\"Running Agglomerative clustering with KNN refinement on {key}...\")\n",
    "                agg_labels = perform_agglomerative(distance_matrix)\n",
    "                \n",
    "                # Perform K-means with MDS embedding and KNN refinement\n",
    "                print(f\"Running K-means with KNN refinement on {key}...\")\n",
    "                kmeans_labels = perform_kmeans(distance_matrix, df)\n",
    "                \n",
    "                # Store results\n",
    "                clustering_results[f\"{key}_dbscan\"] = dbscan_labels\n",
    "                clustering_results[f\"{key}_agglomerative\"] = agg_labels\n",
    "                clustering_results[f\"{key}_kmeans\"] = kmeans_labels\n",
    "                \n",
    "                # Print cluster distribution\n",
    "                print(f\"\\nCluster distribution for {key}:\")\n",
    "                print(f\"DBSCAN: {np.bincount(dbscan_labels + 1) if -1 in dbscan_labels else np.bincount(dbscan_labels)}\")\n",
    "                print(f\"Agglomerative: {np.bincount(agg_labels)}\")\n",
    "                print(f\"K-means: {np.bincount(kmeans_labels)}\")\n",
    "                \n",
    "                # Check if clusters match expected count\n",
    "                dbscan_n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "                print(f\"Number of clusters - DBSCAN: {dbscan_n_clusters}, Agglomerative: {len(set(agg_labels))}, K-means: {len(set(kmeans_labels))}\")\n",
    "                \n",
    "                # If original labels exist, calculate agreement metrics\n",
    "                if original_labels is not None:\n",
    "                    # Calculate Adjusted Rand Index (ARI) for each method\n",
    "                    # For DBSCAN, exclude noise points for agreement calculation\n",
    "                    if -1 in dbscan_labels:\n",
    "                        valid_indices = dbscan_labels != -1\n",
    "                        valid_dbscan = dbscan_labels[valid_indices]\n",
    "                        valid_original = original_labels[valid_indices]\n",
    "                        dbscan_ari = adjusted_rand_score(valid_original, valid_dbscan) if len(valid_dbscan) > 0 else 0\n",
    "                    else:\n",
    "                        dbscan_ari = adjusted_rand_score(original_labels, dbscan_labels)\n",
    "                    \n",
    "                    agg_ari = adjusted_rand_score(original_labels, agg_labels)\n",
    "                    kmeans_ari = adjusted_rand_score(original_labels, kmeans_labels)\n",
    "                    \n",
    "                    print(f\"Adjusted Rand Index - DBSCAN: {dbscan_ari:.3f}, Agglomerative: {agg_ari:.3f}, K-means: {kmeans_ari:.3f}\")\n",
    "                \n",
    "                # Visualize clustering results\n",
    "                # DBSCAN\n",
    "                dbscan_plot = visualize_clustering(scaled_features, dbscan_labels, \n",
    "                                                f\"DBSCAN Clustering with KNN Refinement - Case {case}, {percentage}% Data\", \n",
    "                                                original_labels)\n",
    "                dbscan_plot.savefig(f\"{cluster_viz_dir}/{key}_dbscan_clustering.png\", dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                # Agglomerative\n",
    "                agg_plot = visualize_clustering(scaled_features, agg_labels, \n",
    "                                            f\"Agglomerative Clustering with KNN Refinement - Case {case}, {percentage}% Data\",\n",
    "                                            original_labels)\n",
    "                agg_plot.savefig(f\"{cluster_viz_dir}/{key}_agglomerative_clustering.png\", dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                # K-means\n",
    "                kmeans_plot = visualize_clustering(scaled_features, kmeans_labels, \n",
    "                                                f\"K-means Clustering with KNN Refinement - Case {case}, {percentage}% Data\",\n",
    "                                                original_labels)\n",
    "                kmeans_plot.savefig(f\"{cluster_viz_dir}/{key}_kmeans_clustering.png\", dpi=300, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                print(f\"Visualizations saved for {key} in {cluster_viz_dir} directory\")\n",
    "                \n",
    "            except KeyError:\n",
    "                print(f\"Distance matrix not found for {key}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error clustering {key}: {str(e)}\")\n",
    "\n",
    "    # Save clustering results to CSV files for performance evaluation\n",
    "    print(\"\\nSaving clustering results to CSV files for performance evaluation...\")\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(cluster_viz_dir):\n",
    "        os.makedirs(cluster_viz_dir)\n",
    "\n",
    "    # Save clustering results to CSV files\n",
    "    for case in cases:\n",
    "        for percentage in percentages:\n",
    "            key = f\"{case}_{percentage}\"\n",
    "            \n",
    "            # Get the original data\n",
    "            file_path = f\"{case}_case_{percentage}percent_matrix.csv\"\n",
    "            \n",
    "            try:\n",
    "                # Check if the file exists\n",
    "                if os.path.exists(file_path):\n",
    "                    # Load the data to get the true labels if available\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    \n",
    "                    # Check if we have clustering results for this case/percentage\n",
    "                    dbscan_key = f\"{key}_dbscan\"\n",
    "                    kmeans_key = f\"{key}_kmeans\"\n",
    "                    agg_key = f\"{key}_agglomerative\"\n",
    "                    \n",
    "                    # Choose one of the clustering results (prioritize k-means, then agglomerative, then DBSCAN)\n",
    "                    if kmeans_key in clustering_results:\n",
    "                        selected_labels = clustering_results[kmeans_key]\n",
    "                        selected_algorithm = \"K-means\"\n",
    "                    elif agg_key in clustering_results:\n",
    "                        selected_labels = clustering_results[agg_key]\n",
    "                        selected_algorithm = \"Agglomerative\"\n",
    "                    elif dbscan_key in clustering_results:\n",
    "                        selected_labels = clustering_results[dbscan_key]\n",
    "                        selected_algorithm = \"DBSCAN\"\n",
    "                    else:\n",
    "                        print(f\"No clustering results found for {key}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Create DataFrame with the clustering results\n",
    "                    result_df = pd.DataFrame({\n",
    "                        'Cluster': selected_labels\n",
    "                    })\n",
    "                    \n",
    "                    # Add the original labels if available\n",
    "                    if 'Label' in df.columns:\n",
    "                        result_df['True_Label'] = df['Label'].values\n",
    "                    \n",
    "                    # Save to CSV\n",
    "                    output_path = f\"{cluster_viz_dir}/{key}_clustering_result.csv\"\n",
    "                    result_df.to_csv(output_path, index=False)\n",
    "                    print(f\"Saved clustering result for {key} ({selected_algorithm}) to {output_path}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error saving clustering result for {key}: {str(e)}\")\n",
    "\n",
    "    print(\"Clustering results saved successfully.\")\n",
    "\n",
    "    # Summary of clustering results\n",
    "    print(f\"\\nCompleted clustering for {len(clustering_results) // 3} case-scenario combinations\")\n",
    "    print(f\"Generated {len(clustering_results)} clustering results (3 algorithms per case-scenario)\")\n",
    "    print(f\"Visualization files saved in {cluster_viz_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare detection rates and create combined performance metrics\n",
    "print(\"\\n=== Comparing Detection Rates and Combined Performance Metrics ===\")\n",
    "\n",
    "# Create a directory for performance visualizations\n",
    "perf_viz_dir = \"performance_visualizations\"\n",
    "if not os.path.exists(perf_viz_dir):\n",
    "    os.makedirs(perf_viz_dir)\n",
    "\n",
    "# Dictionary to store detection metrics for all algorithms\n",
    "detection_metrics = {\n",
    "    'case': [],\n",
    "    'percentage': [],\n",
    "    'algorithm': [],\n",
    "    'puea_detection_rate': [],\n",
    "    'false_detection_rate': [],\n",
    "    'accuracy': [],\n",
    "    'ari': []\n",
    "}\n",
    "\n",
    "# Add debug info to track what results we actually have\n",
    "print(\"\\nAvailable clustering results keys:\")\n",
    "print(sorted(list(clustering_results.keys())))\n",
    "\n",
    "# Process each case and percentage\n",
    "for case in cases:\n",
    "    for percentage in percentages:\n",
    "        key = f\"{case}_{percentage}\"\n",
    "        print(f\"\\nProcessing metrics for {key}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load the original data\n",
    "            file_path = f\"{case}_case_{percentage}percent_matrix.csv\"\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"  File not found: {file_path}\")\n",
    "                continue\n",
    "                \n",
    "            df = pd.read_csv(file_path)\n",
    "            true_labels = df['Label'].values if 'Label' in df.columns else None\n",
    "            \n",
    "            if true_labels is None:\n",
    "                print(f\"  No ground truth labels found in {file_path}\")\n",
    "                continue\n",
    "                \n",
    "            # Calculate detection metrics for each algorithm\n",
    "            for alg_name, alg_key in [\n",
    "                ('DBSCAN', f\"{key}_dbscan\"),\n",
    "                ('K-means', f\"{key}_kmeans\"),\n",
    "                ('Agglomerative', f\"{key}_agglomerative\")\n",
    "            ]:\n",
    "                print(f\"  Processing algorithm: {alg_name}, key: {alg_key}\")\n",
    "                if alg_key in clustering_results:\n",
    "                    labels = clustering_results[alg_key]\n",
    "                    \n",
    "                    # Check that the labels have the expected format\n",
    "                    unique_labels = np.unique(labels)\n",
    "                    print(f\"    Found {len(unique_labels)} unique labels: {unique_labels}\")\n",
    "                    \n",
    "                    # For DBSCAN, ignore noise points (-1) in the calculation\n",
    "                    valid_indices = np.ones_like(labels, dtype=bool)\n",
    "                    if -1 in labels:\n",
    "                        valid_indices = labels != -1\n",
    "                        if not np.any(valid_indices):\n",
    "                            print(f\"    All points classified as noise for {alg_name}, skipping\")\n",
    "                            continue\n",
    "                    \n",
    "                    valid_labels = labels[valid_indices]\n",
    "                    valid_true = true_labels[valid_indices]\n",
    "                    \n",
    "                    # Count distribution of items in each cluster\n",
    "                    pu_counts = [0, 0]  # Count of PU items in clusters 0 and 1\n",
    "                    puea_counts = [0, 0]  # Count of PUEA items in clusters 0 and 1\n",
    "                    \n",
    "                    # Ensure that labels are 0 and 1\n",
    "                    if set(np.unique(valid_labels)) != {0, 1}:\n",
    "                        print(f\"    Warning: Labels are not 0 and 1, remapping: {np.unique(valid_labels)}\")\n",
    "                        # Remap labels to 0 and 1\n",
    "                        if len(np.unique(valid_labels)) == 2:\n",
    "                            label_map = {u: i for i, u in enumerate(sorted(np.unique(valid_labels)))}\n",
    "                            valid_labels = np.array([label_map[l] for l in valid_labels])\n",
    "                            print(f\"    Remapped labels: {np.unique(valid_labels)}\")\n",
    "                        else:\n",
    "                            print(f\"    Cannot remap labels, found {len(np.unique(valid_labels))} unique values\")\n",
    "                            continue\n",
    "                    \n",
    "                    for i in range(len(valid_labels)):\n",
    "                        if valid_true[i] == 0:  # PU\n",
    "                            pu_counts[valid_labels[i]] += 1\n",
    "                        else:  # PUEA\n",
    "                            puea_counts[valid_labels[i]] += 1\n",
    "                    \n",
    "                    print(f\"    PU counts: {pu_counts}, PUEA counts: {puea_counts}\")\n",
    "                    \n",
    "                    # Determine which cluster is the PUEA cluster\n",
    "                    # The cluster with higher PUEA concentration is labeled as PUEA cluster\n",
    "                    puea_concentration_0 = puea_counts[0] / (puea_counts[0] + pu_counts[0] + 0.0001)\n",
    "                    puea_concentration_1 = puea_counts[1] / (puea_counts[1] + pu_counts[1] + 0.0001)\n",
    "                    puea_cluster = 1 if puea_concentration_1 > puea_concentration_0 else 0\n",
    "                    \n",
    "                    print(f\"    PUEA cluster: {puea_cluster} (concentrations: {puea_concentration_0:.2f}, {puea_concentration_1:.2f})\")\n",
    "                    \n",
    "                    # Calculate detection rate and false detection rate\n",
    "                    total_puea = puea_counts[0] + puea_counts[1]\n",
    "                    total_pu = pu_counts[0] + pu_counts[1]\n",
    "                    \n",
    "                    puea_detection_rate = puea_counts[puea_cluster] / total_puea if total_puea > 0 else 0\n",
    "                    false_detection_rate = pu_counts[puea_cluster] / total_pu if total_pu > 0 else 0\n",
    "                    \n",
    "                    # Calculate accuracy and ARI\n",
    "                    # Map clusters to original labels (0=PU, 1=PUEA)\n",
    "                    mapped_labels = np.zeros_like(valid_labels)\n",
    "                    if puea_cluster == 1:\n",
    "                        mapped_labels = valid_labels\n",
    "                    else:\n",
    "                        mapped_labels = 1 - valid_labels\n",
    "                        \n",
    "                    accuracy = np.mean(mapped_labels == valid_true) * 100\n",
    "                    ari = adjusted_rand_score(valid_true, valid_labels)\n",
    "                    \n",
    "                    # Add to metrics dictionary\n",
    "                    detection_metrics['case'].append(case)\n",
    "                    detection_metrics['percentage'].append(percentage)\n",
    "                    detection_metrics['algorithm'].append(alg_name)\n",
    "                    detection_metrics['puea_detection_rate'].append(puea_detection_rate)\n",
    "                    detection_metrics['false_detection_rate'].append(false_detection_rate)\n",
    "                    detection_metrics['accuracy'].append(accuracy)\n",
    "                    detection_metrics['ari'].append(ari)\n",
    "                    \n",
    "                    print(f\"    {case}_{percentage}% - {alg_name}: DR={puea_detection_rate:.2f}, FDR={false_detection_rate:.2f}, Acc={accuracy:.1f}%, ARI={ari:.3f}\")\n",
    "                else:\n",
    "                    print(f\"    No clustering results found for {alg_key}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing metrics for {key}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "metrics_df = pd.DataFrame(detection_metrics)\n",
    "\n",
    "# Debug: Check what algorithms we have in the metrics\n",
    "print(\"\\nAlgorithms in metrics_df:\", metrics_df['algorithm'].unique())\n",
    "print(\"\\nMetrics data shape:\", metrics_df.shape)\n",
    "print(\"\\nMetrics data summary by algorithm:\")\n",
    "print(metrics_df.groupby('algorithm').size())\n",
    "\n",
    "# Create combined performance metric plots\n",
    "if not metrics_df.empty:\n",
    "    # Create plots for each case showing detection rates by algorithm\n",
    "    for case in cases:\n",
    "        case_data = metrics_df[metrics_df['case'] == case]\n",
    "        if case_data.empty:\n",
    "            print(f\"No data for case {case}, skipping plot\")\n",
    "            continue\n",
    "            \n",
    "        # Create figure with two subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        fig.suptitle(f'Case {case} - PUEA Detection Performance', fontsize=16)\n",
    "        \n",
    "        # Get unique algorithms for this case\n",
    "        case_algorithms = sorted(case_data['algorithm'].unique())\n",
    "        print(f\"Case {case} has algorithms: {case_algorithms}\")\n",
    "        \n",
    "        # Define distinct visual attributes for each algorithm\n",
    "        algorithm_styles = {\n",
    "            'DBSCAN': {\n",
    "                'color': 'blue', \n",
    "                'marker': 'o', \n",
    "                'linestyle': '-',\n",
    "                'linewidth': 2.5,\n",
    "                'markersize': 8,\n",
    "                'zorder': 3,\n",
    "                'label': 'DBSCAN'\n",
    "            },\n",
    "            'K-means': {\n",
    "                'color': 'red', \n",
    "                'marker': 's', \n",
    "                'linestyle': '--',\n",
    "                'linewidth': 2,\n",
    "                'markersize': 9,\n",
    "                'zorder': 2,\n",
    "                'label': 'K-means'\n",
    "            },\n",
    "            'Agglomerative': {\n",
    "                'color': 'green', \n",
    "                'marker': '^', \n",
    "                'linestyle': '-.',\n",
    "                'linewidth': 2.2,\n",
    "                'markersize': 10,\n",
    "                'zorder': 1,\n",
    "                'label': 'Agglomerative'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Plot PUEA Detection Rate with enhanced visual distinction\n",
    "        for alg in case_algorithms:\n",
    "            alg_data = case_data[case_data['algorithm'] == alg].sort_values('percentage')\n",
    "            if len(alg_data) == 0:\n",
    "                print(f\"  No data points for {alg}, skipping\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"Plotting {alg} with {len(alg_data)} points\")\n",
    "            style = algorithm_styles[alg]\n",
    "            \n",
    "            # Print data points being plotted\n",
    "            print(f\"  Detection rate data for {alg}: {list(zip(alg_data['percentage'], alg_data['puea_detection_rate']))}\")\n",
    "            \n",
    "            # Plot with enhanced visibility\n",
    "            line, = ax1.plot(alg_data['percentage'], alg_data['puea_detection_rate'], \n",
    "                    marker=style['marker'], \n",
    "                    linestyle=style['linestyle'], \n",
    "                    label=style['label'],\n",
    "                    color=style['color'], \n",
    "                    linewidth=style['linewidth'], \n",
    "                    markersize=style['markersize'],\n",
    "                    zorder=style['zorder'],\n",
    "                    markeredgecolor='black',\n",
    "                    markeredgewidth=0.8)\n",
    "            \n",
    "            # Add data point labels for clarity when lines overlap\n",
    "            for x, y in zip(alg_data['percentage'], alg_data['puea_detection_rate']):\n",
    "                ax1.annotate(f'{y:.2f}', \n",
    "                            xy=(x, y),\n",
    "                            xytext=(0, 5),  # 5 points vertical offset\n",
    "                            textcoords='offset points',\n",
    "                            ha='center',\n",
    "                            fontsize=8,\n",
    "                            color=style['color'],\n",
    "                            fontweight='bold')\n",
    "        \n",
    "        # Customize PUEA Detection Rate plot\n",
    "        ax1.set_title('PUEA Detection Rate', fontsize=14)\n",
    "        ax1.set_xlabel('PUEA Percentage (%)', fontsize=12)\n",
    "        ax1.set_ylabel('Detection Rate', fontsize=12)\n",
    "        ax1.set_xticks(percentages)\n",
    "        ax1.set_ylim(0, 1.05)\n",
    "        ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax1.legend(loc='best', fontsize=10, framealpha=0.8, edgecolor='black')\n",
    "        \n",
    "        # Plot False Detection Rate with enhanced visual distinction\n",
    "        for alg in case_algorithms:\n",
    "            alg_data = case_data[case_data['algorithm'] == alg].sort_values('percentage')\n",
    "            if len(alg_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            style = algorithm_styles[alg]\n",
    "            \n",
    "            # Print data points being plotted\n",
    "            print(f\"  False detection rate data for {alg}: {list(zip(alg_data['percentage'], alg_data['false_detection_rate']))}\")\n",
    "            \n",
    "            # Plot with enhanced visibility\n",
    "            line, = ax2.plot(alg_data['percentage'], alg_data['false_detection_rate'], \n",
    "                    marker=style['marker'], \n",
    "                    linestyle=style['linestyle'], \n",
    "                    label=style['label'],\n",
    "                    color=style['color'], \n",
    "                    linewidth=style['linewidth'], \n",
    "                    markersize=style['markersize'],\n",
    "                    zorder=style['zorder'],\n",
    "                    markeredgecolor='black',\n",
    "                    markeredgewidth=0.8)\n",
    "                    \n",
    "            # Add data point labels for clarity when lines overlap\n",
    "            for x, y in zip(alg_data['percentage'], alg_data['false_detection_rate']):\n",
    "                ax2.annotate(f'{y:.2f}', \n",
    "                            xy=(x, y),\n",
    "                            xytext=(0, 5),  # 5 points vertical offset\n",
    "                            textcoords='offset points',\n",
    "                            ha='center',\n",
    "                            fontsize=8,\n",
    "                            color=style['color'],\n",
    "                            fontweight='bold')\n",
    "        \n",
    "        # Customize False Detection Rate plot\n",
    "        ax2.set_title('False Detection Rate', fontsize=14)\n",
    "        ax2.set_xlabel('PUEA Percentage (%)', fontsize=12)\n",
    "        ax2.set_ylabel('False Detection Rate', fontsize=12)\n",
    "        ax2.set_xticks(percentages)\n",
    "        ax2.set_ylim(0, 1.05)\n",
    "        ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax2.legend(loc='best', fontsize=10, framealpha=0.8, edgecolor='black')\n",
    "        \n",
    "        # Add case description title\n",
    "        case_descriptions = {'A': 'Far Distance', 'B': 'Medium Distance', 'C': 'Close Distance'}\n",
    "        if case in case_descriptions:\n",
    "            fig.suptitle(f'Case {case} ({case_descriptions[case]}) - PUEA Detection Performance', fontsize=16)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.88)\n",
    "        \n",
    "        # Save figure\n",
    "        plt.savefig(f\"{perf_viz_dir}/case_{case}_detection_rates.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # Create a summary plot comparing algorithm performance across cases\n",
    "    fig, axes = plt.subplots(len(cases), 2, figsize=(15, 4*len(cases)))\n",
    "    \n",
    "    for i, case in enumerate(cases):\n",
    "        case_metrics = metrics_df[metrics_df['case'] == case]\n",
    "        \n",
    "        if case_metrics.empty:\n",
    "            continue\n",
    "            \n",
    "        # Detection Rate plot\n",
    "        ax1 = axes[i, 0]\n",
    "        \n",
    "        # Group by algorithm and percentage, calculate mean detection rate\n",
    "        pivot_dr = case_metrics.pivot_table(\n",
    "            index='percentage', \n",
    "            columns='algorithm', \n",
    "            values='puea_detection_rate',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        # Print pivot table for debugging\n",
    "        print(f\"\\nPivot table for case {case} detection rates:\")\n",
    "        print(pivot_dr)\n",
    "        \n",
    "        # Plot detection rate lines with distinct styles\n",
    "        colors = ['blue', 'red', 'green']\n",
    "        markers = ['o', 's', '^']\n",
    "        linestyles = ['-', '--', '-.']\n",
    "        \n",
    "        for j, alg in enumerate(pivot_dr.columns):\n",
    "            ax1.plot(pivot_dr.index, pivot_dr[alg], \n",
    "                     marker=markers[j % len(markers)],\n",
    "                     linestyle=linestyles[j % len(linestyles)],\n",
    "                     color=colors[j % len(colors)],\n",
    "                     linewidth=2,\n",
    "                     markersize=8,\n",
    "                     markeredgecolor='black',\n",
    "                     markeredgewidth=0.8,\n",
    "                     label=alg)\n",
    "            \n",
    "            # Add data labels\n",
    "            for x, y in zip(pivot_dr.index, pivot_dr[alg]):\n",
    "                ax1.annotate(f'{y:.2f}', \n",
    "                            xy=(x, y),\n",
    "                            xytext=(0, 5),\n",
    "                            textcoords='offset points',\n",
    "                            ha='center',\n",
    "                            fontsize=8,\n",
    "                            color=colors[j % len(colors)],\n",
    "                            fontweight='bold')\n",
    "        \n",
    "        ax1.set_title(f'Case {case} - PUEA Detection Rate', fontsize=13)\n",
    "        ax1.set_xlabel('PUEA Percentage (%)', fontsize=11)\n",
    "        ax1.set_ylabel('Detection Rate', fontsize=11)\n",
    "        ax1.set_ylim(0, 1.05)\n",
    "        ax1.set_xticks(percentages)\n",
    "        ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax1.legend(loc='best', fontsize=9)\n",
    "        \n",
    "        # False Detection Rate plot\n",
    "        ax2 = axes[i, 1]\n",
    "        \n",
    "        # Group by algorithm and percentage, calculate mean false detection rate\n",
    "        pivot_fdr = case_metrics.pivot_table(\n",
    "            index='percentage', \n",
    "            columns='algorithm', \n",
    "            values='false_detection_rate',\n",
    "            aggfunc='mean'\n",
    "        )\n",
    "        \n",
    "        # Print pivot table for debugging\n",
    "        print(f\"\\nPivot table for case {case} false detection rates:\")\n",
    "        print(pivot_fdr)\n",
    "        \n",
    "        # Plot false detection rate lines with distinct styles\n",
    "        for j, alg in enumerate(pivot_fdr.columns):\n",
    "            ax2.plot(pivot_fdr.index, pivot_fdr[alg], \n",
    "                     marker=markers[j % len(markers)],\n",
    "                     linestyle=linestyles[j % len(linestyles)],\n",
    "                     color=colors[j % len(colors)],\n",
    "                     linewidth=2,\n",
    "                     markersize=8,\n",
    "                     markeredgecolor='black',\n",
    "                     markeredgewidth=0.8,\n",
    "                     label=alg)\n",
    "            \n",
    "            # Add data labels\n",
    "            for x, y in zip(pivot_fdr.index, pivot_fdr[alg]):\n",
    "                ax2.annotate(f'{y:.2f}', \n",
    "                            xy=(x, y),\n",
    "                            xytext=(0, 5),\n",
    "                            textcoords='offset points',\n",
    "                            ha='center',\n",
    "                            fontsize=8,\n",
    "                            color=colors[j % len(colors)],\n",
    "                            fontweight='bold')\n",
    "        \n",
    "        ax2.set_title(f'Case {case} - False Detection Rate', fontsize=13)\n",
    "        ax2.set_xlabel('PUEA Percentage (%)', fontsize=11)\n",
    "        ax2.set_ylabel('False Detection Rate', fontsize=11)\n",
    "        ax2.set_ylim(0, 1.05)\n",
    "        ax2.set_xticks(percentages)\n",
    "        ax2.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax2.legend(loc='best', fontsize=9)\n",
    "        \n",
    "        # Add case description to y-axis label\n",
    "        case_descriptions = {'A': 'Far Distance', 'B': 'Medium Distance', 'C': 'Close Distance'}\n",
    "        if case in case_descriptions:\n",
    "            ax1.set_ylabel(f\"Detection Rate\\nCase {case}: {case_descriptions[case]}\", fontsize=11)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{perf_viz_dir}/all_cases_detection_performance.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a heatmap of detection performance by algorithm and case\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    \n",
    "    # Calculate average detection rate for each algorithm/case combination\n",
    "    avg_metrics = metrics_df.groupby(['algorithm', 'case']).agg({\n",
    "        'puea_detection_rate': 'mean',\n",
    "        'false_detection_rate': 'mean',\n",
    "        'accuracy': 'mean',\n",
    "        'ari': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Print the aggregated metrics\n",
    "    print(\"\\nAggregated metrics by algorithm and case:\")\n",
    "    print(avg_metrics)\n",
    "    \n",
    "    # Create pivot table for heatmap\n",
    "    pivot_metrics = avg_metrics.pivot_table(\n",
    "        index='algorithm', \n",
    "        columns='case', \n",
    "        values='puea_detection_rate'\n",
    "    )\n",
    "    \n",
    "    # Print the pivot table for the heatmap\n",
    "    print(\"\\nPivot table for heatmap:\")\n",
    "    print(pivot_metrics)\n",
    "    \n",
    "    # Plot heatmap (only if we have data)\n",
    "    if not pivot_metrics.empty:\n",
    "        cmap = sns.cm.rocket_r  # Using a perceptually uniform colormap\n",
    "        sns.heatmap(pivot_metrics, annot=True, cmap=cmap, fmt='.2f', vmin=0, vmax=1, \n",
    "                   annot_kws={\"size\": 14, \"weight\": \"bold\"},\n",
    "                   linewidths=0.5, linecolor='white', cbar_kws={\"shrink\": 0.8})\n",
    "        plt.title('Average PUEA Detection Rate by Algorithm and Case', fontsize=16, pad=20)\n",
    "        \n",
    "        # Add case descriptions as second row of column labels\n",
    "        case_descriptions = {'A': 'Far Distance', 'B': 'Medium Distance', 'C': 'Close Distance'}\n",
    "        ax = plt.gca()\n",
    "        for i, case in enumerate(pivot_metrics.columns):\n",
    "            ax.text(i + 0.5, -0.15, case_descriptions.get(case, ''),\n",
    "                   ha='center', va='center', fontsize=10, style='italic')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{perf_viz_dir}/detection_rate_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"Warning: Empty pivot table for heatmap, skipping heatmap generation\")\n",
    "    \n",
    "    # Save metrics to CSV for further analysis\n",
    "    metrics_df.to_csv(f\"{perf_viz_dir}/detection_performance_metrics.csv\", index=False)\n",
    "    print(f\"\\nDetection performance metrics and visualizations saved to {perf_viz_dir}/\")\n",
    "    \n",
    "    # Print best performing algorithm for each case\n",
    "    print(\"\\n=== Best Performing Algorithm by Case ===\")\n",
    "    for case in cases:\n",
    "        case_data = metrics_df[metrics_df['case'] == case]\n",
    "        if not case_data.empty:\n",
    "            # Group by algorithm and find the one with highest detection rate and lowest false detection rate\n",
    "            avg_by_alg = case_data.groupby('algorithm').agg({\n",
    "                'puea_detection_rate': 'mean',\n",
    "                'false_detection_rate': 'mean',\n",
    "                'accuracy': 'mean',\n",
    "                'ari': 'mean'\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Score each algorithm (higher detection rate and lower false detection rate is better)\n",
    "            avg_by_alg['score'] = avg_by_alg['puea_detection_rate'] - avg_by_alg['false_detection_rate']\n",
    "            \n",
    "            # Print scoring for all algorithms\n",
    "            print(f\"\\nAlgorithm scoring for Case {case}:\")\n",
    "            for _, row in avg_by_alg.iterrows():\n",
    "                print(f\"  {row['algorithm']}: Score={row['score']:.3f}, DR={row['puea_detection_rate']:.3f}, FDR={row['false_detection_rate']:.3f}\")\n",
    "            \n",
    "            # Get best algorithm\n",
    "            best_alg = avg_by_alg.loc[avg_by_alg['score'].idxmax()]\n",
    "            \n",
    "            print(f\"Case {case} - Best algorithm: {best_alg['algorithm']}\")\n",
    "            print(f\"  Detection Rate: {best_alg['puea_detection_rate']:.3f}\")\n",
    "            print(f\"  False Detection Rate: {best_alg['false_detection_rate']:.3f}\")\n",
    "            print(f\"  Accuracy: {best_alg['accuracy']:.1f}%\")\n",
    "            print(f\"  ARI: {best_alg['ari']:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No detection metrics available. Make sure clustering results are generated correctly.\")\n",
    "    print(\"Available keys in clustering_results:\", list(clustering_results.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Specialized Algorithms for PUEA Detection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the K-NN algorithm for PUEA detection\n",
    "def knn_algorithm_puea_detection(distance_matrix, candidate_set, normal_set, R=5):\n",
    "    \"\"\"\n",
    "    K-NN algorithm for PUEA detection\n",
    "    \n",
    "    Parameters:\n",
    "        distance_matrix: Manhattan distance matrix\n",
    "        candidate_set: Set of indices of suspected PUEA attackers\n",
    "        normal_set: Set of indices of presumed legitimate users\n",
    "        R: Number of nearest neighbors to consider\n",
    "    \n",
    "    Returns:\n",
    "        outlier_set: Set of nodes classified as PUEA attackers\n",
    "    \"\"\"\n",
    "    n = distance_matrix.shape[0]\n",
    "    outlier_set = set()\n",
    "    \n",
    "    print(f\"Running K-NN algorithm with R={R}\")\n",
    "    print(f\"Initial candidate set size: {len(candidate_set)}\")\n",
    "    print(f\"Initial normal set size: {len(normal_set)}\")\n",
    "    \n",
    "    for i in candidate_set:\n",
    "        # Sort distances to find nearest neighbors (exclude self)\n",
    "        distances = [(j, distance_matrix[i, j]) for j in range(n) if i != j]\n",
    "        distances.sort(key=lambda x: x[1])  # Sort by distance\n",
    "        \n",
    "        # Count neighbors in candidate vs normal set for the R nearest neighbors\n",
    "        cCand = 0\n",
    "        cNormal = 0\n",
    "        \n",
    "        for j in range(min(R, len(distances))):\n",
    "            neighbor_idx = distances[j][0]\n",
    "            if neighbor_idx in candidate_set:\n",
    "                cCand += 1\n",
    "            elif neighbor_idx in normal_set:\n",
    "                cNormal += 1\n",
    "        \n",
    "        # Decision: if more neighbors are in normal set, it's an outlier (PUEA)\n",
    "        if cCand <= cNormal:\n",
    "            outlier_set.add(i)\n",
    "    \n",
    "    print(f\"K-NN algorithm detected {len(outlier_set)} outliers\")\n",
    "    return outlier_set\n",
    "\n",
    "# Modified means_algorithm_puea_detection function with explicit type conversion\n",
    "def means_algorithm_puea_detection(distance_matrix, candidate_set, normal_set):\n",
    "    \"\"\"\n",
    "    Means algorithm for PUEA detection\n",
    "    \n",
    "    Parameters:\n",
    "        distance_matrix: Manhattan distance matrix\n",
    "        candidate_set: Set of indices of suspected PUEA attackers\n",
    "        normal_set: Set of indices of presumed legitimate users\n",
    "    \n",
    "    Returns:\n",
    "        refined_candidate_set: Updated set of suspected PUEA attackers\n",
    "        refined_normal_set: Updated set of normal users\n",
    "    \"\"\"\n",
    "    n = distance_matrix.shape[0]\n",
    "    \n",
    "    # Initialize distance arrays - convert numpy integers to Python integers\n",
    "    cand_dist = {int(i): 0 for i in candidate_set}\n",
    "    norm_dist = {int(i): 0 for i in normal_set}\n",
    "    \n",
    "    print(f\"Running Means algorithm\")\n",
    "    print(f\"Initial candidate set size: {len(candidate_set)}\")\n",
    "    print(f\"Initial normal set size: {len(normal_set)}\")\n",
    "    \n",
    "    # Calculate distances for candidate set\n",
    "    total_cand_dist = 0\n",
    "    for i in candidate_set:\n",
    "        i_int = int(i)  # Convert to Python integer\n",
    "        for j in range(n):\n",
    "            cand_dist[i_int] += distance_matrix[i, j]\n",
    "        total_cand_dist += cand_dist[i_int]\n",
    "    \n",
    "    # Calculate distances for normal set\n",
    "    total_norm_dist = 0\n",
    "    for i in normal_set:\n",
    "        i_int = int(i)  # Convert to Python integer\n",
    "        for j in range(n):\n",
    "            norm_dist[i_int] += distance_matrix[i, j]\n",
    "        total_norm_dist += norm_dist[i_int]\n",
    "    \n",
    "    # Calculate means\n",
    "    cand_dist_mean = total_cand_dist / len(candidate_set) if candidate_set else 0\n",
    "    norm_dist_mean = total_norm_dist / len(normal_set) if normal_set else 0\n",
    "    \n",
    "    # Refine sets - move points that are closer to the normal mean than candidate mean\n",
    "    refined_candidate_set = set(candidate_set)\n",
    "    refined_normal_set = set(normal_set)\n",
    "    \n",
    "    reassigned_count = 0\n",
    "    \n",
    "    for i in list(candidate_set):  # Use list to avoid modification during iteration\n",
    "        i_int = int(i)  # Convert to Python integer\n",
    "        # If point is closer to normal set mean than candidate set mean\n",
    "        if abs(cand_dist_mean - cand_dist[i_int]) > abs(norm_dist_mean - norm_dist[i_int]):\n",
    "            refined_candidate_set.remove(i)\n",
    "            refined_normal_set.add(i)\n",
    "            # Update totals\n",
    "            total_cand_dist -= cand_dist[i_int]\n",
    "            total_norm_dist += cand_dist[i_int]\n",
    "            reassigned_count += 1\n",
    "            \n",
    "            # Update means\n",
    "            if refined_candidate_set:\n",
    "                cand_dist_mean = total_cand_dist / len(refined_candidate_set)\n",
    "            if refined_normal_set:\n",
    "                norm_dist_mean = total_norm_dist / len(refined_normal_set)\n",
    "    \n",
    "    print(f\"Means algorithm reassigned {reassigned_count} points\")\n",
    "    print(f\"Final candidate set size: {len(refined_candidate_set)}\")\n",
    "    print(f\"Final normal set size: {len(refined_normal_set)}\")\n",
    "    \n",
    "    return refined_candidate_set, refined_normal_set\n",
    "\n",
    "# Define the Hybrid K-NN algorithm for PUEA detection\n",
    "def hybrid_knn_puea_detection(distance_matrix, candidate_set, k=5, outlier_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Hybrid K-NN algorithm for PUEA detection\n",
    "    \n",
    "    Parameters:\n",
    "        distance_matrix: Manhattan distance matrix\n",
    "        candidate_set: Set of indices of suspected PUEA attackers\n",
    "        k: Number of nearest neighbors to consider\n",
    "        outlier_fraction: Expected fraction of PUEA attackers in candidate set\n",
    "    \n",
    "    Returns:\n",
    "        outlier_set: Set of nodes classified as PUEA attackers\n",
    "    \"\"\"\n",
    "    n = distance_matrix.shape[0]\n",
    "    outlier_set = set()\n",
    "    \n",
    "    # Convert candidate_set to list for indexing\n",
    "    candidate_list = list(candidate_set)\n",
    "    \n",
    "    # Calculate target outlier count\n",
    "    outlier_count = max(1, int(round(len(candidate_set) * outlier_fraction)))\n",
    "    \n",
    "    print(f\"Running Hybrid K-NN algorithm with k={k}, outlier_fraction={outlier_fraction}\")\n",
    "    print(f\"Target outlier count: {outlier_count}\")\n",
    "    print(f\"Initial candidate set size: {len(candidate_set)}\")\n",
    "    \n",
    "    # Calculate k-NN distances for each candidate\n",
    "    knn_dist = {}\n",
    "    \n",
    "    for i in candidate_set:\n",
    "        # Find distances to all other points\n",
    "        distances = [(j, distance_matrix[i, j]) for j in range(n) if (i != j)]\n",
    "        distances.sort(key=lambda x: x[1])  # Sort by distance\n",
    "        \n",
    "        # Get k nearest neighbors\n",
    "        knn_dist[i] = sum(dist for _, dist in distances[:k])\n",
    "    \n",
    "    # Calculate mean KNN distance\n",
    "    current_candidate_set = set(candidate_set)\n",
    "    total_knn_dist = sum(knn_dist.values())\n",
    "    \n",
    "    # Iteratively detect outliers\n",
    "    detected_outliers = 0\n",
    "    iteration = 0\n",
    "    max_iterations = 100  # Prevent infinite loop\n",
    "    \n",
    "    while detected_outliers < outlier_count and current_candidate_set and iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        \n",
    "        # Calculate mean k-NN distance\n",
    "        knn_dist_mean = total_knn_dist / len(current_candidate_set) if current_candidate_set else 0\n",
    "        \n",
    "        # Find points with distance greater than mean\n",
    "        outliers_this_iteration = []\n",
    "        \n",
    "        for i in list(current_candidate_set):\n",
    "            if knn_dist[i] > knn_dist_mean:\n",
    "                outliers_this_iteration.append(i)\n",
    "        \n",
    "        # If no outliers found in this iteration, break\n",
    "        if not outliers_this_iteration:\n",
    "            break\n",
    "            \n",
    "        # Add new outliers up to the target count\n",
    "        for i in outliers_this_iteration:\n",
    "            if detected_outliers < outlier_count:\n",
    "                outlier_set.add(i)\n",
    "                current_candidate_set.remove(i)\n",
    "                total_knn_dist -= knn_dist[i]\n",
    "                detected_outliers += 1\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    print(f\"Hybrid K-NN algorithm detected {len(outlier_set)} outliers in {iteration} iterations\")\n",
    "    return outlier_set\n",
    "\n",
    "# Function to apply all three detection algorithms to a specific case and percentage\n",
    "def apply_detection_algorithms(case, percentage, distance_dir=\"distance_matrices\"):\n",
    "    \"\"\"\n",
    "    Apply all three detection algorithms to a specific case and percentage\n",
    "    \n",
    "    Parameters:\n",
    "        case: Case identifier (A, B, or C)\n",
    "        percentage: PUEA percentage in the dataset\n",
    "        distance_dir: Directory containing distance matrices\n",
    "    \n",
    "    Returns:\n",
    "        results: Dictionary containing detection results from all algorithms\n",
    "    \"\"\"\n",
    "    # Load distance matrix\n",
    "    distance_matrix_file = os.path.join(distance_dir, f\"{case}_{percentage}_manhattan_dist.csv\")\n",
    "    if not os.path.exists(distance_matrix_file):\n",
    "        print(f\"Error: Distance matrix file {distance_matrix_file} not found.\")\n",
    "        return None\n",
    "    \n",
    "    distance_matrix = pd.read_csv(distance_matrix_file).values\n",
    "    \n",
    "    # Load original labels for validation\n",
    "    original_file = f\"{case}_case_{percentage}percent_matrix.csv\"\n",
    "    if not os.path.exists(original_file):\n",
    "        print(f\"Error: Original file {original_file} not found.\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(original_file)\n",
    "    true_labels = df['Label'].values\n",
    "    \n",
    "    # Initial classification based on statistical features\n",
    "    # For initial division, we can use K-means or another simple clustering\n",
    "    from sklearn.cluster import KMeans\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42).fit(df.iloc[:, :-1].values)\n",
    "    initial_labels = kmeans.labels_\n",
    "    \n",
    "    # Create candidate and normal sets\n",
    "    # Determine which cluster is more likely to be PUEA\n",
    "    cluster_0_puea_count = np.sum(true_labels[initial_labels == 0] == 1)\n",
    "    cluster_1_puea_count = np.sum(true_labels[initial_labels == 1] == 1)\n",
    "    \n",
    "    print(f\"\\nInitial clustering:\")\n",
    "    print(f\"Cluster 0: {np.sum(initial_labels == 0)} points, {cluster_0_puea_count} are PUEA\")\n",
    "    print(f\"Cluster 1: {np.sum(initial_labels == 1)} points, {cluster_1_puea_count} are PUEA\")\n",
    "    \n",
    "    # The cluster with more PUEA labels becomes the candidate set\n",
    "    if cluster_0_puea_count > cluster_1_puea_count:\n",
    "        candidate_set = set(np.where(initial_labels == 0)[0])\n",
    "        normal_set = set(np.where(initial_labels == 1)[0])\n",
    "        print(\"Cluster 0 selected as candidate set\")\n",
    "    else:\n",
    "        candidate_set = set(np.where(initial_labels == 1)[0])\n",
    "        normal_set = set(np.where(initial_labels == 0)[0])\n",
    "        print(\"Cluster 1 selected as candidate set\")\n",
    "    \n",
    "    print(f\"\\nApplying algorithms for Case {case}, {percentage}% PUEA:\")\n",
    "    \n",
    "    # Apply K-NN algorithm\n",
    "    print(\"\\n1. K-NN Algorithm:\")\n",
    "    knn_outliers = knn_algorithm_puea_detection(\n",
    "        distance_matrix, candidate_set, normal_set, R=5)\n",
    "    \n",
    "    # Apply Means algorithm\n",
    "    print(\"\\n2. Means Algorithm:\")\n",
    "    refined_candidate, refined_normal = means_algorithm_puea_detection(\n",
    "        distance_matrix, candidate_set, normal_set)\n",
    "    \n",
    "    # Apply Hybrid K-NN algorithm\n",
    "    print(\"\\n3. Hybrid K-NN Algorithm:\")\n",
    "    hybrid_outliers = hybrid_knn_puea_detection(\n",
    "        distance_matrix, candidate_set, k=5, outlier_fraction=percentage/100)\n",
    "    \n",
    "    # Evaluate results against true labels\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    knn_metrics = evaluate_detection(knn_outliers, true_labels)\n",
    "    means_metrics = evaluate_detection(refined_candidate, true_labels)\n",
    "    hybrid_metrics = evaluate_detection(hybrid_outliers, true_labels)\n",
    "    \n",
    "    print(f\"K-NN: DR={knn_metrics['puea_detection_rate']:.2f}, FDR={knn_metrics['false_detection_rate']:.2f}, Acc={knn_metrics['accuracy']:.2f}\")\n",
    "    print(f\"Means: DR={means_metrics['puea_detection_rate']:.2f}, FDR={means_metrics['false_detection_rate']:.2f}, Acc={means_metrics['accuracy']:.2f}\")\n",
    "    print(f\"Hybrid: DR={hybrid_metrics['puea_detection_rate']:.2f}, FDR={hybrid_metrics['false_detection_rate']:.2f}, Acc={hybrid_metrics['accuracy']:.2f}\")\n",
    "    \n",
    "    results = {\n",
    "        'knn': knn_metrics,\n",
    "        'means': means_metrics,\n",
    "        'hybrid': hybrid_metrics\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_detection(detected_set, true_labels):\n",
    "    \"\"\"\n",
    "    Evaluate detection performance\n",
    "    \n",
    "    Parameters:\n",
    "        detected_set: Set of indices detected as PUEA\n",
    "        true_labels: Array of true labels (0=PU, 1=PUEA)\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dictionary containing detection metrics\n",
    "    \"\"\"\n",
    "    # Create detected labels array\n",
    "    detected_labels = np.zeros_like(true_labels)\n",
    "    for idx in detected_set:\n",
    "        detected_labels[idx] = 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    tp = np.sum((detected_labels == 1) & (true_labels == 1))\n",
    "    fp = np.sum((detected_labels == 1) & (true_labels == 0))\n",
    "    tn = np.sum((detected_labels == 0) & (true_labels == 0))\n",
    "    fn = np.sum((detected_labels == 0) & (true_labels == 1))\n",
    "    \n",
    "    puea_detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    false_detection_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    accuracy = (tp + tn) / len(true_labels)\n",
    "    \n",
    "    return {\n",
    "        'puea_detection_rate': puea_detection_rate,\n",
    "        'false_detection_rate': false_detection_rate,\n",
    "        'accuracy': accuracy,\n",
    "        'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn\n",
    "    }\n",
    "\n",
    "# Add this at the top of your Cell 5 code\n",
    "def run_all_experiments():\n",
    "    results = {}\n",
    "    cases = ['A', 'B', 'C']\n",
    "    percentages = [10, 20, 30, 40, 50]\n",
    "    \n",
    "    # Create directory for results if it doesn't exist\n",
    "    results_dir = \"specialized_algorithm_results\"\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "    \n",
    "    # Process each case and percentage\n",
    "    for case in cases:\n",
    "        for percentage in percentages:\n",
    "            key = f\"{case}_{percentage}\"\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Processing {key}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            try:\n",
    "                # Fix: Convert the numpy int64 indices to Python integers\n",
    "                # This is the key fix for the \"np.int64(0)\" error\n",
    "                result = apply_detection_algorithms(case, percentage)\n",
    "                if result:\n",
    "                    results[key] = result\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {key}: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()  # Print the full traceback for better debugging\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df = []\n",
    "    \n",
    "    for key, result in results.items():\n",
    "        case, percentage = key.split('_')\n",
    "        \n",
    "        for alg_name, metrics in result.items():\n",
    "            results_df.append({\n",
    "                'Case': case,\n",
    "                'Percentage': percentage,\n",
    "                'Algorithm': alg_name,\n",
    "                'PUEA_Detection_Rate': metrics['puea_detection_rate'],\n",
    "                'False_Detection_Rate': metrics['false_detection_rate'],\n",
    "                'Accuracy': metrics['accuracy'],\n",
    "                'TP': metrics['tp'],\n",
    "                'FP': metrics['fp'],\n",
    "                'TN': metrics['tn'],\n",
    "                'FN': metrics['fn']\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    if results_df:\n",
    "        df = pd.DataFrame(results_df)\n",
    "        csv_file = f\"{results_dir}/specialized_algorithms_results.csv\"\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        print(f\"\\nResults saved to {csv_file}\")\n",
    "        \n",
    "        # Return DataFrame for visualization\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No results to save.\")\n",
    "        return None\n",
    "\n",
    "def visualize_results(results_df):\n",
    "    \"\"\"\n",
    "    Create visualizations for algorithm comparison\n",
    "    \n",
    "    Parameters:\n",
    "        results_df: DataFrame with results from all algorithms\n",
    "    \"\"\"\n",
    "    if results_df is None or results_df.empty:\n",
    "        print(\"No results to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Create directory for visualizations if it doesn't exist\n",
    "    viz_dir = \"specialized_algorithm_visualizations\"\n",
    "    if not os.path.exists(viz_dir):\n",
    "        os.makedirs(viz_dir)\n",
    "    \n",
    "    # Plot detection rates for each case\n",
    "    cases = results_df['Case'].unique()\n",
    "    \n",
    "    # Create figure with appropriate layout\n",
    "    plt.figure(figsize=(15, 4*len(cases)))\n",
    "    \n",
    "    # Create color map for algorithms\n",
    "    colors = {'knn': 'blue', 'means': 'green', 'hybrid': 'red'}\n",
    "    markers = {'knn': 'o', 'means': 's', 'hybrid': '^'}\n",
    "    linestyles = {'knn': '-', 'means': '--', 'hybrid': '-.'}\n",
    "    \n",
    "    for i, case in enumerate(cases):\n",
    "        # Filter data for this case\n",
    "        case_data = results_df[results_df['Case'] == case]\n",
    "        \n",
    "        # Create detection rate and false detection rate subplots\n",
    "        plt.subplot(len(cases), 2, 2*i+1)\n",
    "        \n",
    "        # Plot detection rate for each algorithm\n",
    "        for alg in ['knn', 'means', 'hybrid']:\n",
    "            alg_data = case_data[case_data['Algorithm'] == alg].sort_values('Percentage')\n",
    "            if not alg_data.empty:\n",
    "                plt.plot(alg_data['Percentage'], \n",
    "                         alg_data['PUEA_Detection_Rate'],\n",
    "                         marker=markers[alg],\n",
    "                         linestyle=linestyles[alg],\n",
    "                         color=colors[alg],\n",
    "                         label=f\"{alg.capitalize()}\")\n",
    "                \n",
    "                # Add data labels\n",
    "                for x, y in zip(alg_data['Percentage'], alg_data['PUEA_Detection_Rate']):\n",
    "                    plt.text(x, y+0.02, f'{y:.2f}', \n",
    "                             ha='center', va='bottom', fontsize=8, color=colors[alg])\n",
    "        \n",
    "        plt.title(f'Case {case} - PUEA Detection Rate')\n",
    "        plt.xlabel('PUEA Percentage (%)')\n",
    "        plt.ylabel('Detection Rate')\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.xticks(sorted(results_df['Percentage'].unique()))\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.legend()\n",
    "        \n",
    "        # False Detection Rate subplot\n",
    "        plt.subplot(len(cases), 2, 2*i+2)\n",
    "        \n",
    "        # Plot false detection rate for each algorithm\n",
    "        for alg in ['knn', 'means', 'hybrid']:\n",
    "            alg_data = case_data[case_data['Algorithm'] == alg].sort_values('Percentage')\n",
    "            if not alg_data.empty:\n",
    "                plt.plot(alg_data['Percentage'], \n",
    "                         alg_data['False_Detection_Rate'],\n",
    "                         marker=markers[alg],\n",
    "                         linestyle=linestyles[alg],\n",
    "                         color=colors[alg],\n",
    "                         label=f\"{alg.capitalize()}\")\n",
    "                \n",
    "                # Add data labels\n",
    "                for x, y in zip(alg_data['Percentage'], alg_data['False_Detection_Rate']):\n",
    "                    plt.text(x, y+0.02, f'{y:.2f}', \n",
    "                             ha='center', va='bottom', fontsize=8, color=colors[alg])\n",
    "        \n",
    "        plt.title(f'Case {case} - False Detection Rate')\n",
    "        plt.xlabel('PUEA Percentage (%)')\n",
    "        plt.ylabel('False Detection Rate')\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.xticks(sorted(results_df['Percentage'].unique()))\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{viz_dir}/detection_rates_by_case.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create heatmaps for detection rates across all cases and algorithms\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Aggregate data by algorithm and case\n",
    "    pivot_dr = pd.pivot_table(\n",
    "        results_df, \n",
    "        values='PUEA_Detection_Rate', \n",
    "        index=['Algorithm', 'Percentage'], \n",
    "        columns=['Case']\n",
    "    ).round(2)\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(pivot_dr, annot=True, cmap='viridis', \n",
    "                fmt='.2f', linewidths=0.5, cbar_kws={'label': 'PUEA Detection Rate'})\n",
    "    plt.title('PUEA Detection Rate by Algorithm, Case and Percentage')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{viz_dir}/detection_rate_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Compare with existing clustering algorithms if results are available\n",
    "    try:\n",
    "        existing_results = f\"performance_visualizations/detection_performance_metrics.csv\"\n",
    "        if os.path.exists(existing_results):\n",
    "            existing_df = pd.read_csv(existing_results)\n",
    "            # Normalize algorithm names for comparison\n",
    "            existing_df['Algorithm'] = existing_df['algorithm'].str.lower()\n",
    "            existing_df['PUEA_Detection_Rate'] = existing_df['puea_detection_rate']\n",
    "            existing_df['False_Detection_Rate'] = existing_df['false_detection_rate']\n",
    "            \n",
    "            # Create comparison plots\n",
    "            plt.figure(figsize=(16, 12))\n",
    "            \n",
    "            # Plot average detection rates across all cases\n",
    "            plt.subplot(2, 1, 1)\n",
    "            \n",
    "            # Calculate average detection rates\n",
    "            specialized_avg = results_df.groupby(['Algorithm', 'Percentage'])['PUEA_Detection_Rate'].mean().reset_index()\n",
    "            existing_avg = existing_df.groupby(['Algorithm', 'percentage'])['PUEA_Detection_Rate'].mean().reset_index()\n",
    "            existing_avg.rename(columns={'percentage': 'Percentage'}, inplace=True)\n",
    "            \n",
    "            # Plot specialized algorithms\n",
    "            for alg in ['knn', 'means', 'hybrid']:\n",
    "                alg_data = specialized_avg[specialized_avg['Algorithm'] == alg]\n",
    "                plt.plot(alg_data['Percentage'], alg_data['PUEA_Detection_Rate'],\n",
    "                         marker=markers[alg], linestyle=linestyles[alg],\n",
    "                         color=colors[alg], linewidth=2.5,\n",
    "                         label=f\"Specialized {alg.capitalize()}\")\n",
    "            \n",
    "            # Plot existing algorithms with different line styles\n",
    "            existing_colors = {'dbscan': 'purple', 'k-means': 'orange', 'agglomerative': 'brown'}\n",
    "            existing_markers = {'dbscan': 'x', 'k-means': '+', 'agglomerative': 'd'}\n",
    "            \n",
    "            for alg in existing_df['Algorithm'].unique():\n",
    "                if alg.lower() in existing_colors:\n",
    "                    alg_data = existing_avg[existing_avg['Algorithm'] == alg.lower()]\n",
    "                    if not alg_data.empty:\n",
    "                        plt.plot(alg_data['Percentage'], alg_data['PUEA_Detection_Rate'],\n",
    "                                marker=existing_markers.get(alg.lower(), '*'),\n",
    "                                linestyle=':', linewidth=1.5,\n",
    "                                color=existing_colors.get(alg.lower(), 'gray'),\n",
    "                                label=f\"Classic {alg}\")\n",
    "            \n",
    "            plt.title('Average PUEA Detection Rate: Specialized vs Classic Algorithms')\n",
    "            plt.xlabel('PUEA Percentage (%)')\n",
    "            plt.ylabel('Detection Rate')\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "            plt.xticks(sorted(results_df['Percentage'].unique()))\n",
    "            plt.ylim(0, 1.05)\n",
    "            plt.legend(loc='lower right')\n",
    "            \n",
    "            # Plot average false detection rates\n",
    "            plt.subplot(2, 1, 2)\n",
    "            \n",
    "            # Calculate average false detection rates\n",
    "            specialized_fdr = results_df.groupby(['Algorithm', 'Percentage'])['False_Detection_Rate'].mean().reset_index()\n",
    "            existing_fdr = existing_df.groupby(['Algorithm', 'percentage'])['False_Detection_Rate'].mean().reset_index()\n",
    "            existing_fdr.rename(columns={'percentage': 'Percentage'}, inplace=True)\n",
    "            \n",
    "            # Plot specialized algorithms\n",
    "            for alg in ['knn', 'means', 'hybrid']:\n",
    "                alg_data = specialized_fdr[specialized_fdr['Algorithm'] == alg]\n",
    "                plt.plot(alg_data['Percentage'], alg_data['False_Detection_Rate'],\n",
    "                         marker=markers[alg], linestyle=linestyles[alg],\n",
    "                         color=colors[alg], linewidth=2.5,\n",
    "                         label=f\"Specialized {alg.capitalize()}\")\n",
    "            \n",
    "            # Plot existing algorithms\n",
    "            for alg in existing_df['Algorithm'].unique():\n",
    "                if alg.lower() in existing_colors:\n",
    "                    alg_data = existing_fdr[existing_fdr['Algorithm'] == alg.lower()]\n",
    "                    if not alg_data.empty:\n",
    "                        plt.plot(alg_data['Percentage'], alg_data['False_Detection_Rate'],\n",
    "                                marker=existing_markers.get(alg.lower(), '*'),\n",
    "                                linestyle=':', linewidth=1.5,\n",
    "                                color=existing_colors.get(alg.lower(), 'gray'),\n",
    "                                label=f\"Classic {alg}\")\n",
    "            \n",
    "            plt.title('Average False Detection Rate: Specialized vs Classic Algorithms')\n",
    "            plt.xlabel('PUEA Percentage (%)')\n",
    "            plt.ylabel('False Detection Rate')\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "            plt.xticks(sorted(results_df['Percentage'].unique()))\n",
    "            plt.ylim(0, 1.05)\n",
    "            plt.legend(loc='upper right')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{viz_dir}/specialized_vs_classic_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"Comparison visualizations saved to {viz_dir}\")\n",
    "        else:\n",
    "            print(\"No existing algorithm results found for comparison.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating comparison visualizations: {str(e)}\")\n",
    "    \n",
    "    print(f\"Visualizations saved to {viz_dir}\")\n",
    "\n",
    "# Main execution\n",
    "print(\"Running specialized algorithms for PUEA detection...\")\n",
    "results_df = run_all_experiments()\n",
    "if results_df is not None:\n",
    "    visualize_results(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Enhanced Visualizations for PUEA Detection Results\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Create directory for enhanced visualizations\n",
    "viz_dir = \"enhanced_visualizations\"\n",
    "if not os.path.exists(viz_dir):\n",
    "    os.makedirs(viz_dir)\n",
    "\n",
    "# Load results from specialized algorithms\n",
    "specialized_results_file = \"specialized_algorithm_results/specialized_algorithms_results.csv\"\n",
    "if os.path.exists(specialized_results_file):\n",
    "    specialized_df = pd.read_csv(specialized_results_file)\n",
    "    print(f\"Loaded specialized algorithm results: {len(specialized_df)} entries\")\n",
    "else:\n",
    "    specialized_df = None\n",
    "    print(\"Specialized algorithm results not found, run Cell 5 first\")\n",
    "\n",
    "# Load results from clustering algorithms\n",
    "clustering_results_file = \"performance_visualizations/detection_performance_metrics.csv\"\n",
    "if os.path.exists(clustering_results_file):\n",
    "    clustering_df = pd.read_csv(clustering_results_file)\n",
    "    # Normalize column names to match with specialized algorithms\n",
    "    clustering_df = clustering_df.rename(columns={\n",
    "        'puea_detection_rate': 'PUEA_Detection_Rate',\n",
    "        'false_detection_rate': 'False_Detection_Rate',\n",
    "        'accuracy': 'Accuracy',\n",
    "        'algorithm': 'Algorithm',\n",
    "        'case': 'Case',\n",
    "        'percentage': 'Percentage'\n",
    "    })\n",
    "    print(f\"Loaded clustering algorithm results: {len(clustering_df)} entries\")\n",
    "else:\n",
    "    clustering_df = None\n",
    "    print(\"Clustering algorithm results not found, run Cell 4 first\")\n",
    "\n",
    "if specialized_df is not None or clustering_df is not None:\n",
    "    # Create a combined DataFrame of all results\n",
    "    combined_results = []\n",
    "    \n",
    "    if specialized_df is not None:\n",
    "        for _, row in specialized_df.iterrows():\n",
    "            combined_results.append({\n",
    "                'Case': row['Case'],\n",
    "                'Percentage': int(row['Percentage']),\n",
    "                'Algorithm': f\"Specialized_{row['Algorithm'].capitalize()}\",\n",
    "                'Algorithm_Type': 'Specialized',\n",
    "                'Algorithm_Name': row['Algorithm'],\n",
    "                'PUEA_Detection_Rate': row['PUEA_Detection_Rate'],\n",
    "                'False_Detection_Rate': row['False_Detection_Rate'],\n",
    "                'Accuracy': row['Accuracy'] * 100 if row['Accuracy'] <= 1 else row['Accuracy']\n",
    "            })\n",
    "    \n",
    "    if clustering_df is not None:\n",
    "        for _, row in clustering_df.iterrows():\n",
    "            combined_results.append({\n",
    "                'Case': row['Case'],\n",
    "                'Percentage': int(row['Percentage']),\n",
    "                'Algorithm': f\"Clustering_{row['Algorithm']}\",\n",
    "                'Algorithm_Type': 'Clustering',\n",
    "                'Algorithm_Name': row['Algorithm'],\n",
    "                'PUEA_Detection_Rate': row['PUEA_Detection_Rate'],\n",
    "                'False_Detection_Rate': row['False_Detection_Rate'],\n",
    "                'Accuracy': row['Accuracy']\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    all_results_df = pd.DataFrame(combined_results)\n",
    "    \n",
    "    # ===============================================\n",
    "    # 1. Create detection rate comparison by case\n",
    "    # ===============================================\n",
    "    cases = all_results_df['Case'].unique()\n",
    "    percentages = sorted(all_results_df['Percentage'].unique())\n",
    "    \n",
    "    # Define color scheme\n",
    "    colors = {\n",
    "        'Specialized_Knn': '#1f77b4',      # Blue\n",
    "        'Specialized_Means': '#2ca02c',    # Green\n",
    "        'Specialized_Hybrid': '#d62728',   # Red\n",
    "        'Clustering_DBSCAN': '#9467bd',    # Purple\n",
    "        'Clustering_K-means': '#ff7f0e',   # Orange\n",
    "        'Clustering_Agglomerative': '#8c564b'  # Brown\n",
    "    }\n",
    "    \n",
    "    # Create multi-plot figure showing detection rates for each case\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    for i, case in enumerate(cases):\n",
    "        case_data = all_results_df[all_results_df['Case'] == case]\n",
    "        \n",
    "        # Plot PUEA Detection Rate\n",
    "        plt.subplot(3, 2, 2*i+1)\n",
    "        for alg in case_data['Algorithm'].unique():\n",
    "            alg_data = case_data[case_data['Algorithm'] == alg].sort_values('Percentage')\n",
    "            plt.plot(alg_data['Percentage'], alg_data['PUEA_Detection_Rate'], \n",
    "                    marker='o' if 'Specialized' in alg else '^',\n",
    "                    linestyle='-' if 'Specialized' in alg else '--',\n",
    "                    linewidth=2.5 if 'Specialized' in alg else 2.0,\n",
    "                    color=colors.get(alg, 'gray'),\n",
    "                    label=alg)\n",
    "            \n",
    "        plt.title(f'Case {case} - PUEA Detection Rate', fontsize=14)\n",
    "        plt.xlabel('PUEA Percentage (%)', fontsize=12)\n",
    "        plt.ylabel('Detection Rate', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xticks(percentages)\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.legend(loc='lower right', fontsize=10)\n",
    "        \n",
    "        # Plot False Detection Rate\n",
    "        plt.subplot(3, 2, 2*i+2)\n",
    "        for alg in case_data['Algorithm'].unique():\n",
    "            alg_data = case_data[case_data['Algorithm'] == alg].sort_values('Percentage')\n",
    "            plt.plot(alg_data['Percentage'], alg_data['False_Detection_Rate'], \n",
    "                    marker='o' if 'Specialized' in alg else '^',\n",
    "                    linestyle='-' if 'Specialized' in alg else '--',\n",
    "                    linewidth=2.5 if 'Specialized' in alg else 2.0,\n",
    "                    color=colors.get(alg, 'gray'),\n",
    "                    label=alg)\n",
    "            \n",
    "        plt.title(f'Case {case} - False Detection Rate', fontsize=14)\n",
    "        plt.xlabel('PUEA Percentage (%)', fontsize=12)\n",
    "        plt.ylabel('False Detection Rate', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xticks(percentages)\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.legend(loc='upper right', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{viz_dir}/detection_comparison_by_case.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # ===============================================\n",
    "    # 2. Create performance heatmaps\n",
    "    # ===============================================\n",
    "    # Function to create heatmap\n",
    "    def create_performance_heatmap(metric, title, filename, cmap='RdYlGn'):\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Create pivot table for heatmap\n",
    "        pivot_data = pd.pivot_table(\n",
    "            all_results_df, \n",
    "            values=metric,\n",
    "            index=['Algorithm_Type', 'Algorithm_Name'],\n",
    "            columns=['Case', 'Percentage']\n",
    "        ).round(2)\n",
    "        \n",
    "        # Create custom colormap\n",
    "        if metric == 'False_Detection_Rate':\n",
    "            cmap = 'RdYlGn_r'  # Reversed colormap for FDR (lower is better)\n",
    "            \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(pivot_data, annot=True, cmap=cmap, fmt='.2f', \n",
    "                   linewidths=0.5, linecolor='white', \n",
    "                   cbar_kws={'label': metric.replace('_', ' ')})\n",
    "        \n",
    "        plt.title(title, fontsize=16, pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{viz_dir}/{filename}\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # Create heatmaps for each metric\n",
    "    create_performance_heatmap('PUEA_Detection_Rate', \n",
    "                             'PUEA Detection Rate Comparison Across Algorithms and Scenarios',\n",
    "                             'puea_detection_rate_heatmap.png')\n",
    "                             \n",
    "    create_performance_heatmap('False_Detection_Rate', \n",
    "                             'False Detection Rate Comparison Across Algorithms and Scenarios',\n",
    "                             'false_detection_rate_heatmap.png')\n",
    "                             \n",
    "    create_performance_heatmap('Accuracy', \n",
    "                             'Accuracy Comparison Across Algorithms and Scenarios',\n",
    "                             'accuracy_heatmap.png')\n",
    "    \n",
    "    # ===============================================\n",
    "    # 3. Create performance radar charts\n",
    "    # ===============================================\n",
    "    # Aggregate performance metrics by algorithm type\n",
    "    avg_performance = all_results_df.groupby(['Algorithm_Type', 'Algorithm_Name']).agg({\n",
    "        'PUEA_Detection_Rate': 'mean',\n",
    "        'False_Detection_Rate': 'mean',\n",
    "        'Accuracy': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Normalize values for radar chart (higher is better)\n",
    "    avg_performance['Norm_Detection_Rate'] = avg_performance['PUEA_Detection_Rate']\n",
    "    avg_performance['Norm_False_Rate'] = 1 - avg_performance['False_Detection_Rate']\n",
    "    avg_performance['Norm_Accuracy'] = avg_performance['Accuracy'] / 100 if avg_performance['Accuracy'].max() > 1 else avg_performance['Accuracy']\n",
    "    \n",
    "    # Create radar chart\n",
    "    categories = ['Detection Rate', 'Inverse False Rate', 'Accuracy']\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Create a figure with proper size\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create custom color map for specialized vs clustering algorithms\n",
    "    colors_radar = {\n",
    "        'Specialized': ['#1f77b4', '#2ca02c', '#d62728'],\n",
    "        'Clustering': ['#9467bd', '#ff7f0e', '#8c564b']\n",
    "    }\n",
    "    \n",
    "    # Angle of each axis\n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    # Initialize the subplot\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    \n",
    "    # Draw one axis per variable and add labels\n",
    "    plt.xticks(angles[:-1], categories, size=14)\n",
    "    \n",
    "    # Draw ylabels (0 to 1.0)\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([0.2, 0.4, 0.6, 0.8, 1.0], [\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], size=12)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Plot each algorithm\n",
    "    for alg_type in avg_performance['Algorithm_Type'].unique():\n",
    "        type_data = avg_performance[avg_performance['Algorithm_Type'] == alg_type]\n",
    "        type_colors = colors_radar[alg_type]\n",
    "        \n",
    "        for i, (_, row) in enumerate(type_data.iterrows()):\n",
    "            # Calculate values for radar chart\n",
    "            values = [row['Norm_Detection_Rate'], row['Norm_False_Rate'], row['Norm_Accuracy']]\n",
    "            values += values[:1]  # Close the loop\n",
    "            \n",
    "            # Plot the values\n",
    "            ax.plot(angles, values, linewidth=2, linestyle='-' if alg_type == 'Specialized' else '--',\n",
    "                   label=f\"{alg_type}_{row['Algorithm_Name']}\", color=type_colors[i % len(type_colors)])\n",
    "            ax.fill(angles, values, color=type_colors[i % len(type_colors)], alpha=0.1)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1), fontsize=12)\n",
    "    \n",
    "    plt.title('Algorithm Performance Comparison', size=20, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{viz_dir}/algorithm_radar_chart.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # ===============================================\n",
    "    # 4. Create box plots for performance distribution\n",
    "    # ===============================================\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Create a grid for the boxplots\n",
    "    gs = GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1])\n",
    "    \n",
    "    # Detection Rate boxplot\n",
    "    ax1 = plt.subplot(gs[0, 0])\n",
    "    sns.boxplot(x='Algorithm_Type', y='PUEA_Detection_Rate', \n",
    "               hue='Algorithm_Name', data=all_results_df, ax=ax1)\n",
    "    ax1.set_title('PUEA Detection Rate Distribution', fontsize=14)\n",
    "    ax1.set_xlabel('')\n",
    "    ax1.set_ylabel('Detection Rate', fontsize=12)\n",
    "    ax1.legend(loc='upper right', fontsize=10)\n",
    "    \n",
    "    # False Detection Rate boxplot\n",
    "    ax2 = plt.subplot(gs[0, 1])\n",
    "    sns.boxplot(x='Algorithm_Type', y='False_Detection_Rate', \n",
    "               hue='Algorithm_Name', data=all_results_df, ax=ax2)\n",
    "    ax2.set_title('False Detection Rate Distribution', fontsize=14)\n",
    "    ax2.set_xlabel('')\n",
    "    ax2.set_ylabel('False Detection Rate', fontsize=12)\n",
    "    ax2.legend(loc='upper right', fontsize=10)\n",
    "    \n",
    "    # Case comparison boxplot\n",
    "    ax3 = plt.subplot(gs[1, 0:])\n",
    "    sns.boxplot(x='Case', y='PUEA_Detection_Rate', \n",
    "               hue='Algorithm_Type', data=all_results_df, ax=ax3)\n",
    "    ax3.set_title('Detection Performance by Network Scenario', fontsize=14)\n",
    "    ax3.set_xlabel('Network Scenario', fontsize=12)\n",
    "    ax3.set_ylabel('PUEA Detection Rate', fontsize=12)\n",
    "    ax3.legend(loc='lower right', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{viz_dir}/performance_distributions.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # ===============================================\n",
    "    # 5. Create a comprehensive summary chart\n",
    "    # ===============================================\n",
    "    # Calculate average performance metrics across all scenarios\n",
    "    overall_perf = all_results_df.groupby(['Algorithm', 'Algorithm_Type', 'Algorithm_Name']).agg({\n",
    "        'PUEA_Detection_Rate': 'mean',\n",
    "        'False_Detection_Rate': 'mean',\n",
    "        'Accuracy': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Sort by detection rate\n",
    "    overall_perf = overall_perf.sort_values('PUEA_Detection_Rate', ascending=False)\n",
    "    \n",
    "    # Create a bar chart comparing all algorithms\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Set up bar positions\n",
    "    algorithms = overall_perf['Algorithm'].unique()\n",
    "    x = np.arange(len(algorithms))\n",
    "    width = 0.25\n",
    "    \n",
    "    # Create bars for each metric\n",
    "    plt.bar(x - width, overall_perf['PUEA_Detection_Rate'], width, \n",
    "            label='Detection Rate', color='green', alpha=0.7)\n",
    "    plt.bar(x, overall_perf['Accuracy']/100, width, \n",
    "            label='Accuracy', color='blue', alpha=0.7)\n",
    "    plt.bar(x + width, overall_perf['False_Detection_Rate'], width, \n",
    "            label='False Detection Rate', color='red', alpha=0.7)\n",
    "    \n",
    "    # Customize plot\n",
    "    plt.xlabel('Algorithm', fontsize=14)\n",
    "    plt.ylabel('Performance Metric', fontsize=14)\n",
    "    plt.title('Overall Algorithm Performance Comparison', fontsize=16)\n",
    "    plt.xticks(x, algorithms, rotation=45, ha='right')\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, alg in enumerate(algorithms):\n",
    "        row = overall_perf[overall_perf['Algorithm'] == alg].iloc[0]\n",
    "        plt.text(i - width, row['PUEA_Detection_Rate'] + 0.02, f\"{row['PUEA_Detection_Rate']:.2f}\", \n",
    "                ha='center', va='bottom', fontsize=9, color='darkgreen')\n",
    "        plt.text(i, row['Accuracy']/100 + 0.02, f\"{row['Accuracy']:.1f}%\", \n",
    "                ha='center', va='bottom', fontsize=9, color='darkblue')\n",
    "        plt.text(i + width, row['False_Detection_Rate'] + 0.02, f\"{row['False_Detection_Rate']:.2f}\", \n",
    "                ha='center', va='bottom', fontsize=9, color='darkred')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{viz_dir}/overall_algorithm_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Enhanced visualizations saved to {viz_dir}/ directory\")\n",
    "else:\n",
    "    print(\"Cannot create visualizations: No algorithm results available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
